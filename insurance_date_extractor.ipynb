{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "insurance_date_extractor.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ya-stack/PDF_to_text/blob/main/insurance_date_extractor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iyGimZYIdzL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e6d2a2d-2387-4161-cef0-d4191eadaaf7"
      },
      "source": [
        "%cd 'gdrive/My Drive/Colab Notebooks/pdf2text'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/pdf2text\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reex5BtyKsIJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22f0bb84-54de-4bae-9fd1-6fc1e043b21a"
      },
      "source": [
        "pip install pdfminer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfminer in /usr/local/lib/python3.7/dist-packages (20191125)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.7/dist-packages (from pdfminer) (3.11.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tq6PfzGNk22Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b155ea4c-9a37-4356-9266-d10d011c52af"
      },
      "source": [
        "pip install datefinder"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datefinder\n",
            "  Downloading datefinder-0.7.1-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.4.2 in /usr/local/lib/python3.7/dist-packages (from datefinder) (2.8.2)\n",
            "Requirement already satisfied: regex>=2017.02.08 in /usr/local/lib/python3.7/dist-packages (from datefinder) (2019.12.20)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from datefinder) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.4.2->datefinder) (1.15.0)\n",
            "Installing collected packages: datefinder\n",
            "Successfully installed datefinder-0.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcL8hHvfjkPf",
        "outputId": "c1c78503-7064-4f52-b0f2-cb775977b859"
      },
      "source": [
        "!pip install pdfminer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfminer in /usr/local/lib/python3.7/dist-packages (20191125)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.7/dist-packages (from pdfminer) (3.11.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP7iOiZrKd5C"
      },
      "source": [
        "import sys\n",
        "import datetime\n",
        "import datefinder\n",
        "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "from pdfminer.converter import XMLConverter, HTMLConverter, TextConverter\n",
        "from pdfminer.layout import LAParams\n",
        "from io import StringIO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFITz_XFirU3"
      },
      "source": [
        "def find_between_r( s, first, last ):\n",
        "    try:\n",
        "        start = s.rindex( first ) + len( first )\n",
        "        end = s.rindex( last, start )\n",
        "        return s[start:end]\n",
        "    except ValueError:\n",
        "        return \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iUzZt1FHsXw"
      },
      "source": [
        "def pdfparser_new(data):\n",
        "\n",
        "    fp = open(data, 'rb')\n",
        "    rsrcmgr = PDFResourceManager()\n",
        "    retstr = StringIO()\n",
        "    codec = 'utf-8'\n",
        "    laparams = LAParams()\n",
        "    device = TextConverter(rsrcmgr, retstr, laparams=laparams,\n",
        "                           imagewriter = None)\n",
        "    # Create a PDF interpreter object.\n",
        "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
        "    # Process each page contained in the document.\n",
        "\n",
        "    for page in PDFPage.get_pages(fp, check_extractable=False):\n",
        "        interpreter.process_page(page)\n",
        "        data =  retstr.getvalue()\n",
        "\n",
        "    #print (data)\n",
        "    sub1 = 'Bajaj Allianz General Insurance Company Ltd'\n",
        "    sub2 = 'Edelweiss\tGeneral\tInsurance\tCompany\tLimited'\n",
        "    sub3 = 'Go Digit General Insurance Limited'\n",
        "    sub4 = 'Acko General Insurance Ltd'\n",
        "    sub5 = 'BHARTI AXA GENERAL INSURANCE COMPANY LTD'\n",
        "    sub6 = 'HDFC ERGO General Insurance Company Limited'\n",
        "    sub7 = 'IFFCO-TOKIO GENERAL INSURANCE CO.LTD'\n",
        "    sub8 = 'Future Generali India Insurance Company Limited'\n",
        "    sub9 = 'National Insurance Company Limited'\n",
        "    sub11 = 'ICICI LOMBARD GENERAL INSURANCE CO LTD.'\n",
        "    sub12 = 'The Oriental Insurance Company Limited'\n",
        "    sub13 = 'Royal Sundaram General Insurance Co. Limited'\n",
        "    sub14 = 'SHRIRAM GENERAL INSURANCE COMPANY LTD'\n",
        "    sub15 = 'THE NEW INDIA ASSURANCE CO. LTD'\n",
        "    sub16 = 'UNITED INDIA INSURANCE COMPANY LIMITED'\n",
        "    sub17 = 'AIG'\n",
        "    if sub1.lower() in data.lower():\n",
        "        split_text = data.split('\\n')\n",
        "        indexes = [i for i,x in enumerate(split_text) if x == 'POLICY DETAILS']\n",
        "        ind_val = indexes[0]\n",
        "        policy_number = split_text[ind_val + 4]\n",
        "        policy_from = split_text[ind_val + 16].split(' ')\n",
        "        policy_upto = split_text[ind_val + 17].split(' ')\n",
        "        #policy_n = split_text[1123].split('\\n')\n",
        "        print(policy_from)\n",
        "        policy_detail = {'insurance_company': 'Bajaj Allianz General Insurance Company Ltd',\n",
        "                         'policy_number': policy_number,\n",
        "                         'policy_inception_date': datetime.datetime.strptime(policy_from[2] + ' ' + policy_from[3] + ':00', '%d-%b-%Y %H:%M:%S'),\n",
        "                         'policy_expiry_date': datetime.datetime.strptime(policy_upto[2] + ' ' + '23:59:59', '%d-%b-%Y %H:%M:%S')}\n",
        "    elif sub2.lower() in data.lower():\n",
        "        split_text = data.split('\\n')\n",
        "        indexes = [i for i,x in enumerate(split_text) if x == 'Policy\\tDetails']\n",
        "        policy_n = split_text[indexes[0] - 2].split('\\t')\n",
        "        time_period = split_text[indexes[0] - 6].split('\\t')\n",
        "        policy_detail = {'insurance_company': 'Edelweiss General Insurance Company Limited',\n",
        "                         'policy_number': policy_n[3],\n",
        "                         'policy_inception_date': datetime.datetime.strptime(time_period[4] + ' ' + time_period[5] + time_period[6], '%d-%b-%Y %I:%M%p'),\n",
        "                         'policy_expiry_date': datetime.datetime.strptime(time_period[8] + ' ' + time_period[9]  + time_period[10], '%d-%b-%Y %I:%M%p')}\n",
        "    elif sub3.lower() in data.lower():\n",
        "        split_text = data.split('\\n')\n",
        "        indexes = [i for i,x in enumerate(split_text) if x == 'Policy Number']\n",
        "        policy_detail = {'insurance_company': 'Go Digit General Insurance Limited',\n",
        "                         'policy_number': split_text[indexes[0] + 8],\n",
        "                         'policy_inception_date': datetime.datetime.strptime(split_text[indexes[0] + 13] + ' ' + split_text[indexes[0] + 16], '%d-%b-%Y %H:%M:%S'),\n",
        "                         'policy_expiry_date': datetime.datetime.strptime(split_text[indexes[0] + 14] + ' ' + split_text[indexes[0] + 17], '%d-%b-%Y %H:%M:%S')}\n",
        "    elif sub4.lower() in data.lower():\n",
        "        split_text = data.split('\\n')\n",
        "        indexes = [i for i,x in enumerate(split_text) if x == 'PREMIUM RECEIPT']\n",
        "        time = split_text[indexes[0] + 13].split(' ')\n",
        "        policy_detail = {'insurance_company': 'Acko General Insurance Ltd',\n",
        "                         'policy_number': split_text[indexes[0] + 15],\n",
        "                         'policy_inception_date': datetime.datetime.strptime(time[0]+'-'+time[1]+'-'+'20'+time[2]+' '+'00:00:00', '%d-%b-%Y %H:%M:%S'),\n",
        "                         'policy_expiry_date': datetime.datetime.strptime(time[6]+'-'+time[7]+'-'+'20'+time[8]+ ' '+'23:59:59', '%d-%b-%Y %H:%M:%S')}\n",
        "    elif sub5.lower() in data.lower():\n",
        "        split_text = data.split('\\n')\n",
        "        indexes = [i for i,x in enumerate(split_text) if x == 'Policy Period']\n",
        "        policy_number = split_text[indexes[0] + 4]\n",
        "        time = split_text[indexes[0] + 5].split(' ')\n",
        "        date1 = time[1].split(',')\n",
        "        date2 = time[8].split(',')\n",
        "        policy_detail = {'insurance_company': 'BHARTI AXA GENERAL INSURANCE COMPANY LTD',\n",
        "                         'policy_number': policy_number,\n",
        "                         'policy_inception_date':datetime.datetime.strptime(date1[0] + '-' + time[0]+'-'+time[2]+ ' '+'00:00:00', '%d-%B-%Y %H:%M:%S'),\n",
        "                         'policy_expiry_date':datetime.datetime.strptime(date2[0] + '-' + time[7]+'-'+time[9]+ ' '+'23:59:59', '%d-%B-%Y %H:%M:%S')}\n",
        "    elif sub6.lower() in data.lower():\n",
        "        split_text = data.split(' -')\n",
        "        policy_no = split_text[0]\n",
        "        policy = re.findall(r'\\d+', policy_no)\n",
        "        date = list(datefinder.find_dates(split_text[2]))\n",
        "        print(date)\n",
        "        policy_detail = {'insurance_company': 'HDFC ERGO General Insurance Company Limited',\n",
        "                         'policy_number': policy[0]+policy[1]+policy[2]+policy[3]+policy[4]}\n",
        "    elif sub7.lower() in data.lower():\n",
        "        split_text = data.split('\\n')\n",
        "        indexes = [i for i,x in enumerate(split_text) if x == 'Date of Issuance']\n",
        "        policy_n = split_text[indexes[0]-1].split(' ')\n",
        "        date1 = split_text[indexes[0] + 1].split('  ')\n",
        "        date2 = split_text[indexes[0] + 10].split(' On ')\n",
        "        policy_detail = {'insurance_company': 'IFFCO-TOKIO GENERAL INSURANCE CO.LTD',\n",
        "                         'policy_number': policy_n[3],\n",
        "                         'policy_inception_date':datetime.datetime.strptime(date1[6], '%d/%m/%Y %H:%M:%S'),\n",
        "                         'policy_expiry_date':datetime.datetime.strptime(date2[1], '%d/%m/%Y %H:%M:%S')}\n",
        "    elif sub8.lower() in data.lower():\n",
        "        split_text = data.split('\\n')\n",
        "        indexes = [i for i,x in enumerate(split_text) if x == 'INSURED DETAILS']\n",
        "        policy_n = split_text[indexes[0]-6].split(' ')\n",
        "        indexes2 = [i for i,x in enumerate(split_text) if x == 'Period of Insurance']\n",
        "        date1 = split_text[indexes2[0] + 2].split('  ')\n",
        "        date2 = split_text[indexes2[0] + 4].split(' ')\n",
        "        policy_detail = {'insurance_company': 'Future Generali India Insurance Company Limited',\n",
        "                         'policy_number': policy_n[1],\n",
        "                         'policy_inception_date':datetime.datetime.strptime(date1[1] + ' ' + '00:01:00', '%d/%m/%Y %H:%M:%S'),\n",
        "                         'policy_expiry_date':datetime.datetime.strptime(date2[3]+ ' ' + '23:59:59', '%d/%m/%Y %H:%M:%S')}\n",
        "    elif sub9.lower() in data.lower():\n",
        "        split_text = data.split('\\n')\n",
        "        indexes = [i for i,x in enumerate(split_text) if x == 'पॉिलसी संखया Policy Number: ']\n",
        "        res = [i for i in split_text if 'Policy Effective from' in i]\n",
        "        date = res[0].split(' ')\n",
        "        policy_detail = {'insurance_company': 'National Insurance Company Limited',\n",
        "                         'policy_number': split_text[indexes[0]+1],\n",
        "                         'policy_inception_date':datetime.datetime.strptime(date[13] + ' ' + date[10]+':00', '%d/%m/%Y %H:%M:%S'),\n",
        "                         'policy_expiry_date':datetime.datetime.strptime(date[20]+ ' ' + '23:59:59', '%d/%m/%Y %H:%M:%S')}\n",
        "    \n",
        "    elif sub11.lower() in data.lower():\n",
        "        split_text = data.split('\\n')\n",
        "        indexes = [i for i,x in enumerate(split_text) if x == 'Policy No. & Type']\n",
        "        indexes1 = [i for i,x in enumerate(split_text) if x == 'Accounting Code of Service']\n",
        "        print(split_text[indexes1[0]])\n",
        "        date = split_text[indexes1[0]+15].split(' ')\n",
        "        policy_detail = {'insurance_company':'ICICI LOMBARD GENERAL INSURANCE CO LTD',\n",
        "                         'policy_number': split_text[indexes[0]+25],\n",
        "                         'policy_inception_date':datetime.datetime.strptime(date[0] + ' ' + '00:01:00', '%d-%b-%Y %H:%M:%S'),\n",
        "                         'policy_expiry_date':datetime.datetime.strptime(date[2]+ ' ' + '23:59:59', '%d-%b-%Y %H:%M:%S')\n",
        "                         }\n",
        "    elif sub12.lower() in data.lower():\n",
        "        split_text = data.split('\\n')\n",
        "        indexes = [i for i, x in enumerate(split_text) if x == 'Policy No               :']\n",
        "        indexes1 = [i for i, x in enumerate(split_text) if x == 'Period of Insurance  :']\n",
        "        date = split_text[indexes1[0]+2].split(' ')\n",
        "        policy_detail = {'insurance_company': 'The Oriental Insurance Company Limited',\n",
        "                         'policy_number': split_text[indexes[0]+2],\n",
        "                         'policy_inception_date':datetime.datetime.strptime(date[4] + ' ' + '00:00:01', '%d/%m/%Y %H:%M:%S'),\n",
        "                         'policy_expiry_date':datetime.datetime.strptime(date[8]+ ' ' + '23:59:59', '%d/%m/%Y %H:%M:%S')\n",
        "                         }\n",
        "    elif sub13.lower() in data.lower():\n",
        "        split_text = data.split('\\n')\n",
        "        indexes = [i for i, x in enumerate(split_text) if x == 'Certificate of Insurance and Policy No.']\n",
        "        indexes1 = [i for i, x in enumerate(split_text) if x == 'Policy Period:']\n",
        "        date = split_text[indexes1[0]+1].split(' ')\n",
        "        policy_detail = {'insurance_company':'Royal Sundaram General Insurance Co. Limited',\n",
        "                         'policy_number': split_text[indexes[0]+1],\n",
        "                         'policy_inception_date':datetime.datetime.strptime(date[4] + ' ' + date[1], '%d/%m/%Y %H:%M:%S'),\n",
        "                         'policy_expiry_date':datetime.datetime.strptime(date[9]+ ' ' + date[6], '%d/%m/%Y %H:%M:%S')\n",
        "                         }\n",
        "    elif sub14.lower() in data.lower():\n",
        "        split_text = data.split('\\n')\n",
        "        indexes = [i for i, x in enumerate(split_text) if x == 'Policy No ']\n",
        "        indexes1 = [i for i, x in enumerate(split_text) if x == 'Gross Premium:']\n",
        "        date = split_text[indexes1[0]-1].split(' ')\n",
        "        policy_detail = {'insurance_company':'SHRIRAM GENERAL INSURANCE COMPANY LTD',\n",
        "                         'policy_number': split_text[indexes[0]+6],\n",
        "                         'policy_inception_date':datetime.datetime.strptime(date[7] + ' ' + '00:00:01', '%d/%m/%Y %H:%M:%S'),\n",
        "                         'policy_expiry_date':datetime.datetime.strptime(date[11]+ ' ' + '23:59:59', '%d/%m/%Y %H:%M:%S')\n",
        "                         }\n",
        "    elif sub15.lower() in data.lower():\n",
        "        string = find_between_r( data, \"Private Car Package PolicyPolicy Number\", \"Insured Details\" )\n",
        "        policy_n = string.split('Vehicle')\n",
        "        date = string.split(' ')\n",
        "        print(string)\n",
        "        date_from = find_between_r(string, 'CoverFrom: ', 'To:')\n",
        "        print(date_from)\n",
        "        policy_detail = {'insurance_company': 'THE NEW INDIA ASSURANCE CO. LTD',\n",
        "                         'policy_number': policy_n[0],\n",
        "                         'policy_inception_date':datetime.datetime.strptime(date_from, '%d/%m/%Y %I:%M:%S %p'),\n",
        "                         'policy_expiry_date':datetime.datetime.strptime(date[8]+ ' ' + '23:59:59', '%d/%m/%Y %H:%M:%S')\n",
        "                         }\n",
        "    elif sub16.lower() in data.lower():\n",
        "        string = find_between_r(data, \"Policy No.\", \"Registration No.\")\n",
        "        policy_n = string.split('\\xa0')\n",
        "        string1 = find_between_r(string, \"InsuranceFrom\", \"Particulars of Vehicle Insured\")\n",
        "        date = string1.split(' ')\n",
        "        policy_detail = {'insurance_company': 'UNITED INDIA INSURANCE COMPANY LIMITED',\n",
        "                         'policy_number': policy_n[0],\n",
        "                         'policy_inception_date':datetime.datetime.strptime(date[4] + ' '+'00:00:01', '%d/%m/%Y %H:%M:%S'),\n",
        "                         'policy_expiry_date':datetime.datetime.strptime(date[8]+ ' ' + '23:59:59', '%d/%m/%Y %H:%M:%S')\n",
        "                         }\n",
        "    elif sub17.lower() in data.lower():\n",
        "        data = data.replace(u'\\xa0', u' ')\n",
        "        string = find_between_r(data, \"Policy Details : Policy Number : \", \"Policy Period : From\")\n",
        "        string1 = find_between_r(data, \"Policy Period : From\", \"Premium Paid : \")\n",
        "        date = string1.split(' ')\n",
        "        policy_detail = {'insurance_company': 'Tata AIG General Insurance Company LTD',\n",
        "                         'policy_number': string,\n",
        "                         'policy_inception_date':datetime.datetime.strptime(date[1] + ' '+'00:00:01', '%d/%m/%Y %H:%M:%S'),\n",
        "                         'policy_expiry_date':datetime.datetime.strptime(date[5]+ ' ' + '23:59:59', '%d/%m/%Y %H:%M:%S')\n",
        "                         }\n",
        "\n",
        "    return data#, policy_detail"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FhocNurKlbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "51b56898-b079-4bb4-9e6d-27a397960aa4"
      },
      "source": [
        "#text, policy = pdfparser_new('Bajaj INsurance POlicy.pdf')\n",
        "#text, policy = pdfparser_new('Edelwise Insurance Policy.pdf')\n",
        "#text, policy = pdfparser_new('Go Digit Insurance Policy.pdf')\n",
        "#text, policy = pdfparser_new('Acko Insurance Policy.pdf')\n",
        "#text, policy = pdfparser_new('Bharti Axa Insurance Policy.pdf')\n",
        "#text, policy = pdfparser_new('HDFC Insurance Policy.pdf') #### Do not run it\n",
        "#text, policy = pdfparser_new('Iffco Tokio Insurance Policy.pdf')\n",
        "#text, policy = pdfparser_new('Future Generali Insurance Policy.pdf')\n",
        "#text, policy = pdfparser_new('National Insurance Policy.pdf')\n",
        "#text = pdfparser_new('Kotak Insurance Policy.pdf')     #### Do not run it\n",
        "#text, policy = pdfparser_new('ICICI Insurance Policy.pdf')\n",
        "#text, policy = pdfparser_new('Oriental Insurance Policy.pdf')\n",
        "#text, policy = pdfparser_new('Royal Sundaram Insurance Policy.pdf')\n",
        "#text, policy = pdfparser_new('Shri Ram Insurance Policy.pdf')\n",
        "#text, policy = pdfparser_new('The New India Insurance Policy.pdf')\n",
        "#text, policy = pdfparser_new('United Insurance Policy.pdf')\n",
        "#text, policy = pdfparser_new('TATA AIG Insurance Policy.pdf')\n",
        "text = pdfparser_new('SBI Insurance Policy.pdf')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-a9a804763ab4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#text, policy = pdfparser_new('United Insurance Policy.pdf')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#text, policy = pdfparser_new('TATA AIG Insurance Policy.pdf')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpdfparser_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SBI Insurance Policy.pdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-6739eefd8eb7>\u001b[0m in \u001b[0;36mpdfparser_new\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpdfparser_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mrsrcmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPDFResourceManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mretstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'SBI Insurance Policy.pdf'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBJytk47KQoz"
      },
      "source": [
        "data = \"/content/sample_data/UNITED_INDIA_INSURANCE (88).pdf_page_2.jpg\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "EbkkfTJONI_p",
        "outputId": "57fe3d8f-98fa-4f9b-fb19-e21027d3422c"
      },
      "source": [
        "!pip install pdfminer.six"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20211012-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 5.3 MB/s \n",
            "\u001b[?25hCollecting cryptography\n",
            "  Downloading cryptography-35.0.0-cp36-abi3-manylinux_2_24_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 51.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from pdfminer.six) (3.0.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six) (2.20)\n",
            "Installing collected packages: cryptography, pdfminer.six\n",
            "Successfully installed cryptography-35.0.0 pdfminer.six-20211012\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pdfminer"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbZopeJzWBU1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d53d27bb-2884-44af-b3d8-dc6c215a72fe"
      },
      "source": [
        "!pdf2txt.py \"/content/sample_data/UNITED_INDIA_INSURANCE (88).pdf_page_2.jpg\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pdf2txt.py\", line 224, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/bin/pdf2txt.py\", line 218, in main\n",
            "    outfp = extract_text(**vars(A))\n",
            "  File \"/usr/local/bin/pdf2txt.py\", line 86, in extract_text\n",
            "    pdfminer.high_level.extract_text_to_fp(fp, **locals())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pdfminer/high_level.py\", line 107, in extract_text_to_fp\n",
            "    caching=not disable_caching):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pdfminer/pdfpage.py\", line 147, in get_pages\n",
            "    doc = PDFDocument(parser, password=password, caching=caching)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pdfminer/pdfdocument.py\", line 726, in __init__\n",
            "    raise PDFSyntaxError('No /Root object! - Is this really a PDF?')\n",
            "pdfminer.pdfparser.PDFSyntaxError: No /Root object! - Is this really a PDF?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "jG4ScM_ddaH-",
        "outputId": "a2e6b504-12a3-4dde-dcc0-6a6a12f583fe"
      },
      "source": [
        "!pip3 install table_ocr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting table_ocr\n",
            "  Downloading table_ocr-0.2.5-py3-none-any.whl (33.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 33.4 MB 3.6 kB/s \n",
            "\u001b[?25hCollecting pytesseract~=0.3\n",
            "  Downloading pytesseract-0.3.8.tar.gz (14 kB)\n",
            "Requirement already satisfied: numpy~=1.19 in /usr/local/lib/python3.7/dist-packages (from table_ocr) (1.19.5)\n",
            "Requirement already satisfied: requests>=2 in /usr/local/lib/python3.7/dist-packages (from table_ocr) (2.23.0)\n",
            "Collecting opencv-python~=4.2\n",
            "  Downloading opencv_python-4.5.3.56-cp37-cp37m-manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 49.9 MB 16 kB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from pytesseract~=0.3->table_ocr) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2->table_ocr) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2->table_ocr) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2->table_ocr) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2->table_ocr) (3.0.4)\n",
            "Building wheels for collected packages: pytesseract\n",
            "  Building wheel for pytesseract (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytesseract: filename=pytesseract-0.3.8-py2.py3-none-any.whl size=14072 sha256=d5f190a90c278f7e425e0df5f29f3822c07208eded946245a3de6dbab4cd7920\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/89/b9/3f11250225d0f90e5454fcc30fd1b7208db226850715aa9ace\n",
            "Successfully built pytesseract\n",
            "Installing collected packages: pytesseract, opencv-python, table-ocr\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.1.2.30\n",
            "    Uninstalling opencv-python-4.1.2.30:\n",
            "      Successfully uninstalled opencv-python-4.1.2.30\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed opencv-python-4.5.3.56 pytesseract-0.3.8 table-ocr-0.2.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cv2"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lQlpG7YdaCL",
        "outputId": "de0b3c55-8b4d-4c1e-afa8-b160c943c511"
      },
      "source": [
        "!python3 -m table_ocr.demo \\\n",
        "\"https://raw.githubusercontent.com/eihli/image-table-ocr/master/resources/test_data/3dd2b79b-9160-403d-9967-af893d17b580.png\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/table_ocr/demo/__main__.py\", line 51, in <module>\n",
            "    csv_output = main(sys.argv[1])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/table_ocr/demo/__main__.py\", line 23, in main\n",
            "    image_tables = table_ocr.extract_tables.main([image_filepath])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/table_ocr/extract_tables/__init__.py\", line 57, in main\n",
            "    tables = find_tables(image)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/table_ocr/extract_tables/__init__.py\", line 8, in find_tables\n",
            "    blurred = cv2.GaussianBlur(image, BLUR_KERNEL_SIZE, STD_DEV_X_DIRECTION, STD_DEV_Y_DIRECTION)\n",
            "cv2.error: OpenCV(4.5.3) /tmp/pip-req-build-l1r0y34w/opencv/modules/imgproc/src/smooth.dispatch.cpp:617: error: (-215:Assertion failed) !_src.empty() in function 'GaussianBlur'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Otsg1NYceJo0",
        "outputId": "967b9f9e-978e-4b30-a6f1-45b3ef80ca69"
      },
      "source": [
        "!git clone https://github.com/eihli/image-table-ocr.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'image-table-ocr'...\n",
            "remote: Enumerating objects: 609, done.\u001b[K\n",
            "remote: Total 609 (delta 0), reused 0 (delta 0), pack-reused 609\u001b[K\n",
            "Receiving objects: 100% (609/609), 166.27 MiB | 33.30 MiB/s, done.\n",
            "Resolving deltas: 100% (207/207), done.\n",
            "Checking out files: 100% (97/97), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMkCdwKodZ6X",
        "outputId": "37dcf254-76ad-4e02-99c5-646f2e1268bd"
      },
      "source": [
        "!sudo apt-get install tesseract-ocr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 4,795 kB of archives.\n",
            "After this operation, 15.8 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-eng all 4.00~git24-0e00fe6-1.2 [1,588 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-osd all 4.00~git24-0e00fe6-1.2 [2,989 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr amd64 4.00~git2288-10f4998a-2 [218 kB]\n",
            "Fetched 4,795 kB in 1s (3,700 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 155047 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_4.00~git24-0e00fe6-1.2_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_4.00~git24-0e00fe6-1.2_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.00~git2288-10f4998a-2_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.00~git2288-10f4998a-2) ...\n",
            "Setting up tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ...\n",
            "Setting up tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ...\n",
            "Setting up tesseract-ocr (4.00~git2288-10f4998a-2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1XYj9YhdZy7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rP4auy7AdZp8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlEiL47cdZh_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEqeCXI4N9KP"
      },
      "source": [
        "from io import StringIO\n",
        "\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.pdfdocument import PDFDocument\n",
        "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "from pdfminer.pdfparser import PDFParser\n",
        "\n",
        "def convert_pdf_to_string(file_path):\n",
        "\n",
        "\toutput_string = StringIO()\n",
        "\twith open(file_path, 'rb') as in_file:\n",
        "\t    parser = PDFParser(in_file)\n",
        "\t    doc = PDFDocument(parser)\n",
        "\t    rsrcmgr = PDFResourceManager()\n",
        "\t    device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
        "\t    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
        "\t    for page in PDFPage.create_pages(doc):\n",
        "\t        interpreter.process_page(page)\n",
        "\n",
        "\treturn(output_string.getvalue())\n",
        "\n",
        "                \n",
        "def convert_title_to_filename(title):\n",
        "    filename = title.lower()\n",
        "    filename = filename.replace(' ', '_')\n",
        "    return filename\n",
        "\n",
        "\n",
        "def split_to_title_and_pagenum(table_of_contents_entry):\n",
        "    title_and_pagenum = table_of_contents_entry.strip()\n",
        "    \n",
        "    title = None\n",
        "    pagenum = None\n",
        "    \n",
        "    if len(title_and_pagenum) > 0:\n",
        "        if title_and_pagenum[-1].isdigit():\n",
        "            i = -2\n",
        "            while title_and_pagenum[i].isdigit():\n",
        "                i -= 1\n",
        "\n",
        "            title = title_and_pagenum[:i].strip()\n",
        "            pagenum = int(title_and_pagenum[i:].strip())\n",
        "        \n",
        "    return title, pagenum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pAj6M98FN_Li",
        "outputId": "193bca95-1037-407f-fb51-22a6d22c4fa8"
      },
      "source": [
        "convert_pdf_to_string(\"/content/sample_data/UNITED_INDIA_INSURANCE (88).pdf\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1 / 4\\n\\nUNITED INDIA INSURANCE COMPANY LIMITEDSCO 123 124 GROUND FLOOR MADHYA MARG, SECTOR 17-B, MOTOR VERTICAL OFFICESECTOR 17 B CHANDIGARH - 160017 CHANDIGARH U.T. PH:  (172) 2726953   FAX:   (172) 5045861   EMAIL:  PRIVATE CAR - PACKAGE POLICYUIN: IRDAN545RP0047V01199900 POLICY NO.:1122003121P104282894VEHICLE NO.:CH - 01 - BG - 8463   PERIOD OF INSURANCE From 15:00 Hrs of 06/08/2021 To Midnight of 05/08/2022    Insured MR SUNIL KUMARHNO 2560 B SECTOR 39 C 160036 CHANDIGARH CHANDIGARH U.T.CONTACT NUMBER: 7973103064 (M)IMPORTANT NOTICE: KINDLY UPDATE YOUR AADHAAR NO. AND PAN/FORM 60. PLEASE IGNORE IF ALREADY UPDATED.   Agent Name:SHANTI DEVI Agent Code:AGD0124378 Mobile/Landline Number/Email:9876883666rameshjoshi_uii@yahoo.com  The genuineness of the policy can be verified through \"Verify Your Policy\" link at www.uiic.co.in.  For any Information, Service Requests, Claim intimation and Grievances please write to 112200@uiic.co.in Download Customer App(www.uiic.co.in). REGD. & HEAD OFFICE, 24, WHITES ROAD, CHENNAI - 600014.Website: http://www.uiic.co.inPrinted By : CUSTOMER @ 06/08/2021 10:46:53 AM \\x0c2 / 4\\n\\n UNITED INDIA INSURANCE COMPANY LIMITED CERTIFICATE OF INSURANCEPRIVATE CAR PACKAGE POLICYUIN: IRDAN545RP0047V01199900(FORM 51 OF CENTRAL MOTOR VEHICLE RULES 1989)Policy No.1122003121P104282894  Customer Id23063098248  Name of the Insured MR SUNIL KUMAR  Address of the InsuredHNO 2560 B SECTOR 39 C 160036CHANDIGARH CHANDIGARH U.T.Business/OccupationOthers  Telephone: Certificate Number1122003121P104282894  Issuing Office AddressCode112200  SCO 123 124 GROUND FLOOR MADHYA MARG, SECTOR 17-B, MOTORVERTICAL OFFICE SECTOR 17 B 160017 CHANDIGARH CHANDIGARHU.T. Telephone(172) 2726953  Insured\\'s Declared Value    255600 Period of InsuranceFrom 15:00 Hrs of 06/08/2021 To Midnight of 05/08/2022Particulars of Vehicle InsuredRegistration No.ObsoleteVehicleEngine No.Chassis No.Make/ ModelModelType of BodyYear ofMfgCubicCapacity/KWSeatingincludingdriverVehicleTrailer( if any)CH - 01 - BG -8463 No  G3HAGM423816 MALA351ALGM465816*C HYUNDAI /EON(2011 - )  SPORTZ  HATCHBACK 2016  814  5  Registration AuthorityGeographical Area CH01 CHANDIGARH  INDIA              Amount in words:   Five thousand seven hundred sixteen rupees onlyPersons or classes of persons entitled to driveAny person including Insured provided that a person hold an effective driving licence at the time of accident and is not disqualified fromholding or obtaining such a licence. Provided also that the person holding an effective Learner\\'s Licence may also drive the vehicle andsuch a person satisfies the requirements of Rule 3 of Central Motor Vehicle Rule, 1989.Limitations as to use The policy covers use of the vehicle for any purpose other than a) Hire or Rewardb) Carriage of Goods (other than samples or personal luggage)c) Organized Racing d) Pace Making e) Speed Testing and Reliability Trails f) Use in connection with Motor TradePremium:  4,844.00CGST(9%):  436.00UTGST(9%):  436.00Stamp Duty:  1.00Total (Rounded Off) : 5,716.00Receipt Number : 10111220021104534911Receipt Date: 06/08/2021DebitNote Number: Document Date: Limits of LiabilityUnder Section II-I (i) Death or bodily injury in respect of any oneaccident; As per Motor Vehicles Act 1988Under Section II-I (ii) Damage to third party property in respect ofany one claim or series of claims arising out of one event: 750000Agency/Broker Code:SHANTI DEVIDirect Business: Development Officer Code:AGD0124378 Subject to IMT Endorsement No.s, terms and conditions printed herein / attached hereto 22,28I/We hereby certify that the policy to which the certificate relates as well asthe certificate of insurance are issued in accordance with provisions ofChapter X & XI of M.V Act, 1988.For and On behalf ofUnited India Insurance Co. Ltd.Duly Constituted Attorney:Date of Issue: 06/08/2021\\x0c3 / 4\\n\\nPolicy No: 1122003121P104282894 PRIVATE CAR PACKAGE POLICYUIN: IRDAN545RP0047V01199900Policy No. 1122003121P104282894Previous Policy No. NILInsured DetailsCustomer Id 23063098248Name  MR SUNIL KUMARTel (O) Tel (R) Fax: 0 Mobile: 7973103064 Email ims.chd18@gmail.comBusiness / Occupation OthersPeriod of InsuranceFrom 15:00 Hrs of 06/08/2021 To Midnight of 05/08/2022Co-InsuranceType  Particulars of Vehicle InsuredRegistration No.ObsoleteVehicleEngine No.Chassis No.Make/ Model ModelYear ofMfgType ofBodyCubicCapacity/KWSeatingincludingdriverVehicleTrailer( ifany)  CH - 01 - BG- 8463 No   G3HAGM423816 MALA351ALGM465816*CHYUNDAI /EON(2011 - )  SPORTZ  2016 HATCHBACK  814  5Insured\\'s Declared ValueFor VehicleFor TrailerNon Electrical AccessoriesElectrical/Electronic AccessoriesCNG UnitLPG UnitTotal Value  255600  0  0  0  0  0  255600RegistrationAuthorityAuto Association Membership No.Geographical AreaExtension  CH01CHANDIGARH INDIA             Amount in words:   Five thousand seven hundred sixteen rupees onlyPersons or classes of persons entitled to drive Persons or classes of persons entitled to driveAny person including Insured provided that a person hold an effective driving licence at the time of accident and is not disqualified fromholding or obtaining such a licence. Provided also that the person holding an effective Learner\\'s Licence may also drive the vehicle andsuch a person satisfies the requirements of Rule 3 of Central Motor Vehicle Rule, 1989.Limitations as to use The Policy covers use of the Vehicle for any purpose other than a) Hire or Rewardb) Carriage of Goods (other than samples or personal luggage)c) Organized Racing d) Pace Making e) Speed Testing and Reliability Trails f) Use in connection with Motor TradeLimits of Liability As narrated in the Certificate of Insurance attached herewith. EXCLUSIONS : 1) Any accidental loss or damage or Liability / caused or sustained or incurrred outside the geographical area. 2) Any claim arising out of any contractual liability. 3) Any accidental loss or damage to any property whatsoever or any loss or any expense whatsoever resulting or arising there from or anyconsequential loss. 4) Any liability of whatsoever nature directly or indirectly caused by or constituted to or by or arising out of ionizing radiations orcontamination by radioactivity from any nuclear fuel. For the purpose of this exception, combustion shall include any self-sustainingprocess of nuclear fission. 5) Any accidental loss or damage or liability directly or indirectly caused by or contributed to, by or arising from nuclear weapons material. 6) Any accidental loss, damage or liability directly or indirectly or proximatley or remotely occasioned by contributed to, by or traceable toor arising out of or in connection with war, invasion, act of foreign enemies, hostilities or warlike operations (whether before or afterdeclaration of war), civil war, mutiny, rebellion, military or usurped power, or by any direct or indirect consequence of any of the saidoccurrences or nay consequence thereof and in default of such proof, the Company shall not be liable to make any payment in respect ofsuch a claim.Personal Accident covers for Owner-Driver CSI:  1500000This policy is subject to terms and conditions and IMT Endorsement Nos. printed herein/ attached hereto 22,28Imposed Excess  0 Voluntary Excess  0Compulsory Excess  1000\\x0c4 / 4\\n\\nSCHEDULE OF PREMIUM (IN )OWN DAMAGEBasic premium on Vehicle and AccessoriesA. Basic - OD2,446.86Total2,446.86Gross OD(A)2,447.00LIABILITY  B. Basic - TP2,072.00Total2,072.00Add :Compulsory PA for Owner Driver275.00LL to Paid Driver IMT 2850.00Sub Total (Additions)325.00Gross TP(B)2,397.00Gross OD & TP: (A) + (B)4,844.00TERMS AND CONDITIONSAs per the Indian Motor Tariff. Personal copy of the same is available free of cost on request. Further, the Indian Motor Tariff is alsoavailable and displayed at all United India Insurance Company offices and on UIIC website : www.uiic.co.in Disclaimer : The Policy stands cancelled or void in the event of Cheque Dishonor. The Company may cancel the policy by sending 7 daysnotice in case of any fraud or misrepresentation, non-disclosure of material fact or non-co-operation of the insured.IMPORTANT NOTICETHE INSURED IS NOT INDEMNIFIED IF THE VEHICLE IS USED OR DRIVEN OTHERWISE THAN IN ACCORDANCE WITH THIS SCHEDULE. ANYPAYMENT MADE BY THE COMPANY BY REASON OF WIDER TERMS APPEARING IN THE CERTIFICATE IN ORDER TO COMPLY WITH THE MOTORVEHICLES ACT, 1988 IS RECOVERABLE FROM THE INSURED. SEE THE CLAUSE HEADED \"AVOIDANCE OF CERTAIN TERMS AND RIGHT OFRECOVERY\" . FOR LEGAL INTERPRETATION, ENGLISH VERSION WILL HOLD GOOD. Premium4,844.00ReceiptNumber 10111220021104534911Agency/Broker Code:SHANTI DEVI AGD0124378CGST(9%):UTGST(9%):  436.00436.00ReceiptDate  06/08/2021Direct Business: Stamp Duty  1.00DebitNoteNumber Development Officer Code: Total (Rounded Off)  5,716.00DocumentDate     Customer GST/UIN No.: Office GST No.:04AAACU5552C1ZR SAC Code:997134 Invoice No. & Date:3121I104282894 & 06/08/2021 Amount Subject to Reverse Charges-NILAnti Money Laundering Clause:-In the event of a claim under the policy exceeding  1 lakh or a claim for refund of premium exceeding 1 lakh, the insured will comply with the provisions of AML policy of the company. The AML policy is available in all our operating offices aswell as Company\\'s web site.LET US JOIN THE FIGHT AGAINST CORRUPTION. PLEASE TAKE THE PLEDGE AT https://pledge.cvc.nic.in.Date of Proposal and Declaration: 06/08/2021IN WITNESS WHEREOF, this policy has been signed at DO 5 CHANDIGARH 112200 on this 06th day of August 2021. For and On behalf ofUnited India Insurance Co. Ltd.Affix PolicyStamp here.Duly Constituted Attorney:Underwritten By - RAV37287 ( DO UNDERWRITER )This is a system generated document and any manual alteration / correction / overwriting in the document will make it invalid.\\x0c'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyw6ugvaO-9v",
        "outputId": "50782bca-b652-43ac-a6ee-2bc6a6c82ca9"
      },
      "source": [
        "!pip install easyocr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting easyocr\n",
            "  Downloading easyocr-1.4.1-py3-none-any.whl (63.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 63.6 MB 19 kB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from easyocr) (1.9.0+cu111)\n",
            "Collecting opencv-python-headless\n",
            "  Downloading opencv_python_headless-4.5.3.56-cp37-cp37m-manylinux2014_x86_64.whl (37.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 37.1 MB 48 kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from easyocr) (1.4.1)\n",
            "Requirement already satisfied: Pillow<8.3.0 in /usr/local/lib/python3.7/dist-packages (from easyocr) (7.1.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from easyocr) (3.13)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from easyocr) (1.19.5)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from easyocr) (0.16.2)\n",
            "Collecting python-bidi\n",
            "  Downloading python_bidi-0.4.2-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.7/dist-packages (from easyocr) (0.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->easyocr) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from python-bidi->easyocr) (1.15.0)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->easyocr) (3.2.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->easyocr) (2.6.3)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->easyocr) (1.1.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->easyocr) (2.4.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->easyocr) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->easyocr) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->easyocr) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->easyocr) (1.3.2)\n",
            "Installing collected packages: python-bidi, opencv-python-headless, easyocr\n",
            "Successfully installed easyocr-1.4.1 opencv-python-headless-4.5.3.56 python-bidi-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "JGoUnKL4O-8C",
        "outputId": "9e66371d-a238-48f9-88c6-0a2ae85b26bc"
      },
      "source": [
        "import easyocr\n",
        "reader = easyocr.Reader(['ch_sim','en']) # this needs to run only once to load the model into memory\n",
        "result = reader.readtext('/content/sample_data/UNITED_INDIA_INSURANCE (88).pdf_page_2.jpg', detail = 0)\n",
        "\",\".join(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CUDA not available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Policy No: 1122003121P104282894,~,UITCD ID旧,PRIVATE,CAR PACKAGE POLICY,UIN: IRDAN54SRP0047V01199900,Dolicy No=,1122003121104282894,Previous PolicyNo.,NIL,Customer Id,23063098248,IInsured Details,Name,MR SUNIL KUMAR,Fax:,0,ITel (0),ITel (R),Mobile:,7973103064,Email,ims.chd18,gmail. com_,Business,Occupation,Others,Period Of Insurance,From 15:00 Hrs of 06/08,2021,To Midnight of 05/08,2022,Co-Insurance,Type,Particulars Of Vehicle Insured,Registration NO,Obsolete,Engine,NO -,Chassis,No,Make/ Model Model,Kear 0#,Type of,Cubic,Seating,Vehicle,Irailer,Vehicle,Mfg,Body,Iapacity /KWincluding,if,driver,any,HYUNDAI,CH,01,BG,No,EON(2O1I,2016,HATCHBACK,814,5,8463,G3HAGM42381GMMALA35IALGM465816*,SPORTZ,Insured's Declared Valye,For Vehicle,For Trailer,Non Electrical Accessories,Electrical/ Electronic Accessories,CNG Unit,LPG Unit,Total Value,255600,255600,Registration,Auto Association Membership No。,Geographical Area,Extension,Authority_,CHOI,INDIA,CHANDIGARH,Amount in words:,Five thousand seven hundred,Sixteen rupees only,Persons or classes of persons entitled to drive Persons or classes Of persons entitled to drive,Any person including Insured provided that,a,person hold an effective driving licence at the time of accident and is not disqualified from,hholding or obtaining,a licence,Provided also that the person holding an effective Learners Licence may also drive the vehicle and,IsUCh,3,person Satisfies the requirements of Rule,3 Of Centra,Motor Vehicle,Rule,1989,Limitations as to Use,TThe Policy covers use of the Vehicle forany purpose other than,Hire or Reward,0,Carriage Of Goods (other than samples or personal luggage),Organized Racing,Pace Making,Speed Testing and Reliability Trails,[,Use in connection,with Motor Trade,[mits of Liabillty As narrated in the Certificate of Insurance attached,herewith_,EXCLUSIONS,1 ),accidental loss or damage or Liability,caused or sustained or incurrred outside the geographical area,Any claim arising out of any contractual liability,3,Any accidenta,loss or damage to any property Whatsoever orany Ioss orany expense whatsoever resulting,Or,arising there from,orany,Ionsequential Ioss,|4) Any liability of whatsoever nature directly or indirectly caused by or constituted to or by or arising out of ionizing radiations or,lcontamination by radioactivity from any nuclear fuel。,For the purpose of this exception, combustion shall include any self-sustaining,process ofnuclear fission,15) Any,accidental Ioss Or,damage or liability directly or indirectly caused by or contributed to, by or arising from nuclear weapons material_,5,Any accidental loss, damage or liability directly Or indirectly or proximatley or remotely occasioned by contributed tor,by ortraceable to,lorarising out of orin connection with war, invasion, act of foreign enemies, hostilities or warlike operations (whether before orafter,Ideclaration of war), civil War,mutiny, rebellion, military or usurped power or by,any,direct or indirect consequence ofany ofthe said,occurrences or nay consequence thereof and,in default of such proof,the Company shall not be liable to,make any payment in respect of,BUCh,a claim,Personal Accident covers for Owner Driver CSI:,1500000,This policyis Subject to terms and conditions and IMT Endorsement Nosprinted hereinLattached hereto 22,28 _,mposed Excess,Noluntary Excess,CompulsoryExcess,100,3/4,SUCh,AnY\""
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aT_zFrLTUT-",
        "outputId": "92a69027-ab81-4719-d2d4-8c24e70d60cd"
      },
      "source": [
        "!pip install camelot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting camelot\n",
            "  Downloading Camelot-12.06.29.tar.gz (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 5.4 MB/s \n",
            "\u001b[?25hCollecting SQLAlchemy<0.8.0,>=0.7.7\n",
            "  Downloading SQLAlchemy-0.7.10.tar.gz (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 33.7 MB/s \n",
            "\u001b[?25hCollecting Elixir>=0.7.1\n",
            "  Downloading Elixir-0.7.1.tar.gz (47 kB)\n",
            "\u001b[K     |████████████████████████████████| 47 kB 3.4 MB/s \n",
            "\u001b[?25hCollecting sqlalchemy-migrate>=0.7.1\n",
            "  Downloading sqlalchemy_migrate-0.13.0-py2.py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 40.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Jinja2>=2.5.5 in /usr/local/lib/python3.7/dist-packages (from camelot) (2.11.3)\n",
            "Requirement already satisfied: chardet>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from camelot) (3.0.4)\n",
            "Collecting xlwt==0.7.2\n",
            "  Downloading xlwt-0.7.2.zip (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 61.3 MB/s \n",
            "\u001b[?25hCollecting xlrd==0.7.1\n",
            "  Downloading xlrd-0.7.1.zip (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 44.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.5.5->camelot) (2.0.1)\n",
            "Collecting pbr>=1.8\n",
            "  Downloading pbr-5.6.0-py2.py3-none-any.whl (111 kB)\n",
            "\u001b[K     |████████████████████████████████| 111 kB 45.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlparse in /usr/local/lib/python3.7/dist-packages (from sqlalchemy-migrate>=0.7.1->camelot) (0.4.2)\n",
            "Collecting Tempita>=0.4\n",
            "  Downloading Tempita-0.5.2-py3-none-any.whl (12 kB)\n",
            "Collecting sqlalchemy-migrate>=0.7.1\n",
            "  Downloading sqlalchemy_migrate-0.12.0-py2.py3-none-any.whl (108 kB)\n",
            "\u001b[K     |████████████████████████████████| 108 kB 11.4 MB/s \n",
            "\u001b[?25h  Downloading sqlalchemy-migrate-0.11.0.tar.gz (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 65.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from sqlalchemy-migrate>=0.7.1->camelot) (4.4.2)\n",
            "Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy-migrate>=0.7.1->camelot) (1.15.0)\n",
            "Building wheels for collected packages: camelot, xlrd, xlwt, Elixir, SQLAlchemy, sqlalchemy-migrate\n",
            "  Building wheel for camelot (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for camelot: filename=Camelot-12.6.29-py3-none-any.whl size=3919092 sha256=fe57a195aee2b41ebc9d11784acdfa3701da53450a48c27a687ab27ef27b6dc6\n",
            "  Stored in directory: /root/.cache/pip/wheels/c2/4c/60/72ff7d0da04298357923c606570ca35b470233f8810cb3030c\n",
            "  Building wheel for xlrd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for xlrd: filename=xlrd-0.7.1-py3-none-any.whl size=117961 sha256=f746c86874b5668508c8c44e10ce8c8442a492b44bddb650230659277082e1c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/c8/2b/280a2f42fde2459872298da92a373483d9db35c092cb3ad010\n",
            "  Building wheel for xlwt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for xlwt: filename=xlwt-0.7.2-py3-none-any.whl size=120227 sha256=8e9babb9468e8d3acc02739e400fbc0795a0c59c9235107fafb1041b7e06a510\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/45/d5/054d43529d665ccfdf2c096f66a5b7c5c1512fafa960c3b3ee\n",
            "  Building wheel for Elixir (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Elixir: filename=Elixir-0.7.1-py3-none-any.whl size=53907 sha256=4ce944ee7315bb3a12db91a9094eb66b488cdb1e1e6074a04d43fb2239866b42\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/ad/80/a08c6d0b63ec1bbd128cdd1571d0fb1d692876434a59d1d1cc\n",
            "  Building wheel for SQLAlchemy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for SQLAlchemy: filename=SQLAlchemy-0.7.10-py3-none-any.whl size=696626 sha256=b084d64e90cc4e8f6a20f929bb0eff3e51c589a4c3c445856dc08f751762bc1e\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/34/2c/c2e1046d87bcde082cb952c5111830ea81802617b81132a7b5\n",
            "  Building wheel for sqlalchemy-migrate (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlalchemy-migrate: filename=sqlalchemy_migrate-0.11.0-py3-none-any.whl size=108875 sha256=258b98899b6c5932219003132a350a9bb7076e5cabf8a4075917e39e6b4e0596\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/af/5d/cbdc083c2eda779575d22b5d15d7c6a8905e2b9b2c4a1a3da1\n",
            "Successfully built camelot xlrd xlwt Elixir SQLAlchemy sqlalchemy-migrate\n",
            "Installing collected packages: Tempita, SQLAlchemy, pbr, xlwt, xlrd, sqlalchemy-migrate, Elixir, camelot\n",
            "  Attempting uninstall: SQLAlchemy\n",
            "    Found existing installation: SQLAlchemy 1.4.25\n",
            "    Uninstalling SQLAlchemy-1.4.25:\n",
            "      Successfully uninstalled SQLAlchemy-1.4.25\n",
            "  Attempting uninstall: xlwt\n",
            "    Found existing installation: xlwt 1.3.0\n",
            "    Uninstalling xlwt-1.3.0:\n",
            "      Successfully uninstalled xlwt-1.3.0\n",
            "  Attempting uninstall: xlrd\n",
            "    Found existing installation: xlrd 1.1.0\n",
            "    Uninstalling xlrd-1.1.0:\n",
            "      Successfully uninstalled xlrd-1.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "excalibur-py 0.4.3 requires SQLAlchemy>=1.2.12, but you have sqlalchemy 0.7.10 which is incompatible.\u001b[0m\n",
            "Successfully installed Elixir-0.7.1 SQLAlchemy-0.7.10 Tempita-0.5.2 camelot-12.6.29 pbr-5.6.0 sqlalchemy-migrate-0.11.0 xlrd-0.7.1 xlwt-0.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaz_UkbKU8_A",
        "outputId": "440ffbba-a2a9-422c-c514-a60da6291696"
      },
      "source": [
        "!pip uninstall camelot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: Camelot 12.6.29\n",
            "Uninstalling Camelot-12.6.29:\n",
            "  Would remove:\n",
            "    /usr/local/bin/camelot_admin\n",
            "    /usr/local/bin/camelot_example\n",
            "    /usr/local/bin/camelot_mini_example\n",
            "    /usr/local/lib/python3.7/dist-packages/Camelot-12.6.29.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/camelot/*\n",
            "    /usr/local/lib/python3.7/dist-packages/camelot_example/*\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/action_button.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/admin_classes.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/budget.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/camelot-new-project.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/camelot_qt_linguist.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/collection_proxy.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/color.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/cpd_installer.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/editors/ChartEditor_disabled.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/editors/ChartEditor_editable.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/enumeration.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/field_attributes.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/file_delegate.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/final_report.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/image.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/interval_column_delegate.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/main-window.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/manytomany.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/manytoone.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/movie-table.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/navigation-pane.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/new-form.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/new-record.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/onetomany.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/picture1.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/picture2.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/picture3.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/picture4.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/picture5.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/picture6.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/picture7.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/picture8.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/plaintext.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/rating.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/richtext.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/simple_report.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/spyder-new-project.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/start-spyder.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/table-view.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/template_document_word.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/template_document_word_save_as.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/toolbar.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/unittest_dream.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_static/virtualaddress_editor.png\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/_templates/layout.html\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/advanced/debug.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/advanced/deployment.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/advanced/development.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/advanced/i18n.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/advanced/index.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/advanced/permissions.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/advanced/unittests.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/cep/actions_new.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/cep/index.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/cep/table_view.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/contents.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/copyright.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/doc/actions.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/doc/admin.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/doc/application_admin.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/doc/calculated_fields.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/doc/charts.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/doc/data_model.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/doc/delegates.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/doc/documents.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/doc/entity_admin.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/doc/faq.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/doc/field_attributes.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/doc/fields.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/doc/fixtures.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/doc/forms.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/doc/index.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/doc/install.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/doc/manage.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/doc/models.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/doc/object_admin.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/doc/reports.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/doc/schemas.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/doc/shortcuts.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/doc/threads.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/doc/under_the_hood.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/doc/validators.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/doc/views.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/index.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/license.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/migrate/11-12-30.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/migrate/index.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/news.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/tutorial/importer.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/tutorial/index.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/tutorial/reporting.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/doc/sphinx/source/tutorial/videostore.rst\n",
            "    /usr/local/lib/python3.7/dist-packages/test/*\n",
            "  Would not remove (might be manually added):\n",
            "    /usr/local/lib/python3.7/dist-packages/camelot/__main__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/camelot/__version__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/camelot/backends/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/camelot/backends/ghostscript_backend.py\n",
            "    /usr/local/lib/python3.7/dist-packages/camelot/backends/image_conversion.py\n",
            "    /usr/local/lib/python3.7/dist-packages/camelot/backends/poppler_backend.py\n",
            "    /usr/local/lib/python3.7/dist-packages/camelot/cli.py\n",
            "    /usr/local/lib/python3.7/dist-packages/camelot/core.py\n",
            "    /usr/local/lib/python3.7/dist-packages/camelot/handlers.py\n",
            "    /usr/local/lib/python3.7/dist-packages/camelot/image_processing.py\n",
            "    /usr/local/lib/python3.7/dist-packages/camelot/io.py\n",
            "    /usr/local/lib/python3.7/dist-packages/camelot/parsers/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/camelot/parsers/base.py\n",
            "    /usr/local/lib/python3.7/dist-packages/camelot/parsers/lattice.py\n",
            "    /usr/local/lib/python3.7/dist-packages/camelot/parsers/stream.py\n",
            "    /usr/local/lib/python3.7/dist-packages/camelot/plotting.py\n",
            "    /usr/local/lib/python3.7/dist-packages/camelot/utils.py\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled Camelot-12.6.29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qJQQeK_VP87",
        "outputId": "9e7de1d1-52d9-4677-c805-6f7667b60b96"
      },
      "source": [
        "!pip install camelot-py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: camelot-py in /usr/local/lib/python3.7/dist-packages (0.10.1)\n",
            "Requirement already satisfied: pdfminer.six>=20200726 in /usr/local/lib/python3.7/dist-packages (from camelot-py) (20211012)\n",
            "Requirement already satisfied: PyPDF2>=1.26.0 in /usr/local/lib/python3.7/dist-packages (from camelot-py) (1.26.0)\n",
            "Requirement already satisfied: chardet>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from camelot-py) (3.0.4)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from camelot-py) (0.8.9)\n",
            "Requirement already satisfied: click>=6.7 in /usr/local/lib/python3.7/dist-packages (from camelot-py) (7.1.2)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.7/dist-packages (from camelot-py) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from camelot-py) (1.19.5)\n",
            "Requirement already satisfied: openpyxl>=2.5.8 in /usr/local/lib/python3.7/dist-packages (from camelot-py) (2.5.9)\n",
            "Requirement already satisfied: jdcal in /usr/local/lib/python3.7/dist-packages (from openpyxl>=2.5.8->camelot-py) (1.4.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl>=2.5.8->camelot-py) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.4->camelot-py) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.4->camelot-py) (2018.9)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from pdfminer.six>=20200726->camelot-py) (35.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23.4->camelot-py) (1.15.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six>=20200726->camelot-py) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six>=20200726->camelot-py) (2.20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urTQWfnXVP04",
        "outputId": "93fbc7d0-1549-4063-ab2c-8ff1fd5de523"
      },
      "source": [
        "!pip install camelot-py[cv]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: camelot-py[cv] in /usr/local/lib/python3.7/dist-packages (0.10.1)\n",
            "Requirement already satisfied: pdfminer.six>=20200726 in /usr/local/lib/python3.7/dist-packages (from camelot-py[cv]) (20211012)\n",
            "Requirement already satisfied: PyPDF2>=1.26.0 in /usr/local/lib/python3.7/dist-packages (from camelot-py[cv]) (1.26.0)\n",
            "Requirement already satisfied: openpyxl>=2.5.8 in /usr/local/lib/python3.7/dist-packages (from camelot-py[cv]) (2.5.9)\n",
            "Requirement already satisfied: click>=6.7 in /usr/local/lib/python3.7/dist-packages (from camelot-py[cv]) (7.1.2)\n",
            "Requirement already satisfied: chardet>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from camelot-py[cv]) (3.0.4)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from camelot-py[cv]) (1.19.5)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from camelot-py[cv]) (0.8.9)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.7/dist-packages (from camelot-py[cv]) (1.1.5)\n",
            "Requirement already satisfied: opencv-python>=3.4.2.17 in /usr/local/lib/python3.7/dist-packages (from camelot-py[cv]) (4.1.2.30)\n",
            "Requirement already satisfied: pdftopng>=0.2.3 in /usr/local/lib/python3.7/dist-packages (from camelot-py[cv]) (0.2.3)\n",
            "Requirement already satisfied: ghostscript>=0.7 in /usr/local/lib/python3.7/dist-packages (from camelot-py[cv]) (0.7)\n",
            "Requirement already satisfied: setuptools>=38.6.0 in /usr/local/lib/python3.7/dist-packages (from ghostscript>=0.7->camelot-py[cv]) (57.4.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl>=2.5.8->camelot-py[cv]) (1.1.0)\n",
            "Requirement already satisfied: jdcal in /usr/local/lib/python3.7/dist-packages (from openpyxl>=2.5.8->camelot-py[cv]) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.4->camelot-py[cv]) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.4->camelot-py[cv]) (2.8.2)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from pdfminer.six>=20200726->camelot-py[cv]) (35.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23.4->camelot-py[cv]) (1.15.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six>=20200726->camelot-py[cv]) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six>=20200726->camelot-py[cv]) (2.20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqywU8SEVPiW",
        "outputId": "3e5d1d6d-5884-4e23-8b21-7ab056f34fdb"
      },
      "source": [
        "pip install camelot-py[all]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: camelot-py[all] in /usr/local/lib/python3.7/dist-packages (0.10.1)\n",
            "Requirement already satisfied: PyPDF2>=1.26.0 in /usr/local/lib/python3.7/dist-packages (from camelot-py[all]) (1.26.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from camelot-py[all]) (0.8.9)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.7/dist-packages (from camelot-py[all]) (1.1.5)\n",
            "Requirement already satisfied: chardet>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from camelot-py[all]) (3.0.4)\n",
            "Requirement already satisfied: openpyxl>=2.5.8 in /usr/local/lib/python3.7/dist-packages (from camelot-py[all]) (2.5.9)\n",
            "Requirement already satisfied: pdfminer.six>=20200726 in /usr/local/lib/python3.7/dist-packages (from camelot-py[all]) (20211012)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from camelot-py[all]) (1.19.5)\n",
            "Requirement already satisfied: click>=6.7 in /usr/local/lib/python3.7/dist-packages (from camelot-py[all]) (7.1.2)\n",
            "Requirement already satisfied: opencv-python>=3.4.2.17 in /usr/local/lib/python3.7/dist-packages (from camelot-py[all]) (4.1.2.30)\n",
            "Requirement already satisfied: pdftopng>=0.2.3 in /usr/local/lib/python3.7/dist-packages (from camelot-py[all]) (0.2.3)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from camelot-py[all]) (3.2.2)\n",
            "Requirement already satisfied: ghostscript>=0.7 in /usr/local/lib/python3.7/dist-packages (from camelot-py[all]) (0.7)\n",
            "Requirement already satisfied: setuptools>=38.6.0 in /usr/local/lib/python3.7/dist-packages (from ghostscript>=0.7->camelot-py[all]) (57.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->camelot-py[all]) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->camelot-py[all]) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->camelot-py[all]) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->camelot-py[all]) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=2.2.3->camelot-py[all]) (1.15.0)\n",
            "Requirement already satisfied: jdcal in /usr/local/lib/python3.7/dist-packages (from openpyxl>=2.5.8->camelot-py[all]) (1.4.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl>=2.5.8->camelot-py[all]) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.4->camelot-py[all]) (2018.9)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from pdfminer.six>=20200726->camelot-py[all]) (35.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six>=20200726->camelot-py[all]) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six>=20200726->camelot-py[all]) (2.20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITRHA_0cU808",
        "outputId": "19309f78-d546-4954-b3a6-087ceed3f4f6"
      },
      "source": [
        "!pip install tabula-py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tabula-py\n",
            "  Downloading tabula_py-2.3.0-py3-none-any.whl (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 98 kB/s \n",
            "\u001b[?25hCollecting distro\n",
            "  Downloading distro-1.6.0-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tabula-py) (1.19.5)\n",
            "Requirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.7/dist-packages (from tabula-py) (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.3->tabula-py) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.3->tabula-py) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.25.3->tabula-py) (1.15.0)\n",
            "Installing collected packages: distro, tabula-py\n",
            "Successfully installed distro-1.6.0 tabula-py-2.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvZSHJPsU8rw",
        "outputId": "69ccb357-1c42-46d3-859b-a52bcd6148f9"
      },
      "source": [
        "import tabula\n",
        "tabula.read_pdf(\"/content/sample_data/ADITYA_BIRLA (14).pdf\",stream=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "'pages' argument isn't specified.Will extract only from page 1 by default.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiEDd9dAWcc1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYkceBNMWcaN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecaCtH1cWcVI"
      },
      "source": [
        "a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uB1mt-kNMbyB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "57014a91-2969-4151-ef31-7e4488a056d1"
      },
      "source": [
        "text.find('Policy Inception Date:', 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6042"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxrpbT-qDHFN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "79f50bd7-29d1-4977-a10a-0f1cb7d21c22"
      },
      "source": [
        "text[6070]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je0RlLHROzmd"
      },
      "source": [
        "text_split = text.split('\\xa0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oj4u-HmJpZUQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3298d9d5-c197-4395-aac6-f555b5f4a402"
      },
      "source": [
        "text_split"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Auto',\n",
              " 'Secure',\n",
              " 'Private',\n",
              " 'Car',\n",
              " 'Package',\n",
              " 'PolicyName',\n",
              " ':',\n",
              " 'MR',\n",
              " 'GEETESH',\n",
              " 'BHARJAddress',\n",
              " ':',\n",
              " 'B',\n",
              " '11/GF',\n",
              " 'B',\n",
              " 'BLOCK',\n",
              " 'SHIVAJI',\n",
              " 'VIHARJANTA',\n",
              " 'COLONYDELHI',\n",
              " '\\xad',\n",
              " '110027DELHIDELHIINDIADate',\n",
              " ':',\n",
              " '14/02/2019Your',\n",
              " 'Policy',\n",
              " 'Details',\n",
              " ':',\n",
              " 'Policy',\n",
              " 'Number',\n",
              " ':',\n",
              " '0159432761',\n",
              " '00Policy',\n",
              " 'Period',\n",
              " ':',\n",
              " 'From',\n",
              " '14/02/2019',\n",
              " 'to.',\n",
              " 'Midnight',\n",
              " 'Of',\n",
              " '13/02/2020',\n",
              " 'Premium',\n",
              " 'Paid',\n",
              " ':',\n",
              " '',\n",
              " '20,461.00Dear',\n",
              " 'MR',\n",
              " 'GEETESH',\n",
              " 'BHARJ,Welcome',\n",
              " 'to',\n",
              " 'Tata',\n",
              " 'AIG',\n",
              " 'General',\n",
              " 'Insurance',\n",
              " 'Company',\n",
              " 'Limited’s',\n",
              " 'family',\n",
              " '&',\n",
              " 'we',\n",
              " 'thankyou',\n",
              " 'for',\n",
              " 'choosing',\n",
              " 'our',\n",
              " 'Auto',\n",
              " 'Secure',\n",
              " 'Private',\n",
              " 'Car',\n",
              " 'Package',\n",
              " 'Policy',\n",
              " 'for',\n",
              " 'your',\n",
              " 'vehicleinsurance.We',\n",
              " 'are',\n",
              " 'enclosing',\n",
              " 'Policy',\n",
              " 'schedule',\n",
              " 'cum',\n",
              " 'certificate',\n",
              " 'of',\n",
              " 'insurance',\n",
              " 'of',\n",
              " 'your',\n",
              " 'vehicle.You',\n",
              " 'are',\n",
              " 'requested',\n",
              " 'to',\n",
              " 'visit',\n",
              " 'our',\n",
              " 'website',\n",
              " 'www.tataaiginsurance.in',\n",
              " 'for',\n",
              " 'policywording.We',\n",
              " 'would',\n",
              " 'like',\n",
              " 'to',\n",
              " 'inform',\n",
              " 'you',\n",
              " 'that',\n",
              " 'policy',\n",
              " 'has',\n",
              " 'been',\n",
              " 'issued',\n",
              " 'based',\n",
              " 'on',\n",
              " 'theinformation',\n",
              " 'and',\n",
              " 'declaration',\n",
              " 'provided',\n",
              " 'by',\n",
              " 'you.',\n",
              " 'No',\n",
              " 'Claim',\n",
              " 'Bonus',\n",
              " '(NCB)',\n",
              " 'if',\n",
              " 'shownon',\n",
              " 'your',\n",
              " 'policy',\n",
              " 'schedule',\n",
              " 'has',\n",
              " 'been',\n",
              " 'allowed',\n",
              " 'as',\n",
              " 'you',\n",
              " 'had',\n",
              " 'not',\n",
              " 'reported',\n",
              " 'any',\n",
              " 'claimin',\n",
              " 'the',\n",
              " 'previous',\n",
              " 'policy.Kindly',\n",
              " 'go',\n",
              " 'through',\n",
              " 'the',\n",
              " 'enclosed',\n",
              " 'information/declaration',\n",
              " 'provided',\n",
              " 'by',\n",
              " 'you',\n",
              " 'and',\n",
              " 'incase',\n",
              " 'your',\n",
              " 'policy',\n",
              " 'shows',\n",
              " 'No',\n",
              " 'Claim',\n",
              " 'Bonus,',\n",
              " 'for',\n",
              " 'which',\n",
              " 'you',\n",
              " 'are',\n",
              " 'not',\n",
              " 'entitled',\n",
              " 'asexplained',\n",
              " 'above',\n",
              " 'or',\n",
              " 'any',\n",
              " 'other',\n",
              " 'error/discrepancy',\n",
              " 'then',\n",
              " 'we',\n",
              " 'request',\n",
              " 'you',\n",
              " 'to',\n",
              " 'get',\n",
              " 'intouch',\n",
              " 'with',\n",
              " 'us',\n",
              " 'within',\n",
              " '15',\n",
              " 'days',\n",
              " 'of',\n",
              " 'receipt',\n",
              " 'of',\n",
              " 'the',\n",
              " 'policy',\n",
              " 'for',\n",
              " 'correction',\n",
              " 'otherwiseall',\n",
              " 'particulars',\n",
              " 'will',\n",
              " 'be',\n",
              " 'deemed',\n",
              " 'to',\n",
              " 'be',\n",
              " 'correct.You',\n",
              " 'may',\n",
              " 'reach',\n",
              " 'us',\n",
              " 'at',\n",
              " 'our',\n",
              " '24*7',\n",
              " 'helpline',\n",
              " '1800',\n",
              " '266',\n",
              " '7780',\n",
              " 'for',\n",
              " 'providing',\n",
              " 'anyinformation',\n",
              " 'or',\n",
              " 'in',\n",
              " 'case',\n",
              " 'you',\n",
              " 'desire',\n",
              " 'to',\n",
              " 'have',\n",
              " 'a',\n",
              " 'printed',\n",
              " 'copy',\n",
              " 'of',\n",
              " 'policy',\n",
              " 'wording.We,',\n",
              " 'thank',\n",
              " 'you',\n",
              " 'once',\n",
              " 'again,',\n",
              " 'for',\n",
              " 'choosing',\n",
              " 'Tata',\n",
              " 'AIG',\n",
              " 'General',\n",
              " 'Insurance',\n",
              " 'CompanyLimited',\n",
              " 'for',\n",
              " 'insuring',\n",
              " 'your',\n",
              " 'vehicle.',\n",
              " 'We',\n",
              " 'assure',\n",
              " 'you',\n",
              " 'of',\n",
              " 'our',\n",
              " 'best',\n",
              " 'of',\n",
              " 'services',\n",
              " 'at',\n",
              " 'alltimes.',\n",
              " 'Happy',\n",
              " 'driving!',\n",
              " 'Sincerely,For',\n",
              " 'Tata',\n",
              " 'AIG',\n",
              " 'General',\n",
              " 'Insurance',\n",
              " 'Company',\n",
              " 'LTD.Authorized',\n",
              " 'Signatory',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'TATA',\n",
              " 'AIG',\n",
              " 'General',\n",
              " 'Insurance',\n",
              " 'Company',\n",
              " 'Ltd.',\n",
              " 'Regd.',\n",
              " 'Office:',\n",
              " '15th',\n",
              " 'floor,',\n",
              " 'Tower',\n",
              " 'A,',\n",
              " 'Peninsula',\n",
              " 'Business',\n",
              " 'Park,Ganpatrao',\n",
              " 'Kadam',\n",
              " 'Marg,',\n",
              " 'Off',\n",
              " 'Senapati',\n",
              " 'Bapat',\n",
              " 'Marg,',\n",
              " 'Lower',\n",
              " 'Parel,',\n",
              " 'Mumbai\\xad',\n",
              " '400',\n",
              " '013.',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'IRDA',\n",
              " 'Registration',\n",
              " 'No.108,',\n",
              " 'CIN',\n",
              " 'No',\n",
              " ':',\n",
              " 'U85110MH2000PLC128425,',\n",
              " 'PAN',\n",
              " ':',\n",
              " 'AABCT3518Q,',\n",
              " 'UIN',\n",
              " 'No.:',\n",
              " 'IRDAN108P0002V01200001',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Website:',\n",
              " 'www.tataaig.com',\n",
              " '24X7',\n",
              " 'Tollfree',\n",
              " 'Helpline',\n",
              " '1800\\xad266\\xad7780',\n",
              " 'E\\xadmail:',\n",
              " 'customersupport@tataaig.com\\x0c',\n",
              " '',\n",
              " '^Premium',\n",
              " 'mentioned',\n",
              " 'covers',\n",
              " 'Self',\n",
              " 'and',\n",
              " 'Family',\n",
              " 'and',\n",
              " 'is',\n",
              " 'Inclusive',\n",
              " 'of',\n",
              " 'All',\n",
              " 'Applicable',\n",
              " 'Taxes,',\n",
              " 'Conditions',\n",
              " 'apply.',\n",
              " '#Premium',\n",
              " 'mentioned',\n",
              " 'is',\n",
              " 'Inclusive',\n",
              " 'of',\n",
              " 'All',\n",
              " 'Applicable',\n",
              " 'Taxes,',\n",
              " 'Conditions',\n",
              " 'apply.',\n",
              " '*Tax',\n",
              " 'benefit',\n",
              " 'under',\n",
              " 'section',\n",
              " '80D',\n",
              " 'of',\n",
              " 'Income',\n",
              " 'Tax',\n",
              " 'Act',\n",
              " '1961.',\n",
              " 'Tax',\n",
              " 'benefits',\n",
              " 'are',\n",
              " 'subject',\n",
              " 'to',\n",
              " 'changes',\n",
              " 'in',\n",
              " 'tax',\n",
              " 'laws.',\n",
              " 'If',\n",
              " 'one',\n",
              " 'purchases',\n",
              " 'a',\n",
              " 'health',\n",
              " 'insurance',\n",
              " 'policy',\n",
              " 'forself/spouse/children,',\n",
              " 'he/she',\n",
              " 'can',\n",
              " 'claim',\n",
              " 'a',\n",
              " 'tax',\n",
              " 'deduction',\n",
              " 'of',\n",
              " 'upto',\n",
              " '',\n",
              " '25000.',\n",
              " 'When',\n",
              " 'one',\n",
              " 'purchases',\n",
              " 'a',\n",
              " 'health',\n",
              " 'insurance',\n",
              " 'policy',\n",
              " 'for',\n",
              " 'parents',\n",
              " '(a',\n",
              " 'senior',\n",
              " 'citizen),',\n",
              " 'he/she',\n",
              " 'iseligible',\n",
              " 'for',\n",
              " 'an',\n",
              " 'additional',\n",
              " 'tax',\n",
              " 'deduction',\n",
              " 'benefit',\n",
              " 'upto',\n",
              " '',\n",
              " '30000.Purchase',\n",
              " 'of',\n",
              " 'Tata',\n",
              " 'AIG',\n",
              " 'General',\n",
              " 'Insurance',\n",
              " 'Company',\n",
              " 'Limited',\n",
              " 'products',\n",
              " 'are',\n",
              " 'purely',\n",
              " 'on',\n",
              " 'voluntary',\n",
              " 'basis.',\n",
              " 'Insurance',\n",
              " 'is',\n",
              " 'the',\n",
              " 'subject',\n",
              " 'matter',\n",
              " 'of',\n",
              " 'the',\n",
              " 'solicitation.',\n",
              " 'For',\n",
              " 'moredetails',\n",
              " 'on',\n",
              " 'risk',\n",
              " 'factors,',\n",
              " 'terms',\n",
              " 'and',\n",
              " 'conditions',\n",
              " 'please',\n",
              " 'read',\n",
              " 'sales',\n",
              " 'brochure',\n",
              " 'carefully',\n",
              " 'before',\n",
              " 'concluding',\n",
              " 'a',\n",
              " 'sale.Accident',\n",
              " 'Guard',\n",
              " 'Policy',\n",
              " 'UIN:',\n",
              " 'IRDA/NL\\xadHLT/TAGI/P\\xadP/V.I/195/13\\xad14',\n",
              " 'MediPrime',\n",
              " 'UIN:IRDA/NL\\xadHLT/TAGI/P\\xadH/V.I/34/13\\xad14',\n",
              " 'Homesecure',\n",
              " '(Householders)',\n",
              " 'InstachoicePolicy',\n",
              " 'for',\n",
              " 'health',\n",
              " 'component',\n",
              " 'i.e.',\n",
              " 'Personal',\n",
              " 'Accident',\n",
              " 'cover',\n",
              " 'UIN:',\n",
              " '53/IRDAI/HLT/TAGI/NL\\xadPACKAGE/2015\\xad16',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'TATA',\n",
              " 'AIG',\n",
              " 'General',\n",
              " 'Insurance',\n",
              " 'Company',\n",
              " 'Ltd.',\n",
              " 'Regd.',\n",
              " 'Office:',\n",
              " '15th',\n",
              " 'floor,',\n",
              " 'Tower',\n",
              " 'A,',\n",
              " 'Peninsula',\n",
              " 'Business',\n",
              " 'Park,Ganpatrao',\n",
              " 'Kadam',\n",
              " 'Marg,',\n",
              " 'Off',\n",
              " 'Senapati',\n",
              " 'Bapat',\n",
              " 'Marg,',\n",
              " 'Lower',\n",
              " 'Parel,',\n",
              " 'Mumbai\\xad',\n",
              " '400',\n",
              " '013.',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'IRDA',\n",
              " 'Registration',\n",
              " 'No.108,',\n",
              " 'CIN',\n",
              " 'No',\n",
              " ':',\n",
              " 'U85110MH2000PLC128425,',\n",
              " 'PAN',\n",
              " ':',\n",
              " 'AABCT3518Q,',\n",
              " 'UIN',\n",
              " 'No.:',\n",
              " 'IRDAN108P0002V01200001',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Website:',\n",
              " 'www.tataaig.com',\n",
              " '24X7',\n",
              " 'Tollfree',\n",
              " 'Helpline',\n",
              " '1800\\xad266\\xad7780',\n",
              " 'E\\xadmail:',\n",
              " 'customersupport@tataaig.com\\x0c',\n",
              " 'Certificate',\n",
              " 'Of',\n",
              " 'Insurance',\n",
              " 'and',\n",
              " 'Policy',\n",
              " 'Schedule',\n",
              " 'Form',\n",
              " '51',\n",
              " 'of',\n",
              " 'the',\n",
              " 'Central',\n",
              " 'Motor',\n",
              " 'Vehicle',\n",
              " 'Rules,',\n",
              " '1989Agent/Broker/Producer',\n",
              " 'Name:',\n",
              " 'D2C',\n",
              " 'INSURANCE',\n",
              " 'BROKING',\n",
              " 'PVT',\n",
              " 'LTDAgent/Broker',\n",
              " 'License',\n",
              " 'Code:',\n",
              " '505;',\n",
              " 'Agent/Broker',\n",
              " 'Contact',\n",
              " 'No.:',\n",
              " '18004197852',\n",
              " 'Certificate',\n",
              " '&',\n",
              " 'Policy',\n",
              " 'No.:',\n",
              " '0159432761',\n",
              " '00',\n",
              " 'Policy',\n",
              " 'Type:',\n",
              " 'Auto',\n",
              " 'Secure',\n",
              " '\\xad',\n",
              " 'Private',\n",
              " 'Car',\n",
              " 'Package',\n",
              " 'Policy',\n",
              " 'Period',\n",
              " 'of',\n",
              " 'Insurance:',\n",
              " 'From',\n",
              " '15:05',\n",
              " 'Hrs',\n",
              " 'on',\n",
              " '14/02/2019',\n",
              " '',\n",
              " 'Date',\n",
              " 'of',\n",
              " 'Expiry',\n",
              " 'To',\n",
              " 'midnight',\n",
              " 'of',\n",
              " '13/02/2020',\n",
              " 'Insured',\n",
              " 'Name',\n",
              " '&',\n",
              " 'Address:',\n",
              " 'Premium',\n",
              " '(Inclusive',\n",
              " 'of',\n",
              " 'AllApplicable',\n",
              " 'Taxes)',\n",
              " '20,461.00',\n",
              " 'MR',\n",
              " 'GEETESH',\n",
              " 'BHARJB',\n",
              " '11/GF',\n",
              " 'B',\n",
              " 'BLOCK',\n",
              " 'SHIVAJI',\n",
              " 'VIHARJANTA',\n",
              " 'COLONYDELHI',\n",
              " '\\xad',\n",
              " '110027DELHIDELHIINDIA',\n",
              " 'Insured',\n",
              " 'Business/Profession:OTHER',\n",
              " '',\n",
              " 'Geographical',\n",
              " 'Area:India',\n",
              " '',\n",
              " 'Registration',\n",
              " 'Authority:New',\n",
              " 'Delhi:',\n",
              " 'Tilak',\n",
              " 'Marg',\n",
              " '',\n",
              " 'HPA',\n",
              " '/',\n",
              " 'Hyp',\n",
              " '/',\n",
              " 'Lease',\n",
              " 'to:TATA',\n",
              " 'CAPITAL',\n",
              " 'FINANCIAL',\n",
              " 'SERVICES',\n",
              " 'LTD',\n",
              " 'Registration',\n",
              " 'No.Make',\n",
              " '&',\n",
              " 'ModelEngine',\n",
              " 'No.Chassis',\n",
              " 'No.CCMfg.',\n",
              " 'YearBody',\n",
              " 'TypeSeatingCapacityDL',\n",
              " '',\n",
              " '02',\n",
              " '',\n",
              " 'CAX',\n",
              " '',\n",
              " '7314MARUTI',\n",
              " 'CIAZ',\n",
              " '\\xad',\n",
              " 'DELTASMART',\n",
              " 'HYBRID',\n",
              " '3099380',\n",
              " '291200',\n",
              " '1248',\n",
              " '2017',\n",
              " 'SALOON',\n",
              " '5IDV',\n",
              " 'of',\n",
              " 'Vehicle()IDV',\n",
              " 'of',\n",
              " 'trailers()Bi\\xadFuel/CNG/LPGKit()IDV',\n",
              " 'of',\n",
              " 'non\\xadbuilt\\xadin',\n",
              " 'Accessories()Total',\n",
              " 'InsuredDeclaredValues(IDV)',\n",
              " '\\xad',\n",
              " '()ElectricalNon\\xadElectrical',\n",
              " '700,000.00',\n",
              " '0.00',\n",
              " '0.00',\n",
              " '0.00',\n",
              " '0.00',\n",
              " '700,000.00SCHEDULE',\n",
              " 'OF',\n",
              " 'PREMIUMA.',\n",
              " 'OWN',\n",
              " 'DAMAGEPremium',\n",
              " 'on',\n",
              " 'Vehicle',\n",
              " 'and',\n",
              " 'non',\n",
              " 'electrical',\n",
              " 'accessories10,341.45A.',\n",
              " 'TOTAL',\n",
              " 'OWN',\n",
              " 'DAMAGE',\n",
              " 'PREMIUM10,341.45Add:',\n",
              " 'Key',\n",
              " 'Replacement',\n",
              " '(Sum',\n",
              " 'Insured',\n",
              " '(SI)',\n",
              " '',\n",
              " '25,000.00',\n",
              " 'per',\n",
              " 'occurrence',\n",
              " 'limit50%',\n",
              " 'of',\n",
              " 'SI)',\n",
              " '(TA15)265.00Add:',\n",
              " 'Depreciation',\n",
              " 'reimbursement',\n",
              " '(TA01)3,150.00Add:',\n",
              " 'Emergency',\n",
              " 'transport',\n",
              " 'and',\n",
              " 'hotel',\n",
              " 'expenses',\n",
              " '(TA10)Any',\n",
              " 'One',\n",
              " 'Accident',\n",
              " ':',\n",
              " '',\n",
              " '5000Any',\n",
              " 'One',\n",
              " 'Year',\n",
              " ':',\n",
              " '',\n",
              " '10000110.00Add:',\n",
              " 'Loss',\n",
              " 'of',\n",
              " 'Personal',\n",
              " 'belonging',\n",
              " '(TA09)Sum',\n",
              " 'Insured',\n",
              " ':',\n",
              " '',\n",
              " '10000110.00C.',\n",
              " 'TOTAL',\n",
              " 'ADD',\n",
              " 'ON',\n",
              " 'PREMIUM3,635.00B.',\n",
              " 'LIABILITYBasic2,863.00Three',\n",
              " 'Year',\n",
              " 'Compulsory',\n",
              " 'PA',\n",
              " 'Cover',\n",
              " 'for',\n",
              " 'Owner\\xadDriver',\n",
              " '1500000450.00Add:',\n",
              " 'Legal',\n",
              " 'Liability',\n",
              " 'to',\n",
              " 'paid',\n",
              " 'driver',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zBXAJT2DRG1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4bcef8eb-6770-4fe4-c6c3-f354ec0dade8"
      },
      "source": [
        "indexes = [i for i,x in enumerate(text_split) if x == 'Accounting Code of Service']\n",
        "indexes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[39]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pr0VlQOqE6n8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7ea17fe1-25bb-4241-9189-da9990edb432"
      },
      "source": [
        "text_split[45]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'16-Apr-2020 to 15-Apr-2021'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShBO4EDcbKTp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "bef45a2e-d2b0-4355-ae6c-792ad2939b3c"
      },
      "source": [
        "text_split[313].split(' ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['January',\n",
              " '28,',\n",
              " '2020',\n",
              " '0.00',\n",
              " 'to',\n",
              " 'Midnight',\n",
              " 'of',\n",
              " 'January',\n",
              " '27,',\n",
              " '2021']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqXz4RtJQBDY"
      },
      "source": [
        "res = [i for i in text_split if 'Policy Effective from' in i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWj7zzH8R8mh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "126328de-3ae7-49eb-d1e5-86b4f8a1a48d"
      },
      "source": [
        "res"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['  पॉिलसी पभावी समय घंटे, को Policy Effective from 17:30 hours, on 01/07/2019 की अधरराित तक to midnight of 30/06/2020']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSD4BdYWR95J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bedc673b-cdf8-4932-f89f-9b8aec2b3371"
      },
      "source": [
        "text_split[770]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'11:18'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6guJu0RSDBT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "aeeff28e-c0d2-495c-8c46-ef435f4ad173"
      },
      "source": [
        "text_split[370]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'18 Oct 19 (00:00 hrs) to 17 Oct 20 (23:59'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLUbycofSFoG"
      },
      "source": [
        "policy_number = text_split[24].split('\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaZMLhgFSSok",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2e8ad8a8-e551-4a1a-c421-a1938bb4f64d"
      },
      "source": [
        "policy_number"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Policy/Certificate', 'No.', '', '900047026']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWz7lEspVAgA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "07225830-9c5c-4405-a052-78aaf6b65c09"
      },
      "source": [
        "pip install pdf2image"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pdf2image\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/62/bf2df0547cf4e216b329a9d39a7aa6c755f02071e63e17a4b76690ebfe20/pdf2image-1.13.1-py3-none-any.whl\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from pdf2image) (7.0.0)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSzxOmhJzGMJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "62c6f821-d9d6-476f-c5b8-e976556ffc68"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Acko Insurance Policy.pdf'\t   'HDFC Insurance Policy.pdf'\n",
            "'Bajaj INsurance POlicy.pdf'\t   'Iffco Tokio Insurance Policy.pdf'\n",
            "'Bharti Axa Insurance Policy.pdf'   insurance_date_extractor.ipynb\n",
            "'Edelwise Insurance Policy.pdf'     pdfquery.ipynb\n",
            "'Go Digit Insurance Policy.pdf'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyN86eEmzgqa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "baa891f7-5cee-4109-fcae-c63f2dc2e3f3"
      },
      "source": [
        "!apt-get install poppler-utils"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 154 kB of archives.\n",
            "After this operation, 613 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 poppler-utils amd64 0.62.0-2ubuntu2.10 [154 kB]\n",
            "Fetched 154 kB in 1s (178 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 144465 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_0.62.0-2ubuntu2.10_amd64.deb ...\n",
            "Unpacking poppler-utils (0.62.0-2ubuntu2.10) ...\n",
            "Setting up poppler-utils (0.62.0-2ubuntu2.10) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOoKDafryW4r"
      },
      "source": [
        "from pdf2image import convert_from_path, convert_from_bytes\n",
        "from pdf2image.exceptions import (\n",
        "    PDFInfoNotInstalledError,\n",
        "    PDFPageCountError,\n",
        "    PDFSyntaxError\n",
        ")\n",
        "images = convert_from_path('SBI Insurance Policy.pdf')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZebiCo-uygR5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "15122b25-d518-483d-a4b8-9539b8741e78"
      },
      "source": [
        "images"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<PIL.PpmImagePlugin.PpmImageFile image mode=RGB size=1660x2337 at 0x7FE7505399E8>,\n",
              " <PIL.PpmImagePlugin.PpmImageFile image mode=RGB size=1660x2337 at 0x7FE750539C88>,\n",
              " <PIL.PpmImagePlugin.PpmImageFile image mode=RGB size=1660x2337 at 0x7FE75077D518>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 237
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWBIdeb14MZV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "47091da7-7067-4ceb-ef04-b0a5c2851d3d"
      },
      "source": [
        "!pip install pytesseract"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytesseract\n",
            "  Downloading https://files.pythonhosted.org/packages/1d/d8/521db389ff0aae32035bfda6ed39cb2c2e28521c47015f6431f07460c50a/pytesseract-0.3.4.tar.gz\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from pytesseract) (7.0.0)\n",
            "Building wheels for collected packages: pytesseract\n",
            "  Building wheel for pytesseract (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytesseract: filename=pytesseract-0.3.4-py2.py3-none-any.whl size=13431 sha256=d7d5a4bba09ae19434734523cc08f4998ffa5f286b19b4a5086e325743c62ab7\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/a0/7596d2e0a73cf0aeffd6f6170862c4e73f3763b7827e48691a\n",
            "Successfully built pytesseract\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2fIIaKI6VhX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "bb6df667-4bc1-4b95-e783-fb1558125d3c"
      },
      "source": [
        "!sudo apt install tesseract-ocr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 4,795 kB of archives.\n",
            "After this operation, 15.8 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-eng all 4.00~git24-0e00fe6-1.2 [1,588 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-osd all 4.00~git24-0e00fe6-1.2 [2,989 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr amd64 4.00~git2288-10f4998a-2 [218 kB]\n",
            "Fetched 4,795 kB in 2s (2,585 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 144493 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_4.00~git24-0e00fe6-1.2_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_4.00~git24-0e00fe6-1.2_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.00~git2288-10f4998a-2_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.00~git2288-10f4998a-2) ...\n",
            "Setting up tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ...\n",
            "Setting up tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ...\n",
            "Setting up tesseract-ocr (4.00~git2288-10f4998a-2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klrGTdfK4Q9D"
      },
      "source": [
        "import pytesseract\n",
        "#pytesseract.pytesseract.tesseract_cmd = r'/usr/local/bin/pytesseract'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwIwP7D-zmu2"
      },
      "source": [
        "from matplotlib.pyplot import imshow\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "from IPython.display import Image \n",
        "#pil_img = Image(filename='data/empire.jpg')\n",
        "display(images[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeFzqnxh0DWr"
      },
      "source": [
        "text = pytesseract.image_to_string(images[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjolAIFU41Dq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "a55b9b23-b373-404b-8ee9-f0f0f83ee342"
      },
      "source": [
        "text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'Customer Care Number\\n1800-22-1111 (MTNL/BSNL user)\\n1800-102-1111 (for other users)\\ncustomer.care@sbigeneral.in\\n\\n \\n\\nPRIVATE CAR CERTLIOAT. OF JNSURANGE CUM POLICY SCHEDULE\\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n   \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nImportant Note: 1) The Validity of this Certificate of Insurance cum Schedule is subject to realization of the premium cheque. 2) This Insurance Policy cover is valid subject to availabilty of Complete and Correct Registration\\nNumber within 15 days from the Date of Commencement of this Policy. 3) This insurance policy cover is valid subject to Warranties, Terms and Conditions of the policy.\\nHTM | jj 14012020_196468\\nAEA - Palo Cette No :\\nEM0675433491N SpeedPost sia :(0000000003780475-04\\nBNPL A/c. No.: MR/DA-West/1573/17-18. Policy Issue Date 213/01/2020\\nName © Mr. ASHOK KUMAR costomeD 70000000005589935\\nGeographical Area “India\\nAddress : , HOUSE NO-10 BLOCK-A, SECTOR 52, Noida Head Post Office, Noida, Gautam Buddha . a e\\nNagar - 201301, Uttar Pradesh, India Policy Servicing Branch Noida\\nIntermediary Name & Code = Sbi\\nContacto. - +91-9868717811, = ae re : Sbi Tehsil Compound 16606 , 0024581\\nintermediary Cor lo. or\\nEmail ld . VAIBHV16694@GMAIL.COM pe eco\\nPeriod of Insurance 09/02/2020 00:00 Hours To 08/02/2021 Midnight\\n= 4}\\nVehicle Details insured’s Declared Values i Amount (Rs}\\nVehicle Make Model & Variant _| Maruti Suzuki, Alto & bd - BS Ill 53460)\\nYear of Mfg. 2007 0\\n| Registration no | DL2C AG 4290 | 0\\n= 1 7 Saak:\\n| RTO Location Name 0\\nEngine No 149738 se Ps eae CNG/LPG Kit (Rs) 0\\nChassis No. 837792 Sees oe a | Total IDV {Rs} ee ee ee TOR tas 53460)\\nCubic Capacity 796 Type of Body HATCHBACK.\\nFuel Type Petrol\\nSeating Capacity Including Driver | 5\\n\\n \\n\\n \\n\\n \\n\\nLIMITATION AS TO USE : As per Motor Vehicle Rules, 1989, Th\\nPace Making, e) Speed testing f) Reliability Tris ))\\nDRIVERS CLAUSE : Any Person including the Insured Provided g th fied from holding or obtaining such a license; provided also that the\\n\\nward, b) Carriage of Goods (other than samples or personal luggage), c) Organized racing,d)\\n\\n \\n  \\n   \\n \\n    \\n \\n  \\n\\n \\n\\nLIMITS OF LIABILITY = a. Under Section Il-1(i) of the Px\\nproperty other than property belongin:\\nDEDUCTIBLE UNDER SECTION-I: (i)\\n‘SPECIAL CONDITION\\nNO CLAIM BONUS:The Insured is entitled for a No Ci\\nThe preceding year — 20%; Preceding two consecutive years — 25%; Preceding three consecutive ye\\nallowed provided the Policy is renewed within 90 days of the expiry date of the previous policy.\\n\\nct, 1988. b. Under Section Il (1)(i) of the Policy-Damage to\\nx-driver under Section-Ill CSI - Rs. 1500000 /-.\\n\\n  \\n\\nVoluntary dé\\nKk are excl\\n\\n   \\n \\n \\n \\n \\n \\n\\njade or pending during the preceding year(s), as follows:\\nconsecutive years — 45%; Preceding five consecutive years — 50%. The No Claim Bonus will only be\\n\\n \\n\\n    \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n  \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nPremium Computation Table\\nOwnDamage-Sectionh = 1 amount egy | Ganitty section 0 Amount (Rs)\\nOwn Damage Premium for Vehicle & Accessories (Amount will be including 898.65 Basic Liability 2072.00\\nall tariff covers & discounts & Other Loading / Discount) ~~ | Liability Premium for Trailers 0.00\\nBasic Road side Assistance 0.00 | CNG/LPG Bi-fuel Kit 0,00\\nDepreciation reimbursement 0.00 | Geographical Extension 0,00\\nReturn to Invoice 0.00 | Driving Tuition 0.00\\nEngine Guard 0.00 0.00\\nCover for Consumable 0.00 0.00\\nProtection of NCB 0.00 | Adéitional avers SS ee SS\\nCover for Key Replacement 0.00 | PA cover - Owner Driver 325.00\\nInconvenience Allowance 0.00 | PA cover — Unnamed Passengers (SI/Person) 500.00\\nLoss of Personal Belongings 0.00 | Legal Liability Cover 3 |\\nEnhanced PA Cover for Insured (Owner Driver) | 0.0 Liability to paid driver 0.00\\nEnhanced PA cover for Un-nammed Passengers 0,00 | Legal Liability to Employee 0.00\\nEnhanced PA cover for Paid Drivers 0.00 | Legal Liability Soldier / Sailor / Airman 0.00\\nHospital Cash Cover for Insured (Owner Driver) 0.00 | TPPD - Third Party Property Damage Discount 0.00]\\nHospital Cash Cover for unnamed passengers 0.00 | Use confined to Own Premises 0.00\\nHospital Cash Cover for Paid Drivers 0,00 } Vintage Car 0.00\\nAdditional Road Side Assistance 0.00 } Total Third Party Liability Premium (B) 2897.00\\nNCB Details e101 POucy PREMIUM (A+B) 3795.66\\nNo Claim Bonus 50% 516.26 | Taxes as Applicable\\nTotal Own Damage Premium (A) i 898.66 | Total Premium Collected\\n\\n \\n\\nPremium Collection details : Receipt No.: Receipt Date:\\nHire Purchase/ Lease /Hypothecated with :,\\n‘Subject to I.M.T Endorsement Nos. :IMT16,MT22\\n\\nSubject to SBIG Add-On Endorsement Nos.:SBIG-@ @ 0100\\n\\nNominee Details : Name :VAIBHAV SHARMA, Date of Birth16/09/1994, Relation Son\\n\\n \\n\\nFor claims, Please contact us at Toll Free number\\nMTNL/BSNL users ~ 1800-22-1111, Other users - 1800-102-1111For complete Coverage & Policy Wording, kindly visit our website - www.sbigeneral.in\\nWe hereby certify that the Policy to which this Certificate relates as well as this Certificate of Insurance are issued in accordance with the provisions of chapter X and Chapter XI of M.V. Act, 1988.\\n\\nPolicy Servicing Office : 2nd Floor, N-27,N - Block, Nearby Mosica Hotel, Sector -18, Noida, Sec-16 Noida, Uttar Pradesh - 201301, India\\n\\n \\n\\nFor and on behalf of SBI General Insurance Co. Ltd\\n\\n \\n\\nConsolidated Stamp Duty Rs. 0.50 paid towards Insurance Policy Stamps vide Order No. CSD/220/2019/5508/19 Dated: 2020-01-13 15:46:16.0 of General Stamp Office, Mumbai\\n\\nIMPORTANT NOTICE: The Insured is not indemnified if the vehicle is used or driven otherwise than in accordance with this schedule. Any payment made by the Company by reason of Z %\\nwider terms hate in the certificate in order to comply with the Motor Vehicle Act, 1988 is recoverable from the Insured. See the clause headed “AVOIDANCE OF CERTAIN TERMS AND Authorized Signatory\\nRIGHT OF RECOVERY”. For legal interpretation English version will be good.\\n\\nDisclaimer: Please examine this Policy including attached Schedules / Annexure if any. In the event of any discrepancy please contact the office of the Company immediately, it being noted that\\n\\nthis Policy shall be otherwise considered as being entirely in order. Please find claims settlement & grievance redressal procedure available on www.sbigeneral.in\\n\\nSBI General Insurance Company Limited 196468\\n\\nRegistered and Corporate office : “Natraj” 301, Junction of Western Express Highway & Andheri Kurla — Road, Andheri (East), Mumbai— 400 069,IRDA of India Reg.No.144'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZuWGoYI6iEA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "85f82d0a-2db1-4140-9ee3-b6435831c10b"
      },
      "source": [
        "text.split('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Customer Care Number',\n",
              " '1800-22-1111 (MTNL/BSNL user)',\n",
              " '1800-102-1111 (for other users)',\n",
              " 'customer.care@sbigeneral.in',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " 'PRIVATE CAR CERTLIOAT. OF JNSURANGE CUM POLICY SCHEDULE',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " '   ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " 'Important Note: 1) The Validity of this Certificate of Insurance cum Schedule is subject to realization of the premium cheque. 2) This Insurance Policy cover is valid subject to availabilty of Complete and Correct Registration',\n",
              " 'Number within 15 days from the Date of Commencement of this Policy. 3) This insurance policy cover is valid subject to Warranties, Terms and Conditions of the policy.',\n",
              " 'HTM | jj 14012020_196468',\n",
              " 'AEA - Palo Cette No :',\n",
              " 'EM0675433491N SpeedPost sia :(0000000003780475-04',\n",
              " 'BNPL A/c. No.: MR/DA-West/1573/17-18. Policy Issue Date 213/01/2020',\n",
              " 'Name © Mr. ASHOK KUMAR costomeD 70000000005589935',\n",
              " 'Geographical Area “India',\n",
              " 'Address : , HOUSE NO-10 BLOCK-A, SECTOR 52, Noida Head Post Office, Noida, Gautam Buddha . a e',\n",
              " 'Nagar - 201301, Uttar Pradesh, India Policy Servicing Branch Noida',\n",
              " 'Intermediary Name & Code = Sbi',\n",
              " 'Contacto. - +91-9868717811, = ae re : Sbi Tehsil Compound 16606 , 0024581',\n",
              " 'intermediary Cor lo. or',\n",
              " 'Email ld . VAIBHV16694@GMAIL.COM pe eco',\n",
              " 'Period of Insurance 09/02/2020 00:00 Hours To 08/02/2021 Midnight',\n",
              " '= 4}',\n",
              " 'Vehicle Details insured’s Declared Values i Amount (Rs}',\n",
              " 'Vehicle Make Model & Variant _| Maruti Suzuki, Alto & bd - BS Ill 53460)',\n",
              " 'Year of Mfg. 2007 0',\n",
              " '| Registration no | DL2C AG 4290 | 0',\n",
              " '= 1 7 Saak:',\n",
              " '| RTO Location Name 0',\n",
              " 'Engine No 149738 se Ps eae CNG/LPG Kit (Rs) 0',\n",
              " 'Chassis No. 837792 Sees oe a | Total IDV {Rs} ee ee ee TOR tas 53460)',\n",
              " 'Cubic Capacity 796 Type of Body HATCHBACK.',\n",
              " 'Fuel Type Petrol',\n",
              " 'Seating Capacity Including Driver | 5',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " 'LIMITATION AS TO USE : As per Motor Vehicle Rules, 1989, Th',\n",
              " 'Pace Making, e) Speed testing f) Reliability Tris ))',\n",
              " 'DRIVERS CLAUSE : Any Person including the Insured Provided g th fied from holding or obtaining such a license; provided also that the',\n",
              " '',\n",
              " 'ward, b) Carriage of Goods (other than samples or personal luggage), c) Organized racing,d)',\n",
              " '',\n",
              " ' ',\n",
              " '  ',\n",
              " '   ',\n",
              " ' ',\n",
              " '    ',\n",
              " ' ',\n",
              " '  ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " 'LIMITS OF LIABILITY = a. Under Section Il-1(i) of the Px',\n",
              " 'property other than property belongin:',\n",
              " 'DEDUCTIBLE UNDER SECTION-I: (i)',\n",
              " '‘SPECIAL CONDITION',\n",
              " 'NO CLAIM BONUS:The Insured is entitled for a No Ci',\n",
              " 'The preceding year — 20%; Preceding two consecutive years — 25%; Preceding three consecutive ye',\n",
              " 'allowed provided the Policy is renewed within 90 days of the expiry date of the previous policy.',\n",
              " '',\n",
              " 'ct, 1988. b. Under Section Il (1)(i) of the Policy-Damage to',\n",
              " 'x-driver under Section-Ill CSI - Rs. 1500000 /-.',\n",
              " '',\n",
              " '  ',\n",
              " '',\n",
              " 'Voluntary dé',\n",
              " 'Kk are excl',\n",
              " '',\n",
              " '   ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " '',\n",
              " 'jade or pending during the preceding year(s), as follows:',\n",
              " 'consecutive years — 45%; Preceding five consecutive years — 50%. The No Claim Bonus will only be',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " '    ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " '  ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " 'Premium Computation Table',\n",
              " 'OwnDamage-Sectionh = 1 amount egy | Ganitty section 0 Amount (Rs)',\n",
              " 'Own Damage Premium for Vehicle & Accessories (Amount will be including 898.65 Basic Liability 2072.00',\n",
              " 'all tariff covers & discounts & Other Loading / Discount) ~~ | Liability Premium for Trailers 0.00',\n",
              " 'Basic Road side Assistance 0.00 | CNG/LPG Bi-fuel Kit 0,00',\n",
              " 'Depreciation reimbursement 0.00 | Geographical Extension 0,00',\n",
              " 'Return to Invoice 0.00 | Driving Tuition 0.00',\n",
              " 'Engine Guard 0.00 0.00',\n",
              " 'Cover for Consumable 0.00 0.00',\n",
              " 'Protection of NCB 0.00 | Adéitional avers SS ee SS',\n",
              " 'Cover for Key Replacement 0.00 | PA cover - Owner Driver 325.00',\n",
              " 'Inconvenience Allowance 0.00 | PA cover — Unnamed Passengers (SI/Person) 500.00',\n",
              " 'Loss of Personal Belongings 0.00 | Legal Liability Cover 3 |',\n",
              " 'Enhanced PA Cover for Insured (Owner Driver) | 0.0 Liability to paid driver 0.00',\n",
              " 'Enhanced PA cover for Un-nammed Passengers 0,00 | Legal Liability to Employee 0.00',\n",
              " 'Enhanced PA cover for Paid Drivers 0.00 | Legal Liability Soldier / Sailor / Airman 0.00',\n",
              " 'Hospital Cash Cover for Insured (Owner Driver) 0.00 | TPPD - Third Party Property Damage Discount 0.00]',\n",
              " 'Hospital Cash Cover for unnamed passengers 0.00 | Use confined to Own Premises 0.00',\n",
              " 'Hospital Cash Cover for Paid Drivers 0,00 } Vintage Car 0.00',\n",
              " 'Additional Road Side Assistance 0.00 } Total Third Party Liability Premium (B) 2897.00',\n",
              " 'NCB Details e101 POucy PREMIUM (A+B) 3795.66',\n",
              " 'No Claim Bonus 50% 516.26 | Taxes as Applicable',\n",
              " 'Total Own Damage Premium (A) i 898.66 | Total Premium Collected',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " 'Premium Collection details : Receipt No.: Receipt Date:',\n",
              " 'Hire Purchase/ Lease /Hypothecated with :,',\n",
              " '‘Subject to I.M.T Endorsement Nos. :IMT16,MT22',\n",
              " '',\n",
              " 'Subject to SBIG Add-On Endorsement Nos.:SBIG-@ @ 0100',\n",
              " '',\n",
              " 'Nominee Details : Name :VAIBHAV SHARMA, Date of Birth16/09/1994, Relation Son',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " 'For claims, Please contact us at Toll Free number',\n",
              " 'MTNL/BSNL users ~ 1800-22-1111, Other users - 1800-102-1111For complete Coverage & Policy Wording, kindly visit our website - www.sbigeneral.in',\n",
              " 'We hereby certify that the Policy to which this Certificate relates as well as this Certificate of Insurance are issued in accordance with the provisions of chapter X and Chapter XI of M.V. Act, 1988.',\n",
              " '',\n",
              " 'Policy Servicing Office : 2nd Floor, N-27,N - Block, Nearby Mosica Hotel, Sector -18, Noida, Sec-16 Noida, Uttar Pradesh - 201301, India',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " 'For and on behalf of SBI General Insurance Co. Ltd',\n",
              " '',\n",
              " ' ',\n",
              " '',\n",
              " 'Consolidated Stamp Duty Rs. 0.50 paid towards Insurance Policy Stamps vide Order No. CSD/220/2019/5508/19 Dated: 2020-01-13 15:46:16.0 of General Stamp Office, Mumbai',\n",
              " '',\n",
              " 'IMPORTANT NOTICE: The Insured is not indemnified if the vehicle is used or driven otherwise than in accordance with this schedule. Any payment made by the Company by reason of Z %',\n",
              " 'wider terms hate in the certificate in order to comply with the Motor Vehicle Act, 1988 is recoverable from the Insured. See the clause headed “AVOIDANCE OF CERTAIN TERMS AND Authorized Signatory',\n",
              " 'RIGHT OF RECOVERY”. For legal interpretation English version will be good.',\n",
              " '',\n",
              " 'Disclaimer: Please examine this Policy including attached Schedules / Annexure if any. In the event of any discrepancy please contact the office of the Company immediately, it being noted that',\n",
              " '',\n",
              " 'this Policy shall be otherwise considered as being entirely in order. Please find claims settlement & grievance redressal procedure available on www.sbigeneral.in',\n",
              " '',\n",
              " 'SBI General Insurance Company Limited 196468',\n",
              " '',\n",
              " 'Registered and Corporate office : “Natraj” 301, Junction of Western Express Highway & Andheri Kurla — Road, Andheri (East), Mumbai— 400 069,IRDA of India Reg.No.144']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 245
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr0uw9dd62jF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cV2x_rLATUbI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "VHvnRUKATUhu",
        "outputId": "ff670b65-1326-4bc2-b370-89112ab4bece"
      },
      "source": [
        "\n",
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# In[1]:\n",
        "\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "import numpy as np\n",
        "import cv2\n",
        "import math\n",
        "from scipy.ndimage import label\n",
        "\n",
        "\"\"\" auxilary functions \"\"\"\n",
        "# unwarp corodinates\n",
        "def warpCoord(Minv, pt):\n",
        "    out = np.matmul(Minv, (pt[0], pt[1], 1))\n",
        "    return np.array([out[0]/out[2], out[1]/out[2]])\n",
        "\"\"\" end of auxilary functions \"\"\"\n",
        "\n",
        "\n",
        "def getDetBoxes_core(textmap, linkmap, text_threshold, link_threshold, low_text, estimate_num_chars=False):\n",
        "    # prepare data\n",
        "    linkmap = linkmap.copy()\n",
        "    textmap = textmap.copy()\n",
        "    img_h, img_w = textmap.shape\n",
        "\n",
        "    \"\"\" labeling method \"\"\"\n",
        "    ret, text_score = cv2.threshold(textmap, low_text, 1, 0)\n",
        "    ret, link_score = cv2.threshold(linkmap, link_threshold, 1, 0)\n",
        "\n",
        "    text_score_comb = np.clip(text_score + link_score, 0, 1)\n",
        "    nLabels, labels, stats, centroids = cv2.connectedComponentsWithStats(text_score_comb.astype(np.uint8), connectivity=4)\n",
        "\n",
        "    det = []\n",
        "    mapper = []\n",
        "    for k in range(1,nLabels):\n",
        "        # size filtering\n",
        "        size = stats[k, cv2.CC_STAT_AREA]\n",
        "        if size < 10: continue\n",
        "\n",
        "        # thresholding\n",
        "        if np.max(textmap[labels==k]) < text_threshold: continue\n",
        "\n",
        "        # make segmentation map\n",
        "        segmap = np.zeros(textmap.shape, dtype=np.uint8)\n",
        "        segmap[labels==k] = 255\n",
        "        if estimate_num_chars:\n",
        "            _, character_locs = cv2.threshold((textmap - linkmap) * segmap /255., text_threshold, 1, 0)\n",
        "            _, n_chars = label(character_locs)\n",
        "            mapper.append(n_chars)\n",
        "        else:\n",
        "            mapper.append(k)\n",
        "        segmap[np.logical_and(link_score==1, text_score==0)] = 0   # remove link area\n",
        "        x, y = stats[k, cv2.CC_STAT_LEFT], stats[k, cv2.CC_STAT_TOP]\n",
        "        w, h = stats[k, cv2.CC_STAT_WIDTH], stats[k, cv2.CC_STAT_HEIGHT]\n",
        "        niter = int(math.sqrt(size * min(w, h) / (w * h)) * 2)\n",
        "        sx, ex, sy, ey = x - niter, x + w + niter + 1, y - niter, y + h + niter + 1\n",
        "        # boundary check\n",
        "        if sx < 0 : sx = 0\n",
        "        if sy < 0 : sy = 0\n",
        "        if ex >= img_w: ex = img_w\n",
        "        if ey >= img_h: ey = img_h\n",
        "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(1 + niter, 1 + niter))\n",
        "        segmap[sy:ey, sx:ex] = cv2.dilate(segmap[sy:ey, sx:ex], kernel)\n",
        "\n",
        "        # make box\n",
        "        np_contours = np.roll(np.array(np.where(segmap!=0)),1,axis=0).transpose().reshape(-1,2)\n",
        "        rectangle = cv2.minAreaRect(np_contours)\n",
        "        box = cv2.boxPoints(rectangle)\n",
        "\n",
        "        # align diamond-shape\n",
        "        w, h = np.linalg.norm(box[0] - box[1]), np.linalg.norm(box[1] - box[2])\n",
        "        box_ratio = max(w, h) / (min(w, h) + 1e-5)\n",
        "        if abs(1 - box_ratio) <= 0.1:\n",
        "            l, r = min(np_contours[:,0]), max(np_contours[:,0])\n",
        "            t, b = min(np_contours[:,1]), max(np_contours[:,1])\n",
        "            box = np.array([[l, t], [r, t], [r, b], [l, b]], dtype=np.float32)\n",
        "\n",
        "        # make clock-wise order\n",
        "        startidx = box.sum(axis=1).argmin()\n",
        "        box = np.roll(box, 4-startidx, 0)\n",
        "        box = np.array(box)\n",
        "\n",
        "        det.append(box)\n",
        "\n",
        "    return det, labels, mapper\n",
        "\n",
        "def getPoly_core(boxes, labels, mapper, linkmap):\n",
        "    # configs\n",
        "    num_cp = 5\n",
        "    max_len_ratio = 0.7\n",
        "    expand_ratio = 1.45\n",
        "    max_r = 2.0\n",
        "    step_r = 0.2\n",
        "\n",
        "    polys = []  \n",
        "    for k, box in enumerate(boxes):\n",
        "        # size filter for small instance\n",
        "        w, h = int(np.linalg.norm(box[0] - box[1]) + 1), int(np.linalg.norm(box[1] - box[2]) + 1)\n",
        "        if w < 10 or h < 10:\n",
        "            polys.append(None); continue\n",
        "\n",
        "        # warp image\n",
        "        tar = np.float32([[0,0],[w,0],[w,h],[0,h]])\n",
        "        M = cv2.getPerspectiveTransform(box, tar)\n",
        "        word_label = cv2.warpPerspective(labels, M, (w, h), flags=cv2.INTER_NEAREST)\n",
        "        try:\n",
        "            Minv = np.linalg.inv(M)\n",
        "        except:\n",
        "            polys.append(None); continue\n",
        "\n",
        "        # binarization for selected label\n",
        "        cur_label = mapper[k]\n",
        "        word_label[word_label != cur_label] = 0\n",
        "        word_label[word_label > 0] = 1\n",
        "\n",
        "        \"\"\" Polygon generation \"\"\"\n",
        "        # find top/bottom contours\n",
        "        cp = []\n",
        "        max_len = -1\n",
        "        for i in range(w):\n",
        "            region = np.where(word_label[:,i] != 0)[0]\n",
        "            if len(region) < 2 : continue\n",
        "            cp.append((i, region[0], region[-1]))\n",
        "            length = region[-1] - region[0] + 1\n",
        "            if length > max_len: max_len = length\n",
        "\n",
        "        # pass if max_len is similar to h\n",
        "        if h * max_len_ratio < max_len:\n",
        "            polys.append(None); continue\n",
        "\n",
        "        # get pivot points with fixed length\n",
        "        tot_seg = num_cp * 2 + 1\n",
        "        seg_w = w / tot_seg     # segment width\n",
        "        pp = [None] * num_cp    # init pivot points\n",
        "        cp_section = [[0, 0]] * tot_seg\n",
        "        seg_height = [0] * num_cp\n",
        "        seg_num = 0\n",
        "        num_sec = 0\n",
        "        prev_h = -1\n",
        "        for i in range(0,len(cp)):\n",
        "            (x, sy, ey) = cp[i]\n",
        "            if (seg_num + 1) * seg_w <= x and seg_num <= tot_seg:\n",
        "                # average previous segment\n",
        "                if num_sec == 0: break\n",
        "                cp_section[seg_num] = [cp_section[seg_num][0] / num_sec, cp_section[seg_num][1] / num_sec]\n",
        "                num_sec = 0\n",
        "\n",
        "                # reset variables\n",
        "                seg_num += 1\n",
        "                prev_h = -1\n",
        "\n",
        "            # accumulate center points\n",
        "            cy = (sy + ey) * 0.5\n",
        "            cur_h = ey - sy + 1\n",
        "            cp_section[seg_num] = [cp_section[seg_num][0] + x, cp_section[seg_num][1] + cy]\n",
        "            num_sec += 1\n",
        "\n",
        "            if seg_num % 2 == 0: continue # No polygon area\n",
        "\n",
        "            if prev_h < cur_h:\n",
        "                pp[int((seg_num - 1)/2)] = (x, cy)\n",
        "                seg_height[int((seg_num - 1)/2)] = cur_h\n",
        "                prev_h = cur_h\n",
        "\n",
        "        # processing last segment\n",
        "        if num_sec != 0:\n",
        "            cp_section[-1] = [cp_section[-1][0] / num_sec, cp_section[-1][1] / num_sec]\n",
        "\n",
        "        # pass if num of pivots is not sufficient or segment width is smaller than character height \n",
        "        if None in pp or seg_w < np.max(seg_height) * 0.25:\n",
        "            polys.append(None); continue\n",
        "\n",
        "        # calc median maximum of pivot points\n",
        "        half_char_h = np.median(seg_height) * expand_ratio / 2\n",
        "\n",
        "        # calc gradiant and apply to make horizontal pivots\n",
        "        new_pp = []\n",
        "        for i, (x, cy) in enumerate(pp):\n",
        "            dx = cp_section[i * 2 + 2][0] - cp_section[i * 2][0]\n",
        "            dy = cp_section[i * 2 + 2][1] - cp_section[i * 2][1]\n",
        "            if dx == 0:     # gradient if zero\n",
        "                new_pp.append([x, cy - half_char_h, x, cy + half_char_h])\n",
        "                continue\n",
        "            rad = - math.atan2(dy, dx)\n",
        "            c, s = half_char_h * math.cos(rad), half_char_h * math.sin(rad)\n",
        "            new_pp.append([x - s, cy - c, x + s, cy + c])\n",
        "\n",
        "        # get edge points to cover character heatmaps\n",
        "        isSppFound, isEppFound = False, False\n",
        "        grad_s = (pp[1][1] - pp[0][1]) / (pp[1][0] - pp[0][0]) + (pp[2][1] - pp[1][1]) / (pp[2][0] - pp[1][0])\n",
        "        grad_e = (pp[-2][1] - pp[-1][1]) / (pp[-2][0] - pp[-1][0]) + (pp[-3][1] - pp[-2][1]) / (pp[-3][0] - pp[-2][0])\n",
        "        for r in np.arange(0.5, max_r, step_r):\n",
        "            dx = 2 * half_char_h * r\n",
        "            if not isSppFound:\n",
        "                line_img = np.zeros(word_label.shape, dtype=np.uint8)\n",
        "                dy = grad_s * dx\n",
        "                p = np.array(new_pp[0]) - np.array([dx, dy, dx, dy])\n",
        "                cv2.line(line_img, (int(p[0]), int(p[1])), (int(p[2]), int(p[3])), 1, thickness=1)\n",
        "                if np.sum(np.logical_and(word_label, line_img)) == 0 or r + 2 * step_r >= max_r:\n",
        "                    spp = p\n",
        "                    isSppFound = True\n",
        "            if not isEppFound:\n",
        "                line_img = np.zeros(word_label.shape, dtype=np.uint8)\n",
        "                dy = grad_e * dx\n",
        "                p = np.array(new_pp[-1]) + np.array([dx, dy, dx, dy])\n",
        "                cv2.line(line_img, (int(p[0]), int(p[1])), (int(p[2]), int(p[3])), 1, thickness=1)\n",
        "                if np.sum(np.logical_and(word_label, line_img)) == 0 or r + 2 * step_r >= max_r:\n",
        "                    epp = p\n",
        "                    isEppFound = True\n",
        "            if isSppFound and isEppFound:\n",
        "                break\n",
        "\n",
        "        # pass if boundary of polygon is not found\n",
        "        if not (isSppFound and isEppFound):\n",
        "            polys.append(None); continue\n",
        "\n",
        "        # make final polygon\n",
        "        poly = []\n",
        "        poly.append(warpCoord(Minv, (spp[0], spp[1])))\n",
        "        for p in new_pp:\n",
        "            poly.append(warpCoord(Minv, (p[0], p[1])))\n",
        "        poly.append(warpCoord(Minv, (epp[0], epp[1])))\n",
        "        poly.append(warpCoord(Minv, (epp[2], epp[3])))\n",
        "        for p in reversed(new_pp):\n",
        "            poly.append(warpCoord(Minv, (p[2], p[3])))\n",
        "        poly.append(warpCoord(Minv, (spp[2], spp[3])))\n",
        "\n",
        "        # add to final result\n",
        "        polys.append(np.array(poly))\n",
        "\n",
        "    return polys\n",
        "\n",
        "def getDetBoxes(textmap, linkmap, text_threshold, link_threshold, low_text, poly=False, estimate_num_chars=False):\n",
        "    if poly and estimate_num_chars:\n",
        "        raise Exception(\"Estimating the number of characters not currently supported with poly.\")\n",
        "    boxes, labels, mapper = getDetBoxes_core(textmap, linkmap, text_threshold, link_threshold, low_text, estimate_num_chars)\n",
        "\n",
        "    if poly:\n",
        "        polys = getPoly_core(boxes, labels, mapper, linkmap)\n",
        "    else:\n",
        "        polys = [None] * len(boxes)\n",
        "\n",
        "    return boxes, polys, mapper\n",
        "\n",
        "def adjustResultCoordinates(polys, ratio_w, ratio_h, ratio_net = 2):\n",
        "    if len(polys) > 0:\n",
        "        polys = np.array(polys)\n",
        "        for k in range(len(polys)):\n",
        "            if polys[k] is not None:\n",
        "                polys[k] *= (ratio_w * ratio_net, ratio_h * ratio_net)\n",
        "    return polys\n",
        "\n",
        "\n",
        "# In[2]:\n",
        "\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "import numpy as np\n",
        "from skimage import io\n",
        "import cv2\n",
        "\n",
        "def loadImage(img_file):\n",
        "    img = io.imread(img_file)           # RGB order\n",
        "    if img.shape[0] == 2: img = img[0]\n",
        "    if len(img.shape) == 2 : img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
        "    if img.shape[2] == 4:   img = img[:,:,:3]\n",
        "    img = np.array(img)\n",
        "\n",
        "    return img\n",
        "\n",
        "def normalizeMeanVariance(in_img, mean=(0.485, 0.456, 0.406), variance=(0.229, 0.224, 0.225)):\n",
        "    # should be RGB order\n",
        "    img = in_img.copy().astype(np.float32)\n",
        "\n",
        "    img -= np.array([mean[0] * 255.0, mean[1] * 255.0, mean[2] * 255.0], dtype=np.float32)\n",
        "    img /= np.array([variance[0] * 255.0, variance[1] * 255.0, variance[2] * 255.0], dtype=np.float32)\n",
        "    return img\n",
        "\n",
        "def denormalizeMeanVariance(in_img, mean=(0.485, 0.456, 0.406), variance=(0.229, 0.224, 0.225)):\n",
        "    # should be RGB order\n",
        "    img = in_img.copy()\n",
        "    img *= variance\n",
        "    img += mean\n",
        "    img *= 255.0\n",
        "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
        "    return img\n",
        "\n",
        "def resize_aspect_ratio(img, square_size, interpolation, mag_ratio=1):\n",
        "    height, width, channel = img.shape\n",
        "\n",
        "    # magnify image size\n",
        "    target_size = mag_ratio * max(height, width)\n",
        "\n",
        "    # set original image size\n",
        "    if target_size > square_size:\n",
        "        target_size = square_size\n",
        "    \n",
        "    ratio = target_size / max(height, width)    \n",
        "\n",
        "    target_h, target_w = int(height * ratio), int(width * ratio)\n",
        "    proc = cv2.resize(img, (target_w, target_h), interpolation = interpolation)\n",
        "\n",
        "\n",
        "    # make canvas and paste image\n",
        "    target_h32, target_w32 = target_h, target_w\n",
        "    if target_h % 32 != 0:\n",
        "        target_h32 = target_h + (32 - target_h % 32)\n",
        "    if target_w % 32 != 0:\n",
        "        target_w32 = target_w + (32 - target_w % 32)\n",
        "    resized = np.zeros((target_h32, target_w32, channel), dtype=np.float32)\n",
        "    resized[0:target_h, 0:target_w, :] = proc\n",
        "    target_h, target_w = target_h32, target_w32\n",
        "\n",
        "    size_heatmap = (int(target_w/2), int(target_h/2))\n",
        "\n",
        "    return resized, ratio, size_heatmap\n",
        "\n",
        "def cvt2HeatmapImg(img):\n",
        "    img = (np.clip(img, 0, 1) * 255).astype(np.uint8)\n",
        "    img = cv2.applyColorMap(img, cv2.COLORMAP_JET)\n",
        "    return img\n",
        "\n",
        "\n",
        "# In[3]:\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "from torchvision import models\n",
        "from torchvision.models.vgg import model_urls\n",
        "from collections import namedtuple\n",
        "\n",
        "def init_weights(modules):\n",
        "    for m in modules:\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            init.xavier_uniform_(m.weight.data)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.zero_()\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            m.weight.data.fill_(1)\n",
        "            m.bias.data.zero_()\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            m.weight.data.normal_(0, 0.01)\n",
        "            m.bias.data.zero_()\n",
        "\n",
        "class vgg16_bn(torch.nn.Module):\n",
        "    def __init__(self, pretrained=True, freeze=True):\n",
        "        super(vgg16_bn, self).__init__()\n",
        "        model_urls['vgg16_bn'] = model_urls['vgg16_bn'].replace('https://', 'http://')\n",
        "        vgg_pretrained_features = models.vgg16_bn(pretrained=pretrained).features\n",
        "        self.slice1 = torch.nn.Sequential()\n",
        "        self.slice2 = torch.nn.Sequential()\n",
        "        self.slice3 = torch.nn.Sequential()\n",
        "        self.slice4 = torch.nn.Sequential()\n",
        "        self.slice5 = torch.nn.Sequential()\n",
        "        for x in range(12):         # conv2_2\n",
        "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(12, 19):         # conv3_3\n",
        "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(19, 29):         # conv4_3\n",
        "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(29, 39):         # conv5_3\n",
        "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
        "\n",
        "        # fc6, fc7 without atrous conv\n",
        "        self.slice5 = torch.nn.Sequential(\n",
        "                nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "                nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6),\n",
        "                nn.Conv2d(1024, 1024, kernel_size=1)\n",
        "        )\n",
        "\n",
        "        if not pretrained:\n",
        "            init_weights(self.slice1.modules())\n",
        "            init_weights(self.slice2.modules())\n",
        "            init_weights(self.slice3.modules())\n",
        "            init_weights(self.slice4.modules())\n",
        "\n",
        "        init_weights(self.slice5.modules())        # no pretrained model for fc6 and fc7\n",
        "\n",
        "        if freeze:\n",
        "            for param in self.slice1.parameters():      # only first conv\n",
        "                param.requires_grad= False\n",
        "\n",
        "    def forward(self, X):\n",
        "        h = self.slice1(X)\n",
        "        h_relu2_2 = h\n",
        "        h = self.slice2(h)\n",
        "        h_relu3_2 = h\n",
        "        h = self.slice3(h)\n",
        "        h_relu4_3 = h\n",
        "        h = self.slice4(h)\n",
        "        h_relu5_3 = h\n",
        "        h = self.slice5(h)\n",
        "        h_fc7 = h\n",
        "        vgg_outputs = namedtuple(\"VggOutputs\", ['fc7', 'relu5_3', 'relu4_3', 'relu3_2', 'relu2_2'])\n",
        "        out = vgg_outputs(h_fc7, h_relu5_3, h_relu4_3, h_relu3_2, h_relu2_2)\n",
        "        return out\n",
        "\n",
        "class BidirectionalLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(BidirectionalLSTM, self).__init__()\n",
        "        self.rnn = nn.LSTM(input_size, hidden_size, bidirectional=True, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size * 2, output_size)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        input : visual feature [batch_size x T x input_size]\n",
        "        output : contextual feature [batch_size x T x output_size]\n",
        "        \"\"\"\n",
        "        try: # multi gpu needs this\n",
        "            self.rnn.flatten_parameters()\n",
        "        except: # quantization doesn't work with this \n",
        "            pass\n",
        "        recurrent, _ = self.rnn(input)  # batch_size x T x input_size -> batch_size x T x (2*hidden_size)\n",
        "        output = self.linear(recurrent)  # batch_size x T x output_size\n",
        "        return output\n",
        "\n",
        "class VGG_FeatureExtractor(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channel, output_channel=256):\n",
        "        super(VGG_FeatureExtractor, self).__init__()\n",
        "        self.output_channel = [int(output_channel / 8), int(output_channel / 4),\n",
        "                               int(output_channel / 2), output_channel]\n",
        "        self.ConvNet = nn.Sequential(\n",
        "            nn.Conv2d(input_channel, self.output_channel[0], 3, 1, 1), nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(self.output_channel[0], self.output_channel[1], 3, 1, 1), nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(self.output_channel[1], self.output_channel[2], 3, 1, 1), nn.ReLU(True),\n",
        "            nn.Conv2d(self.output_channel[2], self.output_channel[2], 3, 1, 1), nn.ReLU(True),\n",
        "            nn.MaxPool2d((2, 1), (2, 1)),\n",
        "            nn.Conv2d(self.output_channel[2], self.output_channel[3], 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.output_channel[3]), nn.ReLU(True),\n",
        "            nn.Conv2d(self.output_channel[3], self.output_channel[3], 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.output_channel[3]), nn.ReLU(True),\n",
        "            nn.MaxPool2d((2, 1), (2, 1)),\n",
        "            nn.Conv2d(self.output_channel[3], self.output_channel[3], 2, 1, 0), nn.ReLU(True))\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.ConvNet(input)\n",
        "\n",
        "class ResNet_FeatureExtractor(nn.Module):\n",
        "    \"\"\" FeatureExtractor of FAN (http://openaccess.thecvf.com/content_ICCV_2017/papers/Cheng_Focusing_Attention_Towards_ICCV_2017_paper.pdf) \"\"\"\n",
        "\n",
        "    def __init__(self, input_channel, output_channel=512):\n",
        "        super(ResNet_FeatureExtractor, self).__init__()\n",
        "        self.ConvNet = ResNet(input_channel, output_channel, BasicBlock, [1, 2, 5, 3])\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.ConvNet(input)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = self._conv3x3(inplanes, planes)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = self._conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def _conv3x3(self, in_planes, out_planes, stride=1):\n",
        "        \"3x3 convolution with padding\"\n",
        "        return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                         padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channel, output_channel, block, layers):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.output_channel_block = [int(output_channel / 4), int(output_channel / 2), output_channel, output_channel]\n",
        "\n",
        "        self.inplanes = int(output_channel / 8)\n",
        "        self.conv0_1 = nn.Conv2d(input_channel, int(output_channel / 16),\n",
        "                                 kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn0_1 = nn.BatchNorm2d(int(output_channel / 16))\n",
        "        self.conv0_2 = nn.Conv2d(int(output_channel / 16), self.inplanes,\n",
        "                                 kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn0_2 = nn.BatchNorm2d(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.layer1 = self._make_layer(block, self.output_channel_block[0], layers[0])\n",
        "        self.conv1 = nn.Conv2d(self.output_channel_block[0], self.output_channel_block[\n",
        "                               0], kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.output_channel_block[0])\n",
        "\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.layer2 = self._make_layer(block, self.output_channel_block[1], layers[1], stride=1)\n",
        "        self.conv2 = nn.Conv2d(self.output_channel_block[1], self.output_channel_block[\n",
        "                               1], kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(self.output_channel_block[1])\n",
        "\n",
        "        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=(2, 1), padding=(0, 1))\n",
        "        self.layer3 = self._make_layer(block, self.output_channel_block[2], layers[2], stride=1)\n",
        "        self.conv3 = nn.Conv2d(self.output_channel_block[2], self.output_channel_block[\n",
        "                               2], kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.output_channel_block[2])\n",
        "\n",
        "        self.layer4 = self._make_layer(block, self.output_channel_block[3], layers[3], stride=1)\n",
        "        self.conv4_1 = nn.Conv2d(self.output_channel_block[3], self.output_channel_block[\n",
        "                                 3], kernel_size=2, stride=(2, 1), padding=(0, 1), bias=False)\n",
        "        self.bn4_1 = nn.BatchNorm2d(self.output_channel_block[3])\n",
        "        self.conv4_2 = nn.Conv2d(self.output_channel_block[3], self.output_channel_block[\n",
        "                                 3], kernel_size=2, stride=1, padding=0, bias=False)\n",
        "        self.bn4_2 = nn.BatchNorm2d(self.output_channel_block[3])\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv0_1(x)\n",
        "        x = self.bn0_1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv0_2(x)\n",
        "        x = self.bn0_2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.maxpool2(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.maxpool3(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer4(x)\n",
        "        x = self.conv4_1(x)\n",
        "        x = self.bn4_1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv4_2(x)\n",
        "        x = self.bn4_2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# In[4]:\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class double_conv(nn.Module):\n",
        "    def __init__(self, in_ch, mid_ch, out_ch):\n",
        "        super(double_conv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch + mid_ch, mid_ch, kernel_size=1),\n",
        "            nn.BatchNorm2d(mid_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class CRAFT(nn.Module):\n",
        "    def __init__(self, pretrained=False, freeze=False):\n",
        "        super(CRAFT, self).__init__()\n",
        "\n",
        "        \"\"\" Base network \"\"\"\n",
        "        self.basenet = vgg16_bn(pretrained, freeze)\n",
        "\n",
        "        \"\"\" U network \"\"\"\n",
        "        self.upconv1 = double_conv(1024, 512, 256)\n",
        "        self.upconv2 = double_conv(512, 256, 128)\n",
        "        self.upconv3 = double_conv(256, 128, 64)\n",
        "        self.upconv4 = double_conv(128, 64, 32)\n",
        "\n",
        "        num_class = 2\n",
        "        self.conv_cls = nn.Sequential(\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 16, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(16, 16, kernel_size=1), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(16, num_class, kernel_size=1),\n",
        "        )\n",
        "\n",
        "        init_weights(self.upconv1.modules())\n",
        "        init_weights(self.upconv2.modules())\n",
        "        init_weights(self.upconv3.modules())\n",
        "        init_weights(self.upconv4.modules())\n",
        "        init_weights(self.conv_cls.modules())\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Base network \"\"\"\n",
        "        sources = self.basenet(x)\n",
        "\n",
        "        \"\"\" U network \"\"\"\n",
        "        y = torch.cat([sources[0], sources[1]], dim=1)\n",
        "        y = self.upconv1(y)\n",
        "\n",
        "        y = F.interpolate(y, size=sources[2].size()[2:], mode='bilinear', align_corners=False)\n",
        "        y = torch.cat([y, sources[2]], dim=1)\n",
        "        y = self.upconv2(y)\n",
        "\n",
        "        y = F.interpolate(y, size=sources[3].size()[2:], mode='bilinear', align_corners=False)\n",
        "        y = torch.cat([y, sources[3]], dim=1)\n",
        "        y = self.upconv3(y)\n",
        "\n",
        "        y = F.interpolate(y, size=sources[4].size()[2:], mode='bilinear', align_corners=False)\n",
        "        y = torch.cat([y, sources[4]], dim=1)\n",
        "        feature = self.upconv4(y)\n",
        "\n",
        "        y = self.conv_cls(feature)\n",
        "\n",
        "        return y.permute(0,2,3,1), feature\n",
        "\n",
        "\n",
        "# In[5]:\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "from collections import OrderedDict\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def copyStateDict(state_dict):\n",
        "    if list(state_dict.keys())[0].startswith(\"module\"):\n",
        "        start_idx = 1\n",
        "    else:\n",
        "        start_idx = 0\n",
        "    new_state_dict = OrderedDict()\n",
        "    for k, v in state_dict.items():\n",
        "        name = \".\".join(k.split(\".\")[start_idx:])\n",
        "        new_state_dict[name] = v\n",
        "    return new_state_dict\n",
        "\n",
        "def test_net(canvas_size, mag_ratio, net, image, text_threshold, link_threshold, low_text, poly, device, estimate_num_chars=False):\n",
        "    if isinstance(image, np.ndarray) and len(image.shape) == 4:  # image is batch of np arrays\n",
        "        image_arrs = image\n",
        "    else:                                                        # image is single numpy array\n",
        "        image_arrs = [image]\n",
        "\n",
        "    img_resized_list = []\n",
        "    # resize\n",
        "    for img in image_arrs:\n",
        "        img_resized, target_ratio, size_heatmap = resize_aspect_ratio(img, canvas_size,\n",
        "                                                                      interpolation=cv2.INTER_LINEAR,\n",
        "                                                                      mag_ratio=mag_ratio)\n",
        "        img_resized_list.append(img_resized)\n",
        "    ratio_h = ratio_w = 1 / target_ratio\n",
        "    # preprocessing\n",
        "    x = [np.transpose(normalizeMeanVariance(n_img), (2, 0, 1))\n",
        "         for n_img in img_resized_list]\n",
        "    x = torch.from_numpy(np.array(x))\n",
        "    x = x.to(device)\n",
        "\n",
        "    # forward pass\n",
        "    with torch.no_grad():\n",
        "        y, feature = net(x)\n",
        "\n",
        "    boxes_list, polys_list = [], []\n",
        "    for out in y:\n",
        "        # make score and link map\n",
        "        score_text = out[:, :, 0].cpu().data.numpy()\n",
        "        score_link = out[:, :, 1].cpu().data.numpy()\n",
        "\n",
        "        # Post-processing\n",
        "        boxes, polys, mapper = getDetBoxes(\n",
        "            score_text, score_link, text_threshold, link_threshold, low_text, poly, estimate_num_chars)\n",
        "\n",
        "        # coordinate adjustment\n",
        "        boxes = adjustResultCoordinates(boxes, ratio_w, ratio_h)\n",
        "        polys = adjustResultCoordinates(polys, ratio_w, ratio_h)\n",
        "        if estimate_num_chars:\n",
        "            boxes = list(boxes)\n",
        "            polys = list(polys)\n",
        "        for k in range(len(polys)):\n",
        "            if estimate_num_chars:\n",
        "                boxes[k] = (boxes[k], mapper[k])\n",
        "            if polys[k] is None:\n",
        "                polys[k] = boxes[k]\n",
        "        boxes_list.append(boxes)\n",
        "        polys_list.append(polys)\n",
        "\n",
        "    return boxes_list, polys_list\n",
        "\n",
        "def get_detector(trained_model, device='cpu', quantize=True, cudnn_benchmark=False):\n",
        "    net = CRAFT()\n",
        "\n",
        "    if device == 'cpu':\n",
        "        net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))\n",
        "        if quantize:\n",
        "            try:\n",
        "                torch.quantization.quantize_dynamic(net, dtype=torch.qint8, inplace=True)\n",
        "            except:\n",
        "                pass\n",
        "    else:\n",
        "        net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))\n",
        "        net = torch.nn.DataParallel(net).to(device)\n",
        "        cudnn.benchmark = cudnn_benchmark\n",
        "\n",
        "    net.eval()\n",
        "    return net\n",
        "\n",
        "def get_textbox(detector, image, canvas_size, mag_ratio, text_threshold, link_threshold, low_text, poly, device, optimal_num_chars=None):\n",
        "    result = []\n",
        "    estimate_num_chars = optimal_num_chars is not None\n",
        "    bboxes_list, polys_list = test_net(canvas_size, mag_ratio, detector,\n",
        "                                       image, text_threshold,\n",
        "                                       link_threshold, low_text, poly,\n",
        "                                       device, estimate_num_chars)\n",
        "    if estimate_num_chars:\n",
        "        polys_list = [[p for p, _ in sorted(polys, key=lambda x: abs(optimal_num_chars - x[1]))]\n",
        "                      for polys in polys_list]\n",
        "\n",
        "    for polys in polys_list:\n",
        "        single_img_result = []\n",
        "        for i, box in enumerate(polys):\n",
        "            poly = np.array(box).astype(np.int32).reshape((-1))\n",
        "            single_img_result.append(poly)\n",
        "        result.append(single_img_result)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# In[6]:\n",
        "\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "import math\n",
        "import cv2\n",
        "from PIL import Image, JpegImagePlugin\n",
        "from scipy import ndimage\n",
        "import hashlib\n",
        "import sys, os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "if sys.version_info[0] == 2:\n",
        "    from six.moves.urllib.request import urlretrieve\n",
        "else:\n",
        "    from urllib.request import urlretrieve\n",
        "\n",
        "def consecutive(data, mode ='first', stepsize=1):\n",
        "    group = np.split(data, np.where(np.diff(data) != stepsize)[0]+1)\n",
        "    group = [item for item in group if len(item)>0]\n",
        "\n",
        "    if mode == 'first': result = [l[0] for l in group]\n",
        "    elif mode == 'last': result = [l[-1] for l in group]\n",
        "    return result\n",
        "\n",
        "def word_segmentation(mat, separator_idx =  {'th': [1,2],'en': [3,4]}, separator_idx_list = [1,2,3,4]):\n",
        "    result = []\n",
        "    sep_list = []\n",
        "    start_idx = 0\n",
        "    sep_lang = ''\n",
        "    for sep_idx in separator_idx_list:\n",
        "        if sep_idx % 2 == 0: mode ='first'\n",
        "        else: mode ='last'\n",
        "        a = consecutive( np.argwhere(mat == sep_idx).flatten(), mode)\n",
        "        new_sep = [ [item, sep_idx] for item in a]\n",
        "        sep_list += new_sep\n",
        "    sep_list = sorted(sep_list, key=lambda x: x[0])\n",
        "\n",
        "    for sep in sep_list:\n",
        "        for lang in separator_idx.keys():\n",
        "            if sep[1] == separator_idx[lang][0]: # start lang\n",
        "                sep_lang = lang\n",
        "                sep_start_idx = sep[0]\n",
        "            elif sep[1] == separator_idx[lang][1]: # end lang\n",
        "                if sep_lang == lang: # check if last entry if the same start lang\n",
        "                    new_sep_pair = [lang, [sep_start_idx+1, sep[0]-1]]\n",
        "                    if sep_start_idx > start_idx:\n",
        "                        result.append( ['', [start_idx, sep_start_idx-1] ] )\n",
        "                    start_idx = sep[0]+1\n",
        "                    result.append(new_sep_pair)\n",
        "                sep_lang = ''# reset\n",
        "\n",
        "    if start_idx <= len(mat)-1:\n",
        "        result.append( ['', [start_idx, len(mat)-1] ] )\n",
        "    return result\n",
        "\n",
        "# code is based from https://github.com/githubharald/CTCDecoder/blob/master/src/BeamSearch.py\n",
        "class BeamEntry:\n",
        "    \"information about one single beam at specific time-step\"\n",
        "    def __init__(self):\n",
        "        self.prTotal = 0 # blank and non-blank\n",
        "        self.prNonBlank = 0 # non-blank\n",
        "        self.prBlank = 0 # blank\n",
        "        self.prText = 1 # LM score\n",
        "        self.lmApplied = False # flag if LM was already applied to this beam\n",
        "        self.labeling = () # beam-labeling\n",
        "        self.simplified = True  # To run simplyfiy label\n",
        "\n",
        "class BeamState:\n",
        "    \"information about the beams at specific time-step\"\n",
        "    def __init__(self):\n",
        "        self.entries = {}\n",
        "\n",
        "    def norm(self):\n",
        "        \"length-normalise LM score\"\n",
        "        for (k, _) in self.entries.items():\n",
        "            labelingLen = len(self.entries[k].labeling)\n",
        "            self.entries[k].prText = self.entries[k].prText ** (1.0 / (labelingLen if labelingLen else 1.0))\n",
        "\n",
        "    def sort(self):\n",
        "        \"return beam-labelings, sorted by probability\"\n",
        "        beams = [v for (_, v) in self.entries.items()]\n",
        "        sortedBeams = sorted(beams, reverse=True, key=lambda x: x.prTotal*x.prText)\n",
        "        return [x.labeling for x in sortedBeams]\n",
        "\n",
        "    def wordsearch(self, classes, ignore_idx, maxCandidate, dict_list):\n",
        "        beams = [v for (_, v) in self.entries.items()]\n",
        "        sortedBeams = sorted(beams, reverse=True, key=lambda x: x.prTotal*x.prText)\n",
        "        if len(sortedBeams) >  maxCandidate: sortedBeams = sortedBeams[:maxCandidate]\n",
        "\n",
        "        for j, candidate in enumerate(sortedBeams):\n",
        "            idx_list = candidate.labeling\n",
        "            text = ''\n",
        "            for i,l in enumerate(idx_list):\n",
        "                if l not in ignore_idx and (not (i > 0 and idx_list[i - 1] == idx_list[i])):\n",
        "                    text += classes[l]\n",
        "\n",
        "            if j == 0: best_text = text\n",
        "            if text in dict_list:\n",
        "                #print('found text: ', text)\n",
        "                best_text = text\n",
        "                break\n",
        "            else:\n",
        "                pass\n",
        "                #print('not in dict: ', text)\n",
        "        return best_text\n",
        "\n",
        "def applyLM(parentBeam, childBeam, classes, lm):\n",
        "    \"calculate LM score of child beam by taking score from parent beam and bigram probability of last two chars\"\n",
        "    if lm and not childBeam.lmApplied:\n",
        "        c1 = classes[parentBeam.labeling[-1] if parentBeam.labeling else classes.index(' ')] # first char\n",
        "        c2 = classes[childBeam.labeling[-1]] # second char\n",
        "        lmFactor = 0.01 # influence of language model\n",
        "        bigramProb = lm.getCharBigram(c1, c2) ** lmFactor # probability of seeing first and second char next to each other\n",
        "        childBeam.prText = parentBeam.prText * bigramProb # probability of char sequence\n",
        "        childBeam.lmApplied = True # only apply LM once per beam entry\n",
        "\n",
        "def simplify_label(labeling, blankIdx = 0):\n",
        "    labeling = np.array(labeling)\n",
        "\n",
        "    # collapse blank\n",
        "    idx = np.where(~((np.roll(labeling,1) == labeling) & (labeling == blankIdx)))[0]\n",
        "    labeling = labeling[idx]\n",
        "\n",
        "    # get rid of blank between different characters\n",
        "    idx = np.where( ~((np.roll(labeling,1) != np.roll(labeling,-1)) & (labeling == blankIdx)) )[0]\n",
        "\n",
        "    if len(labeling) > 0:\n",
        "        last_idx = len(labeling)-1\n",
        "        if last_idx not in idx: idx = np.append(idx, [last_idx])\n",
        "    labeling = labeling[idx]\n",
        "\n",
        "    return tuple(labeling)\n",
        "\n",
        "def fast_simplify_label(labeling, c, blankIdx=0):\n",
        "\n",
        "    # Adding BlankIDX after Non-Blank IDX\n",
        "    if labeling and c == blankIdx and labeling[-1] != blankIdx:\n",
        "        newLabeling = labeling + (c,)\n",
        "\n",
        "    # Case when a nonBlankChar is added after BlankChar |len(char) - 1\n",
        "    elif labeling and c != blankIdx and labeling[-1] == blankIdx:\n",
        "\n",
        "        # If Blank between same character do nothing | As done by Simplify label\n",
        "        if labeling[-2] == c:\n",
        "            newLabeling = labeling + (c,)\n",
        "\n",
        "        # if blank between different character, remove it | As done by Simplify Label\n",
        "        else:\n",
        "            newLabeling = labeling[:-1] + (c,)\n",
        "\n",
        "    # if consecutive blanks : Keep the original label\n",
        "    elif labeling and c == blankIdx and labeling[-1] == blankIdx:\n",
        "        newLabeling = labeling\n",
        "\n",
        "    # if empty beam & first index is blank\n",
        "    elif not labeling and c == blankIdx:\n",
        "        newLabeling = labeling\n",
        "\n",
        "    # if empty beam & first index is non-blank\n",
        "    elif not labeling and c != blankIdx:\n",
        "        newLabeling = labeling + (c,)\n",
        "\n",
        "    elif labeling and c != blankIdx:\n",
        "        newLabeling = labeling + (c,)\n",
        "\n",
        "    # Cases that might still require simplyfying\n",
        "    else:\n",
        "        newLabeling = labeling + (c,)\n",
        "        newLabeling = simplify_label(newLabeling, blankIdx)\n",
        "\n",
        "    return newLabeling\n",
        "\n",
        "def addBeam(beamState, labeling):\n",
        "    \"add beam if it does not yet exist\"\n",
        "    if labeling not in beamState.entries:\n",
        "        beamState.entries[labeling] = BeamEntry()\n",
        "\n",
        "def ctcBeamSearch(mat, classes, ignore_idx, lm, beamWidth=25, dict_list = []):\n",
        "    blankIdx = 0\n",
        "    maxT, maxC = mat.shape\n",
        "\n",
        "    # initialise beam state\n",
        "    last = BeamState()\n",
        "    labeling = ()\n",
        "    last.entries[labeling] = BeamEntry()\n",
        "    last.entries[labeling].prBlank = 1\n",
        "    last.entries[labeling].prTotal = 1\n",
        "\n",
        "    # go over all time-steps\n",
        "    for t in range(maxT):\n",
        "        curr = BeamState()\n",
        "        # get beam-labelings of best beams\n",
        "        bestLabelings = last.sort()[0:beamWidth]\n",
        "        # go over best beams\n",
        "        for labeling in bestLabelings:\n",
        "            # probability of paths ending with a non-blank\n",
        "            prNonBlank = 0\n",
        "            # in case of non-empty beam\n",
        "            if labeling:\n",
        "                # probability of paths with repeated last char at the end\n",
        "                prNonBlank = last.entries[labeling].prNonBlank * mat[t, labeling[-1]]\n",
        "\n",
        "            # probability of paths ending with a blank\n",
        "            prBlank = (last.entries[labeling].prTotal) * mat[t, blankIdx]\n",
        "\n",
        "            # add beam at current time-step if needed\n",
        "            prev_labeling = labeling\n",
        "            if not last.entries[labeling].simplified:\n",
        "                labeling = simplify_label(labeling, blankIdx)\n",
        "\n",
        "            # labeling = simplify_label(labeling, blankIdx)\n",
        "            addBeam(curr, labeling)\n",
        "\n",
        "            # fill in data\n",
        "            curr.entries[labeling].labeling = labeling\n",
        "            curr.entries[labeling].prNonBlank += prNonBlank\n",
        "            curr.entries[labeling].prBlank += prBlank\n",
        "            curr.entries[labeling].prTotal += prBlank + prNonBlank\n",
        "            curr.entries[labeling].prText = last.entries[prev_labeling].prText\n",
        "            # beam-labeling not changed, therefore also LM score unchanged from\n",
        "\n",
        "            #curr.entries[labeling].lmApplied = True # LM already applied at previous time-step for this beam-labeling\n",
        "\n",
        "            # extend current beam-labeling\n",
        "            # char_highscore = np.argpartition(mat[t, :], -5)[-5:] # run through 5 highest probability\n",
        "            char_highscore = np.where(mat[t, :] >= 0.5/maxC)[0] # run through all probable characters\n",
        "            for c in char_highscore:\n",
        "            #for c in range(maxC - 1):\n",
        "                # add new char to current beam-labeling\n",
        "                # newLabeling = labeling + (c,)\n",
        "                # newLabeling = simplify_label(newLabeling, blankIdx)\n",
        "                newLabeling = fast_simplify_label(labeling, c, blankIdx)\n",
        "\n",
        "                # if new labeling contains duplicate char at the end, only consider paths ending with a blank\n",
        "                if labeling and labeling[-1] == c:\n",
        "                    prNonBlank = mat[t, c] * last.entries[prev_labeling].prBlank\n",
        "                else:\n",
        "                    prNonBlank = mat[t, c] * last.entries[prev_labeling].prTotal\n",
        "\n",
        "                # add beam at current time-step if needed\n",
        "                addBeam(curr, newLabeling)\n",
        "\n",
        "                # fill in data\n",
        "                curr.entries[newLabeling].labeling = newLabeling\n",
        "                curr.entries[newLabeling].prNonBlank += prNonBlank\n",
        "                curr.entries[newLabeling].prTotal += prNonBlank\n",
        "\n",
        "                # apply LM\n",
        "                #applyLM(curr.entries[labeling], curr.entries[newLabeling], classes, lm)\n",
        "\n",
        "        # set new beam state\n",
        "\n",
        "        last = curr\n",
        "\n",
        "    # normalise LM scores according to beam-labeling-length\n",
        "    last.norm()\n",
        "\n",
        "    if dict_list == []:\n",
        "        bestLabeling = last.sort()[0] # get most probable labeling\n",
        "        res = ''\n",
        "        for i,l in enumerate(bestLabeling):\n",
        "            # removing repeated characters and blank.\n",
        "            if l not in ignore_idx and (not (i > 0 and bestLabeling[i - 1] == bestLabeling[i])):\n",
        "                res += classes[l]\n",
        "    else:\n",
        "        res = last.wordsearch(classes, ignore_idx, 20, dict_list)\n",
        "    return res\n",
        "\n",
        "\n",
        "class CTCLabelConverter(object):\n",
        "    \"\"\" Convert between text-label and text-index \"\"\"\n",
        "\n",
        "    def __init__(self, character, separator_list = {}, dict_pathlist = {}):\n",
        "        # character (str): set of the possible characters.\n",
        "        dict_character = list(character)\n",
        "\n",
        "        self.dict = {}\n",
        "        for i, char in enumerate(dict_character):\n",
        "            self.dict[char] = i + 1\n",
        "\n",
        "        self.character = ['[blank]'] + dict_character  # dummy '[blank]' token for CTCLoss (index 0)\n",
        "\n",
        "        self.separator_list = separator_list\n",
        "        separator_char = []\n",
        "        for lang, sep in separator_list.items():\n",
        "            separator_char += sep\n",
        "        self.ignore_idx = [0] + [i+1 for i,item in enumerate(separator_char)]\n",
        "\n",
        "        ####### latin dict\n",
        "        if len(separator_list) == 0:\n",
        "            dict_list = []\n",
        "            for lang, dict_path in dict_pathlist.items():\n",
        "                try:\n",
        "                    with open(dict_path, \"r\", encoding = \"utf-8-sig\") as input_file:\n",
        "                        word_count =  input_file.read().splitlines()\n",
        "                    dict_list += word_count\n",
        "                except:\n",
        "                    pass\n",
        "        else:\n",
        "            dict_list = {}\n",
        "            for lang, dict_path in dict_pathlist.items():\n",
        "                with open(dict_path, \"r\", encoding = \"utf-8-sig\") as input_file:\n",
        "                    word_count =  input_file.read().splitlines()\n",
        "                dict_list[lang] = word_count\n",
        "\n",
        "        self.dict_list = dict_list\n",
        "\n",
        "    def encode(self, text, batch_max_length=25):\n",
        "        \"\"\"convert text-label into text-index.\n",
        "        input:\n",
        "            text: text labels of each image. [batch_size]\n",
        "        output:\n",
        "            text: concatenated text index for CTCLoss.\n",
        "                    [sum(text_lengths)] = [text_index_0 + text_index_1 + ... + text_index_(n - 1)]\n",
        "            length: length of each text. [batch_size]\n",
        "        \"\"\"\n",
        "        length = [len(s) for s in text]\n",
        "        text = ''.join(text)\n",
        "        text = [self.dict[char] for char in text]\n",
        "\n",
        "        return (torch.IntTensor(text), torch.IntTensor(length))\n",
        "\n",
        "    def decode_greedy(self, text_index, length):\n",
        "        \"\"\" convert text-index into text-label. \"\"\"\n",
        "        texts = []\n",
        "        index = 0\n",
        "        for l in length:\n",
        "            t = text_index[index:index + l]\n",
        "            # Returns a boolean array where true is when the value is not repeated\n",
        "            a = np.insert(~((t[1:]==t[:-1])),0,True)\n",
        "            # Returns a boolean array where true is when the value is not in the ignore_idx list\n",
        "            b = ~np.isin(t,np.array(self.ignore_idx))\n",
        "            # Combine the two boolean array\n",
        "            c = a & b\n",
        "            # Gets the corresponding character according to the saved indexes\n",
        "            text = ''.join(np.array(self.character)[t[c.nonzero()]])\n",
        "            texts.append(text)\n",
        "            index += l\n",
        "        return texts\n",
        "\n",
        "    def decode_beamsearch(self, mat, beamWidth=5):\n",
        "        texts = []\n",
        "        for i in range(mat.shape[0]):\n",
        "            t = ctcBeamSearch(mat[i], self.character, self.ignore_idx, None, beamWidth=beamWidth)\n",
        "            texts.append(t)\n",
        "        return texts\n",
        "\n",
        "    def decode_wordbeamsearch(self, mat, beamWidth=5):\n",
        "        texts = []\n",
        "        argmax = np.argmax(mat, axis = 2)\n",
        "\n",
        "        for i in range(mat.shape[0]):\n",
        "            string = ''\n",
        "            # without separators - use space as separator\n",
        "            if len(self.separator_list) == 0:\n",
        "                space_idx = self.dict[' ']\n",
        "\n",
        "                data = np.argwhere(argmax[i]!=space_idx).flatten()\n",
        "                group = np.split(data, np.where(np.diff(data) != 1)[0]+1)\n",
        "                group = [ list(item) for item in group if len(item)>0]\n",
        "\n",
        "                for j, list_idx in enumerate(group):\n",
        "                    matrix = mat[i, list_idx,:]\n",
        "                    t = ctcBeamSearch(matrix, self.character, self.ignore_idx, None,                                      beamWidth=beamWidth, dict_list=self.dict_list)\n",
        "                    if j == 0: string += t\n",
        "                    else: string += ' '+t\n",
        "\n",
        "            # with separators\n",
        "            else:\n",
        "                words = word_segmentation(argmax[i])\n",
        "\n",
        "                for word in words:\n",
        "                    matrix = mat[i, word[1][0]:word[1][1]+1,:]\n",
        "                    if word[0] == '': dict_list = []\n",
        "                    else: dict_list = self.dict_list[word[0]]\n",
        "                    t = ctcBeamSearch(matrix, self.character, self.ignore_idx, None, beamWidth=beamWidth, dict_list=dict_list)\n",
        "                    string += t\n",
        "            texts.append(string)\n",
        "        return texts\n",
        "\n",
        "def four_point_transform(image, rect):\n",
        "    (tl, tr, br, bl) = rect\n",
        "\n",
        "    widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
        "    widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
        "    maxWidth = max(int(widthA), int(widthB))\n",
        "\n",
        "    # compute the height of the new image, which will be the\n",
        "    # maximum distance between the top-right and bottom-right\n",
        "    # y-coordinates or the top-left and bottom-left y-coordinates\n",
        "    heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
        "    heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
        "    maxHeight = max(int(heightA), int(heightB))\n",
        "\n",
        "    dst = np.array([[0, 0],[maxWidth - 1, 0],[maxWidth - 1, maxHeight - 1],[0, maxHeight - 1]], dtype = \"float32\")\n",
        "\n",
        "    # compute the perspective transform matrix and then apply it\n",
        "    M = cv2.getPerspectiveTransform(rect, dst)\n",
        "    warped = cv2.warpPerspective(image, M, (maxWidth, maxHeight))\n",
        "\n",
        "    return warped\n",
        "\n",
        "def group_text_box(polys, slope_ths = 0.1, ycenter_ths = 0.5, height_ths = 0.5, width_ths = 1.0, add_margin = 0.05, sort_output = True):\n",
        "    # poly top-left, top-right, low-right, low-left\n",
        "    horizontal_list, free_list,combined_list, merged_list = [],[],[],[]\n",
        "\n",
        "    for poly in polys:\n",
        "        slope_up = (poly[3]-poly[1])/np.maximum(10, (poly[2]-poly[0]))\n",
        "        slope_down = (poly[5]-poly[7])/np.maximum(10, (poly[4]-poly[6]))\n",
        "        if max(abs(slope_up), abs(slope_down)) < slope_ths:\n",
        "            x_max = max([poly[0],poly[2],poly[4],poly[6]])\n",
        "            x_min = min([poly[0],poly[2],poly[4],poly[6]])\n",
        "            y_max = max([poly[1],poly[3],poly[5],poly[7]])\n",
        "            y_min = min([poly[1],poly[3],poly[5],poly[7]])\n",
        "            horizontal_list.append([x_min, x_max, y_min, y_max, 0.5*(y_min+y_max), y_max-y_min])\n",
        "        else:\n",
        "            height = np.linalg.norm([poly[6]-poly[0],poly[7]-poly[1]])\n",
        "            width = np.linalg.norm([poly[2]-poly[0],poly[3]-poly[1]])\n",
        "\n",
        "            margin = int(1.44*add_margin*min(width, height))\n",
        "\n",
        "            theta13 = abs(np.arctan( (poly[1]-poly[5])/np.maximum(10, (poly[0]-poly[4]))))\n",
        "            theta24 = abs(np.arctan( (poly[3]-poly[7])/np.maximum(10, (poly[2]-poly[6]))))\n",
        "            # do I need to clip minimum, maximum value here?\n",
        "            x1 = poly[0] - np.cos(theta13)*margin\n",
        "            y1 = poly[1] - np.sin(theta13)*margin\n",
        "            x2 = poly[2] + np.cos(theta24)*margin\n",
        "            y2 = poly[3] - np.sin(theta24)*margin\n",
        "            x3 = poly[4] + np.cos(theta13)*margin\n",
        "            y3 = poly[5] + np.sin(theta13)*margin\n",
        "            x4 = poly[6] - np.cos(theta24)*margin\n",
        "            y4 = poly[7] + np.sin(theta24)*margin\n",
        "\n",
        "            free_list.append([[x1,y1],[x2,y2],[x3,y3],[x4,y4]])\n",
        "    if sort_output:\n",
        "        horizontal_list = sorted(horizontal_list, key=lambda item: item[4])\n",
        "\n",
        "    # combine box\n",
        "    new_box = []\n",
        "    for poly in horizontal_list:\n",
        "\n",
        "        if len(new_box) == 0:\n",
        "            b_height = [poly[5]]\n",
        "            b_ycenter = [poly[4]]\n",
        "            new_box.append(poly)\n",
        "        else:\n",
        "            # comparable height and comparable y_center level up to ths*height\n",
        "            if abs(np.mean(b_ycenter) - poly[4]) < ycenter_ths*np.mean(b_height):\n",
        "                b_height.append(poly[5])\n",
        "                b_ycenter.append(poly[4])\n",
        "                new_box.append(poly)\n",
        "            else:\n",
        "                b_height = [poly[5]]\n",
        "                b_ycenter = [poly[4]]\n",
        "                combined_list.append(new_box)\n",
        "                new_box = [poly]\n",
        "    combined_list.append(new_box)\n",
        "\n",
        "    # merge list use sort again\n",
        "    for boxes in combined_list:\n",
        "        if len(boxes) == 1: # one box per line\n",
        "            box = boxes[0]\n",
        "            margin = int(add_margin*min(box[1]-box[0],box[5]))\n",
        "            merged_list.append([box[0]-margin,box[1]+margin,box[2]-margin,box[3]+margin])\n",
        "        else: # multiple boxes per line\n",
        "            boxes = sorted(boxes, key=lambda item: item[0])\n",
        "\n",
        "            merged_box, new_box = [],[]\n",
        "            for box in boxes:\n",
        "                if len(new_box) == 0:\n",
        "                    b_height = [box[5]]\n",
        "                    x_max = box[1]\n",
        "                    new_box.append(box)\n",
        "                else:\n",
        "                    if (abs(np.mean(b_height) - box[5]) < height_ths*np.mean(b_height)) and ((box[0]-x_max) < width_ths *(box[3]-box[2])): # merge boxes\n",
        "                        b_height.append(box[5])\n",
        "                        x_max = box[1]\n",
        "                        new_box.append(box)\n",
        "                    else:\n",
        "                        b_height = [box[5]]\n",
        "                        x_max = box[1]\n",
        "                        merged_box.append(new_box)\n",
        "                        new_box = [box]\n",
        "            if len(new_box) >0: merged_box.append(new_box)\n",
        "\n",
        "            for mbox in merged_box:\n",
        "                if len(mbox) != 1: # adjacent box in same line\n",
        "                    # do I need to add margin here?\n",
        "                    x_min = min(mbox, key=lambda x: x[0])[0]\n",
        "                    x_max = max(mbox, key=lambda x: x[1])[1]\n",
        "                    y_min = min(mbox, key=lambda x: x[2])[2]\n",
        "                    y_max = max(mbox, key=lambda x: x[3])[3]\n",
        "\n",
        "                    box_width = x_max - x_min\n",
        "                    box_height = y_max - y_min\n",
        "                    margin = int(add_margin * (min(box_width, box_height)))\n",
        "\n",
        "                    merged_list.append([x_min-margin, x_max+margin, y_min-margin, y_max+margin])\n",
        "                else: # non adjacent box in same line\n",
        "                    box = mbox[0]\n",
        "\n",
        "                    box_width = box[1] - box[0]\n",
        "                    box_height = box[3] - box[2]\n",
        "                    margin = int(add_margin * (min(box_width, box_height)))\n",
        "\n",
        "                    merged_list.append([box[0]-margin,box[1]+margin,box[2]-margin,box[3]+margin])\n",
        "    # may need to check if box is really in image\n",
        "    return merged_list, free_list\n",
        "\n",
        "def calculate_ratio(width,height):\n",
        "    '''\n",
        "    Calculate aspect ratio for normal use case (w>h) and vertical text (h>w)\n",
        "    '''\n",
        "    ratio = width/height\n",
        "    if ratio<1.0:\n",
        "        ratio = 1./ratio\n",
        "    return ratio\n",
        "\n",
        "def compute_ratio_and_resize(img,width,height,model_height):\n",
        "    '''\n",
        "    Calculate ratio and resize correctly for both horizontal text\n",
        "    and vertical case\n",
        "    '''\n",
        "    ratio = width/height\n",
        "    if ratio<1.0:\n",
        "        ratio = calculate_ratio(width,height)\n",
        "        img = cv2.resize(img,(model_height,int(model_height*ratio)), interpolation=Image.ANTIALIAS)\n",
        "    else:\n",
        "        img = cv2.resize(img,(int(model_height*ratio),model_height),interpolation=Image.ANTIALIAS)\n",
        "    return img,ratio\n",
        "\n",
        "\n",
        "def get_image_list(horizontal_list, free_list, img, model_height = 64, sort_output = True):\n",
        "    image_list = []\n",
        "    maximum_y,maximum_x = img.shape\n",
        "\n",
        "    max_ratio_hori, max_ratio_free = 1,1\n",
        "    for box in free_list:\n",
        "        rect = np.array(box, dtype = \"float32\")\n",
        "        transformed_img = four_point_transform(img, rect)\n",
        "        ratio = calculate_ratio(transformed_img.shape[1],transformed_img.shape[0])\n",
        "        new_width = int(model_height*ratio)\n",
        "        if new_width == 0:\n",
        "            pass\n",
        "        else:\n",
        "            crop_img,ratio = compute_ratio_and_resize(transformed_img,transformed_img.shape[1],transformed_img.shape[0],model_height)\n",
        "            image_list.append( (box,crop_img) ) # box = [[x1,y1],[x2,y2],[x3,y3],[x4,y4]]\n",
        "            max_ratio_free = max(ratio, max_ratio_free)\n",
        "\n",
        "\n",
        "    max_ratio_free = math.ceil(max_ratio_free)\n",
        "\n",
        "    for box in horizontal_list:\n",
        "        x_min = max(0,box[0])\n",
        "        x_max = min(box[1],maximum_x)\n",
        "        y_min = max(0,box[2])\n",
        "        y_max = min(box[3],maximum_y)\n",
        "        crop_img = img[y_min : y_max, x_min:x_max]\n",
        "        width = x_max - x_min\n",
        "        height = y_max - y_min\n",
        "        ratio = calculate_ratio(width,height)\n",
        "        new_width = int(model_height*ratio)\n",
        "        if new_width == 0:\n",
        "            pass\n",
        "        else:\n",
        "            crop_img,ratio = compute_ratio_and_resize(crop_img,width,height,model_height)\n",
        "            image_list.append( ( [[x_min,y_min],[x_max,y_min],[x_max,y_max],[x_min,y_max]] ,crop_img) )\n",
        "            max_ratio_hori = max(ratio, max_ratio_hori)\n",
        "\n",
        "    max_ratio_hori = math.ceil(max_ratio_hori)\n",
        "    max_ratio = max(max_ratio_hori, max_ratio_free)\n",
        "    max_width = math.ceil(max_ratio)*model_height\n",
        "\n",
        "    if sort_output:\n",
        "        image_list = sorted(image_list, key=lambda item: item[0][0][1]) # sort by vertical position\n",
        "    return image_list, max_width\n",
        "\n",
        "def download_and_unzip(url, filename, model_storage_directory, verbose=True):\n",
        "    zip_path = os.path.join(model_storage_directory, 'temp.zip')\n",
        "    reporthook = printProgressBar(prefix='Progress:', suffix='Complete', length=50) if verbose else None\n",
        "    urlretrieve(url, zip_path, reporthook=reporthook)\n",
        "    with ZipFile(zip_path, 'r') as zipObj:\n",
        "        zipObj.extract(filename, model_storage_directory)\n",
        "    os.remove(zip_path)\n",
        "\n",
        "def calculate_md5(fname):\n",
        "    hash_md5 = hashlib.md5()\n",
        "    with open(fname, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
        "            hash_md5.update(chunk)\n",
        "    return hash_md5.hexdigest()\n",
        "\n",
        "def diff(input_list):\n",
        "    return max(input_list)-min(input_list)\n",
        "\n",
        "def get_paragraph(raw_result, x_ths=1, y_ths=0.5, mode = 'ltr'):\n",
        "    # create basic attributes\n",
        "    box_group = []\n",
        "    for box in raw_result:\n",
        "        all_x = [int(coord[0]) for coord in box[0]]\n",
        "        all_y = [int(coord[1]) for coord in box[0]]\n",
        "        min_x = min(all_x)\n",
        "        max_x = max(all_x)\n",
        "        min_y = min(all_y)\n",
        "        max_y = max(all_y)\n",
        "        height = max_y - min_y\n",
        "        box_group.append([box[1], min_x, max_x, min_y, max_y, height, 0.5*(min_y+max_y), 0]) # last element indicates group\n",
        "    # cluster boxes into paragraph\n",
        "    current_group = 1\n",
        "    while len([box for box in box_group if box[7]==0]) > 0:\n",
        "        box_group0 = [box for box in box_group if box[7]==0] # group0 = non-group\n",
        "        # new group\n",
        "        if len([box for box in box_group if box[7]==current_group]) == 0:\n",
        "            box_group0[0][7] = current_group # assign first box to form new group\n",
        "        # try to add group\n",
        "        else:\n",
        "            current_box_group = [box for box in box_group if box[7]==current_group]\n",
        "            mean_height = np.mean([box[5] for box in current_box_group])\n",
        "            min_gx = min([box[1] for box in current_box_group]) - x_ths*mean_height\n",
        "            max_gx = max([box[2] for box in current_box_group]) + x_ths*mean_height\n",
        "            min_gy = min([box[3] for box in current_box_group]) - y_ths*mean_height\n",
        "            max_gy = max([box[4] for box in current_box_group]) + y_ths*mean_height\n",
        "            add_box = False\n",
        "            for box in box_group0:\n",
        "                same_horizontal_level = (min_gx<=box[1]<=max_gx) or (min_gx<=box[2]<=max_gx)\n",
        "                same_vertical_level = (min_gy<=box[3]<=max_gy) or (min_gy<=box[4]<=max_gy)\n",
        "                if same_horizontal_level and same_vertical_level:\n",
        "                    box[7] = current_group\n",
        "                    add_box = True\n",
        "                    break\n",
        "            # cannot add more box, go to next group\n",
        "            if add_box==False:\n",
        "                current_group += 1\n",
        "    # arrage order in paragraph\n",
        "    result = []\n",
        "    for i in set(box[7] for box in box_group):\n",
        "        current_box_group = [box for box in box_group if box[7]==i]\n",
        "        mean_height = np.mean([box[5] for box in current_box_group])\n",
        "        min_gx = min([box[1] for box in current_box_group])\n",
        "        max_gx = max([box[2] for box in current_box_group])\n",
        "        min_gy = min([box[3] for box in current_box_group])\n",
        "        max_gy = max([box[4] for box in current_box_group])\n",
        "\n",
        "        text = ''\n",
        "        while len(current_box_group) > 0:\n",
        "            highest = min([box[6] for box in current_box_group])\n",
        "            candidates = [box for box in current_box_group if box[6]<highest+0.4*mean_height]\n",
        "            # get the far left\n",
        "            if mode == 'ltr':\n",
        "                most_left = min([box[1] for box in candidates])\n",
        "                for box in candidates:\n",
        "                    if box[1] == most_left: best_box = box\n",
        "            elif mode == 'rtl':\n",
        "                most_right = max([box[2] for box in candidates])\n",
        "                for box in candidates:\n",
        "                    if box[2] == most_right: best_box = box\n",
        "            text += ' '+best_box[0]\n",
        "            current_box_group.remove(best_box)\n",
        "\n",
        "        result.append([ [[min_gx,min_gy],[max_gx,min_gy],[max_gx,max_gy],[min_gx,max_gy]], text[1:]])\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def printProgressBar (prefix = '', suffix = '', decimals = 1, length = 100, fill = '█', printEnd = \"\\r\"):\n",
        "    \"\"\"\n",
        "    Call in a loop to create terminal progress bar\n",
        "    @params:\n",
        "        prefix      - Optional  : prefix string (Str)\n",
        "        suffix      - Optional  : suffix string (Str)\n",
        "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
        "        length      - Optional  : character length of bar (Int)\n",
        "        fill        - Optional  : bar fill character (Str)\n",
        "        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
        "    \"\"\"\n",
        "    def progress_hook(count, blockSize, totalSize):\n",
        "        progress = count * blockSize / totalSize\n",
        "        percent = (\"{0:.\" + str(decimals) + \"f}\").format(progress * 100)\n",
        "        filledLength = int(length * progress)\n",
        "        bar = fill * filledLength + '-' * (length - filledLength)\n",
        "        print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end = printEnd)\n",
        "\n",
        "    return progress_hook\n",
        "\n",
        "def reformat_input(image):\n",
        "    if type(image) == str:\n",
        "        if image.startswith('http://') or image.startswith('https://'):\n",
        "            tmp, _ = urlretrieve(image , reporthook=printProgressBar(prefix = 'Progress:', suffix = 'Complete', length = 50))\n",
        "            img_cv_grey = cv2.imread(tmp, cv2.IMREAD_GRAYSCALE)\n",
        "            os.remove(tmp)\n",
        "        else:\n",
        "            img_cv_grey = cv2.imread(image, cv2.IMREAD_GRAYSCALE)\n",
        "            image = os.path.expanduser(image)\n",
        "        img = loadImage(image)  # can accept URL\n",
        "    elif type(image) == bytes:\n",
        "        nparr = np.frombuffer(image, np.uint8)\n",
        "        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img_cv_grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    elif type(image) == np.ndarray:\n",
        "        if len(image.shape) == 2: # grayscale\n",
        "            img_cv_grey = image\n",
        "            img = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
        "        elif len(image.shape) == 3 and image.shape[2] == 1:\n",
        "            img_cv_grey = np.squeeze(image)\n",
        "            img = cv2.cvtColor(img_cv_grey, cv2.COLOR_GRAY2BGR)\n",
        "        elif len(image.shape) == 3 and image.shape[2] == 3: # BGRscale\n",
        "            img = image\n",
        "            img_cv_grey = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        elif len(image.shape) == 3 and image.shape[2] == 4: # RGBAscale\n",
        "            img = image[:,:,:3]\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "            img_cv_grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    elif type(image) == JpegImagePlugin.JpegImageFile:\n",
        "        image_array = np.array(image)\n",
        "        img = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)\n",
        "        img_cv_grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    else:\n",
        "        raise ValueError('Invalid input type. Supporting format = string(file path or url), bytes, numpy array')\n",
        "\n",
        "    return img, img_cv_grey\n",
        "\n",
        "\n",
        "def reformat_input_batched(image, n_width=None, n_height=None):\n",
        "    \"\"\"\n",
        "    reformats an image or list of images or a 4D numpy image array &\n",
        "    returns a list of corresponding img, img_cv_grey nd.arrays\n",
        "    image:\n",
        "        [file path, numpy-array, byte stream object,\n",
        "        list of file paths, list of numpy-array, 4D numpy array,\n",
        "        list of byte stream objects]\n",
        "    \"\"\"\n",
        "    if ((isinstance(image, np.ndarray) and len(image.shape) == 4) or isinstance(image, list)):\n",
        "        # process image batches if image is list of image np arr, paths, bytes\n",
        "        img, img_cv_grey = [], []\n",
        "        for single_img in image:\n",
        "            clr, gry = reformat_input(single_img)\n",
        "            if n_width is not None and n_height is not None:\n",
        "                clr = cv2.resize(clr, (n_width, n_height))\n",
        "                gry = cv2.resize(gry, (n_width, n_height))\n",
        "            img.append(clr)\n",
        "            img_cv_grey.append(gry)\n",
        "        img, img_cv_grey = np.array(img), np.array(img_cv_grey)\n",
        "        # ragged tensors created when all input imgs are not of the same size\n",
        "        if len(img.shape) == 1 and len(img_cv_grey.shape) == 1:\n",
        "            raise ValueError(\"The input image array contains images of different sizes. \" +\n",
        "                             \"Please resize all images to same shape or pass n_width, n_height to auto-resize\")\n",
        "    else:\n",
        "        img, img_cv_grey = reformat_input(image)\n",
        "    return img, img_cv_grey\n",
        "\n",
        "\n",
        "\n",
        "def make_rotated_img_list(rotationInfo, img_list):\n",
        "\n",
        "    result_img_list = img_list[:]\n",
        "\n",
        "    # add rotated images to original image_list\n",
        "    max_ratio=1\n",
        "    \n",
        "    for angle in rotationInfo:\n",
        "        for img_info in img_list : \n",
        "            rotated = ndimage.rotate(img_info[1], angle, reshape=True) \n",
        "            height,width = rotated.shape\n",
        "            ratio = calculate_ratio(width,height)\n",
        "            max_ratio = max(max_ratio,ratio)\n",
        "            result_img_list.append((img_info[0], rotated))\n",
        "    return result_img_list\n",
        "\n",
        "\n",
        "def set_result_with_confidence(results):\n",
        "    \"\"\" Select highest confidence augmentation for TTA\n",
        "    Given a list of lists of results (outer list has one list per augmentation,\n",
        "    inner lists index the images being recognized), choose the best result \n",
        "    according to confidence level.\n",
        "    Each \"result\" is of the form (box coords, text, confidence)\n",
        "    A final_result is returned which contains one result for each image\n",
        "    \"\"\"\n",
        "    final_result = []\n",
        "    for col_ix in range(len(results[0])):\n",
        "        # Take the row_ix associated with the max confidence\n",
        "        best_row = max(\n",
        "            [(row_ix, results[row_ix][col_ix][2]) for row_ix in range(len(results))],\n",
        "            key=lambda x: x[1])[0]\n",
        "        final_result.append(results[best_row][col_ix])\n",
        "\n",
        "    return final_result\n",
        "\n",
        "\n",
        "# In[7]:\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "import importlib\n",
        "import math\n",
        "\n",
        "def custom_mean(x):\n",
        "    return x.prod()**(2.0/np.sqrt(len(x)))\n",
        "\n",
        "def contrast_grey(img):\n",
        "    high = np.percentile(img, 90)\n",
        "    low  = np.percentile(img, 10)\n",
        "    return (high-low)/np.maximum(10, high+low), high, low\n",
        "\n",
        "def adjust_contrast_grey(img, target = 0.4):\n",
        "    contrast, high, low = contrast_grey(img)\n",
        "    if contrast < target:\n",
        "        img = img.astype(int)\n",
        "        ratio = 200./np.maximum(10, high-low)\n",
        "        img = (img - low + 25)*ratio\n",
        "        img = np.maximum(np.full(img.shape, 0) ,np.minimum(np.full(img.shape, 255), img)).astype(np.uint8)\n",
        "    return img\n",
        "\n",
        "class NormalizePAD(object):\n",
        "\n",
        "    def __init__(self, max_size, PAD_type='right'):\n",
        "        self.toTensor = transforms.ToTensor()\n",
        "        self.max_size = max_size\n",
        "        self.max_width_half = math.floor(max_size[2] / 2)\n",
        "        self.PAD_type = PAD_type\n",
        "\n",
        "    def __call__(self, img):\n",
        "        img = self.toTensor(img)\n",
        "        img.sub_(0.5).div_(0.5)\n",
        "        c, h, w = img.size()\n",
        "        Pad_img = torch.FloatTensor(*self.max_size).fill_(0)\n",
        "        Pad_img[:, :, :w] = img  # right pad\n",
        "        if self.max_size[2] != w:  # add border Pad\n",
        "            Pad_img[:, :, w:] = img[:, :, w - 1].unsqueeze(2).expand(c, h, self.max_size[2] - w)\n",
        "\n",
        "        return Pad_img\n",
        "\n",
        "class ListDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, image_list):\n",
        "        self.image_list = image_list\n",
        "        self.nSamples = len(image_list)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.nSamples\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = self.image_list[index]\n",
        "        return Image.fromarray(img, 'L')\n",
        "\n",
        "class AlignCollate(object):\n",
        "\n",
        "    def __init__(self, imgH=32, imgW=100, keep_ratio_with_pad=False, adjust_contrast = 0.):\n",
        "        self.imgH = imgH\n",
        "        self.imgW = imgW\n",
        "        self.keep_ratio_with_pad = keep_ratio_with_pad\n",
        "        self.adjust_contrast = adjust_contrast\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        batch = filter(lambda x: x is not None, batch)\n",
        "        images = batch\n",
        "\n",
        "        resized_max_w = self.imgW\n",
        "        input_channel = 1\n",
        "        transform = NormalizePAD((input_channel, self.imgH, resized_max_w))\n",
        "\n",
        "        resized_images = []\n",
        "        for image in images:\n",
        "            w, h = image.size\n",
        "            #### augmentation here - change contrast\n",
        "            if self.adjust_contrast > 0:\n",
        "                image = np.array(image.convert(\"L\"))\n",
        "                image = adjust_contrast_grey(image, target = self.adjust_contrast)\n",
        "                image = Image.fromarray(image, 'L')\n",
        "\n",
        "            ratio = w / float(h)\n",
        "            if math.ceil(self.imgH * ratio) > self.imgW:\n",
        "                resized_w = self.imgW\n",
        "            else:\n",
        "                resized_w = math.ceil(self.imgH * ratio)\n",
        "\n",
        "            resized_image = image.resize((resized_w, self.imgH), Image.BICUBIC)\n",
        "            resized_images.append(transform(resized_image))\n",
        "\n",
        "        image_tensors = torch.cat([t.unsqueeze(0) for t in resized_images], 0)\n",
        "        return image_tensors\n",
        "\n",
        "def recognizer_predict(model, converter, test_loader, batch_max_length,                       ignore_idx, char_group_idx, decoder = 'greedy', beamWidth= 5, device = 'cpu'):\n",
        "    model.eval()\n",
        "    result = []\n",
        "    with torch.no_grad():\n",
        "        for image_tensors in test_loader:\n",
        "            batch_size = image_tensors.size(0)\n",
        "            image = image_tensors.to(device)\n",
        "            # For max length prediction\n",
        "            length_for_pred = torch.IntTensor([batch_max_length] * batch_size).to(device)\n",
        "            text_for_pred = torch.LongTensor(batch_size, batch_max_length + 1).fill_(0).to(device)\n",
        "\n",
        "            preds = model(image, text_for_pred)\n",
        "\n",
        "            # Select max probabilty (greedy decoding) then decode index to character\n",
        "            preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
        "\n",
        "            ######## filter ignore_char, rebalance\n",
        "            preds_prob = F.softmax(preds, dim=2)\n",
        "            preds_prob = preds_prob.cpu().detach().numpy()\n",
        "            preds_prob[:,:,ignore_idx] = 0.\n",
        "            pred_norm = preds_prob.sum(axis=2)\n",
        "            preds_prob = preds_prob/np.expand_dims(pred_norm, axis=-1)\n",
        "            preds_prob = torch.from_numpy(preds_prob).float().to(device)\n",
        "\n",
        "            if decoder == 'greedy':\n",
        "                # Select max probabilty (greedy decoding) then decode index to character\n",
        "                _, preds_index = preds_prob.max(2)\n",
        "                preds_index = preds_index.view(-1)\n",
        "                preds_str = converter.decode_greedy(preds_index.data.cpu().detach().numpy(), preds_size.data)\n",
        "            elif decoder == 'beamsearch':\n",
        "                k = preds_prob.cpu().detach().numpy()\n",
        "                preds_str = converter.decode_beamsearch(k, beamWidth=beamWidth)\n",
        "            elif decoder == 'wordbeamsearch':\n",
        "                k = preds_prob.cpu().detach().numpy()\n",
        "                preds_str = converter.decode_wordbeamsearch(k, beamWidth=beamWidth)\n",
        "\n",
        "            preds_prob = preds_prob.cpu().detach().numpy()\n",
        "            values = preds_prob.max(axis=2)\n",
        "            indices = preds_prob.argmax(axis=2)\n",
        "            preds_max_prob = []\n",
        "            for v,i in zip(values, indices):\n",
        "                max_probs = v[i!=0]\n",
        "                if len(max_probs)>0:\n",
        "                    preds_max_prob.append(max_probs)\n",
        "                else:\n",
        "                    preds_max_prob.append(np.array([0]))\n",
        "\n",
        "            for pred, pred_max_prob in zip(preds_str, preds_max_prob):\n",
        "                confidence_score = custom_mean(pred_max_prob)\n",
        "                result.append([pred, confidence_score])\n",
        "\n",
        "    return result\n",
        "\n",
        "def get_recognizer(recog_network, network_params, character,                   separator_list, dict_list, model_path,                   device = 'cpu', quantize = True):\n",
        "\n",
        "    converter = CTCLabelConverter(character, separator_list, dict_list)\n",
        "    num_class = len(converter.character)\n",
        "\n",
        "    if recog_network == 'generation1':\n",
        "        model_pkg = importlib.import_module(\"model.model\")\n",
        "    elif recog_network == 'generation2':\n",
        "        model_pkg = importlib.import_module(\"model.vgg_model\")\n",
        "    else:\n",
        "        model_pkg = importlib.import_module(recog_network)\n",
        "    \n",
        "    model = model_pkg.Model(num_class=num_class, **network_params)\n",
        "\n",
        "    if device == 'cpu':\n",
        "        state_dict = torch.load(model_path, map_location=device)\n",
        "        new_state_dict = OrderedDict()\n",
        "        for key, value in state_dict.items():\n",
        "            new_key = key[7:]\n",
        "            new_state_dict[new_key] = value\n",
        "        model.load_state_dict(new_state_dict)\n",
        "        if quantize:\n",
        "            try:\n",
        "                torch.quantization.quantize_dynamic(model, dtype=torch.qint8, inplace=True)\n",
        "            except:\n",
        "                pass\n",
        "    else:\n",
        "        model = torch.nn.DataParallel(model).to(device)\n",
        "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "\n",
        "    return model, converter\n",
        "\n",
        "def get_text(character, imgH, imgW, recognizer, converter, image_list,             ignore_char = '',decoder = 'greedy', beamWidth =5, batch_size=1, contrast_ths=0.1,             adjust_contrast=0.5, filter_ths = 0.003, workers = 1, device = 'cpu'):\n",
        "    batch_max_length = int(imgW/10)\n",
        "\n",
        "    char_group_idx = {}\n",
        "    ignore_idx = []\n",
        "    for char in ignore_char:\n",
        "        try: ignore_idx.append(character.index(char)+1)\n",
        "        except: pass\n",
        "\n",
        "    coord = [item[0] for item in image_list]\n",
        "    img_list = [item[1] for item in image_list]\n",
        "    AlignCollate_normal = AlignCollate(imgH=imgH, imgW=imgW, keep_ratio_with_pad=True)\n",
        "    test_data = ListDataset(img_list)\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_data, batch_size=batch_size, shuffle=False,\n",
        "        num_workers=int(workers), collate_fn=AlignCollate_normal, pin_memory=True)\n",
        "\n",
        "    # predict first round\n",
        "    result1 = recognizer_predict(recognizer, converter, test_loader,batch_max_length,                                 ignore_idx, char_group_idx, decoder, beamWidth, device = device)\n",
        "\n",
        "    # predict second round\n",
        "    low_confident_idx = [i for i,item in enumerate(result1) if (item[1] < contrast_ths)]\n",
        "    if len(low_confident_idx) > 0:\n",
        "        img_list2 = [img_list[i] for i in low_confident_idx]\n",
        "        AlignCollate_contrast = AlignCollate(imgH=imgH, imgW=imgW, keep_ratio_with_pad=True, adjust_contrast=adjust_contrast)\n",
        "        test_data = ListDataset(img_list2)\n",
        "        test_loader = torch.utils.data.DataLoader(\n",
        "                        test_data, batch_size=batch_size, shuffle=False,\n",
        "                        num_workers=int(workers), collate_fn=AlignCollate_contrast, pin_memory=True)\n",
        "        result2 = recognizer_predict(recognizer, converter, test_loader, batch_max_length,                                     ignore_idx, char_group_idx, decoder, beamWidth, device = device)\n",
        "\n",
        "    result = []\n",
        "    for i, zipped in enumerate(zip(coord, result1)):\n",
        "        box, pred1 = zipped\n",
        "        if i in low_confident_idx:\n",
        "            pred2 = result2[low_confident_idx.index(i)]\n",
        "            if pred1[1]>pred2[1]:\n",
        "                result.append( (box, pred1[0], pred1[1]) )\n",
        "            else:\n",
        "                result.append( (box, pred2[0], pred2[1]) )\n",
        "        else:\n",
        "            result.append( (box, pred1[0], pred1[1]) )\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# In[8]:\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"LRU_CACHE_CAPACITY\"] = \"1\"\n",
        "\n",
        "BASE_PATH = os.path.dirname(\"__file__\")\n",
        "MODULE_PATH = os.path.dirname(\"__file__\")\n",
        "\n",
        "# detector parameters\n",
        "detection_models = {\n",
        "    'craft' : {\n",
        "        'filename': 'craft_mlt_25k.pth',\n",
        "        'filesize': '2f8227d2def4037cdb3b34389dcf9ec1'\n",
        "    }\n",
        "}\n",
        "\n",
        "# recognizer parameters\n",
        "latin_lang_list = ['af','az','bs','cs','cy','da','de','en','es','et','fr','ga',                   'hr','hu','id','is','it','ku','la','lt','lv','mi','ms','mt',                   'nl','no','oc','pi','pl','pt','ro','rs_latin','sk','sl','sq',                   'sv','sw','tl','tr','uz','vi']\n",
        "arabic_lang_list = ['ar','fa','ug','ur']\n",
        "bengali_lang_list = ['bn','as','mni']\n",
        "cyrillic_lang_list = ['ru','rs_cyrillic','be','bg','uk','mn','abq','ady','kbd',                      'ava','dar','inh','che','lbe','lez','tab','tjk']\n",
        "devanagari_lang_list = ['hi','mr','ne','bh','mai','ang','bho','mah','sck','new',                        'gom','sa','bgc']\n",
        "other_lang_list = ['th','ch_sim','ch_tra','ja','ko','ta','te','kn']\n",
        "\n",
        "all_lang_list = latin_lang_list + arabic_lang_list+ cyrillic_lang_list +                devanagari_lang_list + bengali_lang_list + other_lang_list\n",
        "imgH = 64\n",
        "separator_list = {\n",
        "    'th': ['\\xa2', '\\xa3'],\n",
        "    'en': ['\\xa4', '\\xa5']\n",
        "}\n",
        "separator_char = []\n",
        "for lang, sep in separator_list.items():\n",
        "    separator_char += sep\n",
        "\n",
        "recognition_models = {\n",
        "    'gen1' : {\n",
        "        \n",
        "    },\n",
        "    'gen2' : {\n",
        "        'english_g2':{\n",
        "            'filename': 'english_g2.pth',\n",
        "            'model_script': 'english',\n",
        "            'filesize': '5864788e1821be9e454ec108d61b887d',\n",
        "            'symbols': \"0123456789!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~ €\",\n",
        "            'characters': \"0123456789!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~ €ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "# In[9]:\n",
        "\n",
        "\n",
        "from bidi.algorithm import get_display\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import os\n",
        "import sys\n",
        "from PIL import Image\n",
        "from logging import getLogger\n",
        "import yaml\n",
        "\n",
        "if sys.version_info[0] == 2:\n",
        "    from io import open\n",
        "    from six.moves.urllib.request import urlretrieve\n",
        "    from pathlib2 import Path\n",
        "else:\n",
        "    from urllib.request import urlretrieve\n",
        "    from pathlib import Path\n",
        "\n",
        "LOGGER = getLogger(__name__)\n",
        "\n",
        "class Reader(object):\n",
        "\n",
        "    def __init__(self, lang_list, gpu=True, model_storage_directory=None,\n",
        "                 user_network_directory=None, recog_network = 'standard',\n",
        "                 download_enabled=True, detector=True, recognizer=True,\n",
        "                 verbose=True, quantize=True, cudnn_benchmark=False):\n",
        "        \"\"\"Create an Reader\n",
        "        Parameters:\n",
        "            lang_list (list): Language codes (ISO 639) for languages to be recognized during analysis.\n",
        "            gpu (bool): Enable GPU support (default)\n",
        "            model_storage_directory (string): Path to directory for model data. If not specified,\n",
        "            models will be read from a directory as defined by the environment variable MODULE_PATH (if defined).\n",
        "            user_network_directory (string): Path to directory for custom network architecture.\n",
        "            If not specified, it is as defined by the environment variable MODULE_PATH (if defined).\n",
        "            download_enabled (bool): Enabled downloading of model data via HTTP (default).\n",
        "        \"\"\"\n",
        "        self.download_enabled = download_enabled\n",
        "\n",
        "        self.model_storage_directory = MODULE_PATH + '/model'\n",
        "        if model_storage_directory:\n",
        "            self.model_storage_directory = model_storage_directory\n",
        "        Path(self.model_storage_directory).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        self.user_network_directory = MODULE_PATH + '/user_network'\n",
        "        if user_network_directory:\n",
        "            self.user_network_directory = user_network_directory\n",
        "        Path(self.user_network_directory).mkdir(parents=True, exist_ok=True)\n",
        "        sys.path.append(self.user_network_directory)\n",
        "\n",
        "        if gpu is False:\n",
        "            self.device = 'cpu'\n",
        "            if verbose:\n",
        "                LOGGER.warning('Using CPU. Note: This module is much faster with a GPU.')\n",
        "        elif not torch.cuda.is_available():\n",
        "            self.device = 'cpu'\n",
        "            if verbose:\n",
        "                LOGGER.warning('CUDA not available - defaulting to CPU. Note: This module is much faster with a GPU.')\n",
        "        elif gpu is True:\n",
        "            self.device = 'cuda'\n",
        "        else:\n",
        "            self.device = gpu\n",
        "        self.recognition_models = recognition_models\n",
        "\n",
        "        # check and download detection model\n",
        "        detector_model = 'craft'\n",
        "        corrupt_msg = 'MD5 hash mismatch, possible file corruption'\n",
        "        detector_path = os.path.join(self.model_storage_directory, detection_models[detector_model]['filename'])\n",
        "        if detector:\n",
        "            if os.path.isfile(detector_path) == False:\n",
        "                if not self.download_enabled:\n",
        "                    raise FileNotFoundError(\"Missing %s and downloads disabled\" % detector_path)\n",
        "                LOGGER.warning('Downloading detection model, please wait. '\n",
        "                               'This may take several minutes depending upon your network connection.')\n",
        "                download_and_unzip(detection_models[detector_model]['url'], detection_models[detector_model]['filename'], self.model_storage_directory, verbose)\n",
        "                assert calculate_md5(detector_path) == detection_models[detector_model]['filesize'], corrupt_msg\n",
        "                LOGGER.info('Download complete')\n",
        "            elif calculate_md5(detector_path) != detection_models[detector_model]['filesize']:\n",
        "                if not self.download_enabled:\n",
        "                    raise FileNotFoundError(\"MD5 mismatch for %s and downloads disabled\" % detector_path)\n",
        "                LOGGER.warning(corrupt_msg)\n",
        "                os.remove(detector_path)\n",
        "                LOGGER.warning('Re-downloading the detection model, please wait. '\n",
        "                               'This may take several minutes depending upon your network connection.')\n",
        "                download_and_unzip(detection_models[detector_model]['url'], detection_models[detector_model]['filename'], self.model_storage_directory, verbose)\n",
        "                assert calculate_md5(detector_path) == detection_models[detector_model]['filesize'], corrupt_msg\n",
        "\n",
        "        # recognition model\n",
        "        separator_list = {}\n",
        "\n",
        "        if recog_network in ['standard'] + [model for model in recognition_models['gen1']] + [model for model in recognition_models['gen2']]:\n",
        "            if recog_network in [model for model in recognition_models['gen1']]:\n",
        "                model = recognition_models['gen1'][recog_network]\n",
        "                recog_network = 'generation1'\n",
        "                self.model_lang = model['model_script']\n",
        "            elif recog_network in [model for model in recognition_models['gen2']]:\n",
        "                model = recognition_models['gen2'][recog_network]\n",
        "                recog_network = 'generation2'\n",
        "                self.model_lang = model['model_script']\n",
        "            else: # auto-detect\n",
        "                unknown_lang = set(lang_list) - set(all_lang_list)\n",
        "                if unknown_lang != set():\n",
        "                    raise ValueError(unknown_lang, 'is not supported')\n",
        "                # choose recognition model\n",
        "                if lang_list == ['en']:\n",
        "                    self.setModelLanguage('english', lang_list, ['en'], '[\"en\"]')\n",
        "                    model = recognition_models['gen2']['english_g2']\n",
        "                    recog_network = 'generation2'\n",
        "                elif 'th' in lang_list:\n",
        "                    self.setModelLanguage('thai', lang_list, ['th','en'], '[\"th\",\"en\"]')\n",
        "                    model = recognition_models['gen1']['thai_g1']\n",
        "                    recog_network = 'generation1'\n",
        "                elif 'ch_tra' in lang_list:\n",
        "                    self.setModelLanguage('chinese_tra', lang_list, ['ch_tra','en'], '[\"ch_tra\",\"en\"]')\n",
        "                    model = recognition_models['gen1']['zh_tra_g1']\n",
        "                    recog_network = 'generation1'\n",
        "                elif 'ch_sim' in lang_list:\n",
        "                    self.setModelLanguage('chinese_sim', lang_list, ['ch_sim','en'], '[\"ch_sim\",\"en\"]')\n",
        "                    model = recognition_models['gen2']['zh_sim_g2']\n",
        "                    recog_network = 'generation2'\n",
        "                elif 'ja' in lang_list:\n",
        "                    self.setModelLanguage('japanese', lang_list, ['ja','en'], '[\"ja\",\"en\"]')\n",
        "                    model = recognition_models['gen2']['japanese_g2']\n",
        "                    recog_network = 'generation2'\n",
        "                elif 'ko' in lang_list:\n",
        "                    self.setModelLanguage('korean', lang_list, ['ko','en'], '[\"ko\",\"en\"]')\n",
        "                    model = recognition_models['gen2']['korean_g2']\n",
        "                    recog_network = 'generation2'\n",
        "                elif 'ta' in lang_list:\n",
        "                    self.setModelLanguage('tamil', lang_list, ['ta','en'], '[\"ta\",\"en\"]')\n",
        "                    model = recognition_models['gen1']['tamil_g1']\n",
        "                    recog_network = 'generation1'\n",
        "                elif 'te' in lang_list:\n",
        "                    self.setModelLanguage('telugu', lang_list, ['te','en'], '[\"te\",\"en\"]')\n",
        "                    model = recognition_models['gen2']['telugu_g2']\n",
        "                    recog_network = 'generation2'\n",
        "                elif 'kn' in lang_list:\n",
        "                    self.setModelLanguage('kannada', lang_list, ['kn','en'], '[\"kn\",\"en\"]')\n",
        "                    model = recognition_models['gen2']['kannada_g2']\n",
        "                    recog_network = 'generation2'\n",
        "                elif set(lang_list) & set(bengali_lang_list):\n",
        "                    self.setModelLanguage('bengali', lang_list, bengali_lang_list+['en'], '[\"bn\",\"as\",\"en\"]')\n",
        "                    model = recognition_models['gen1']['bengali_g1']\n",
        "                    recog_network = 'generation1'\n",
        "                elif set(lang_list) & set(arabic_lang_list):\n",
        "                    self.setModelLanguage('arabic', lang_list, arabic_lang_list+['en'], '[\"ar\",\"fa\",\"ur\",\"ug\",\"en\"]')\n",
        "                    model = recognition_models['gen1']['arabic_g1']\n",
        "                    recog_network = 'generation1'\n",
        "                elif set(lang_list) & set(devanagari_lang_list):\n",
        "                    self.setModelLanguage('devanagari', lang_list, devanagari_lang_list+['en'], '[\"hi\",\"mr\",\"ne\",\"en\"]')\n",
        "                    model = recognition_models['gen1']['devanagari_g1']\n",
        "                    recog_network = 'generation1'\n",
        "                elif set(lang_list) & set(cyrillic_lang_list):\n",
        "                    self.setModelLanguage('cyrillic', lang_list, cyrillic_lang_list+['en'],\n",
        "                                          '[\"ru\",\"rs_cyrillic\",\"be\",\"bg\",\"uk\",\"mn\",\"en\"]')\n",
        "                    model = recognition_models['gen1']['cyrillic_g1']\n",
        "                    recog_network = 'generation1'\n",
        "                else:\n",
        "                    self.model_lang = 'latin'\n",
        "                    model = recognition_models['gen2']['latin_g2']\n",
        "                    recog_network = 'generation2'\n",
        "            self.character = model['characters']\n",
        "\n",
        "            model_path = os.path.join(self.model_storage_directory, model['filename'])\n",
        "            # check recognition model file\n",
        "            if recognizer:\n",
        "                if os.path.isfile(model_path) == False:\n",
        "                    if not self.download_enabled:\n",
        "                        raise FileNotFoundError(\"Missing %s and downloads disabled\" % model_path)\n",
        "                    LOGGER.warning('Downloading recognition model, please wait. '\n",
        "                                   'This may take several minutes depending upon your network connection.')\n",
        "                    download_and_unzip(model['url'], model['filename'], self.model_storage_directory, verbose)\n",
        "                    assert calculate_md5(model_path) == model['filesize'], corrupt_msg\n",
        "                    LOGGER.info('Download complete.')\n",
        "                elif calculate_md5(model_path) != model['filesize']:\n",
        "                    if not self.download_enabled:\n",
        "                        raise FileNotFoundError(\"MD5 mismatch for %s and downloads disabled\" % model_path)\n",
        "                    LOGGER.warning(corrupt_msg)\n",
        "                    os.remove(model_path)\n",
        "                    LOGGER.warning('Re-downloading the recognition model, please wait. '\n",
        "                                   'This may take several minutes depending upon your network connection.')\n",
        "                    download_and_unzip(model['url'], model['filename'], self.model_storage_directory, verbose)\n",
        "                    assert calculate_md5(model_path) == model['filesize'], corrupt_msg\n",
        "                    LOGGER.info('Download complete')\n",
        "            self.setLanguageList(lang_list, model)\n",
        "\n",
        "        else: # user-defined model\n",
        "            with open(os.path.join(self.user_network_directory, recog_network+ '.yaml'), encoding='utf8') as file:\n",
        "                recog_config = yaml.load(file, Loader=yaml.FullLoader)\n",
        "            imgH = recog_config['imgH']\n",
        "            available_lang = recog_config['lang_list']\n",
        "            self.setModelLanguage(recog_network, lang_list, available_lang, available_lang)\n",
        "            #char_file = os.path.join(self.user_network_directory, recog_network+ '.txt')\n",
        "            self.character = recog_config['character_list']\n",
        "            model_file = recog_network+ '.pth'\n",
        "            model_path = os.path.join(self.model_storage_directory, model_file)\n",
        "            self.setLanguageList(lang_list, None)\n",
        "\n",
        "        dict_list = {}\n",
        "        for lang in lang_list:\n",
        "            dict_list[lang] = os.path.join(BASE_PATH, 'dict', lang + \".txt\")\n",
        "\n",
        "        if detector:\n",
        "            self.detector = get_detector(detector_path, self.device, quantize, cudnn_benchmark=cudnn_benchmark)\n",
        "        if recognizer:\n",
        "            if recog_network == 'generation1':\n",
        "                network_params = {\n",
        "                    'input_channel': 1,\n",
        "                    'output_channel': 512,\n",
        "                    'hidden_size': 512\n",
        "                    }\n",
        "            elif recog_network == 'generation2':\n",
        "                network_params = {\n",
        "                    'input_channel': 1,\n",
        "                    'output_channel': 256,\n",
        "                    'hidden_size': 256\n",
        "                    }\n",
        "            else:\n",
        "                network_params = recog_config['network_params']\n",
        "            self.recognizer, self.converter = get_recognizer(recog_network, network_params,                                                         self.character, separator_list,                                                         dict_list, model_path, device = self.device, quantize=quantize)\n",
        "\n",
        "    def setModelLanguage(self, language, lang_list, list_lang, list_lang_string):\n",
        "        self.model_lang = language\n",
        "        if set(lang_list) - set(list_lang) != set():\n",
        "            if language == 'ch_tra' or language == 'ch_sim':\n",
        "                language = 'chinese'\n",
        "            raise ValueError(language.capitalize() + ' is only compatible with English, try lang_list=' + list_lang_string)\n",
        "\n",
        "    def getChar(self, fileName):\n",
        "        char_file = os.path.join(BASE_PATH, 'character', fileName)\n",
        "        with open(char_file, \"r\", encoding=\"utf-8-sig\") as input_file:\n",
        "            list = input_file.read().splitlines()\n",
        "            char = ''.join(list)\n",
        "        return char\n",
        "\n",
        "    def setLanguageList(self, lang_list, model):\n",
        "        self.lang_char = []\n",
        "        for lang in lang_list:\n",
        "            char_file = os.path.join(BASE_PATH, 'character', lang + \"_char.txt\")\n",
        "            with open(char_file, \"r\", encoding = \"utf-8-sig\") as input_file:\n",
        "                char_list =  input_file.read().splitlines()\n",
        "            self.lang_char += char_list\n",
        "        if model:\n",
        "            symbol = model['symbols']\n",
        "        else:\n",
        "            symbol = '0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ '\n",
        "        self.lang_char = set(self.lang_char).union(set(symbol))\n",
        "        self.lang_char = ''.join(self.lang_char)\n",
        "\n",
        "    def detect(self, img, min_size = 20, text_threshold = 0.7, low_text = 0.4,               link_threshold = 0.4,canvas_size = 2560, mag_ratio = 1.,               slope_ths = 0.1, ycenter_ths = 0.5, height_ths = 0.5,               width_ths = 0.5, add_margin = 0.1, reformat=True, optimal_num_chars=None):\n",
        "\n",
        "        if reformat:\n",
        "            img, img_cv_grey = reformat_input(img)\n",
        "\n",
        "        text_box_list = get_textbox(self.detector, img, canvas_size, mag_ratio,\n",
        "                                    text_threshold, link_threshold, low_text,\n",
        "                                    False, self.device, optimal_num_chars)\n",
        "\n",
        "        horizontal_list_agg, free_list_agg = [], []\n",
        "        for text_box in text_box_list:\n",
        "            horizontal_list, free_list = group_text_box(text_box, slope_ths,\n",
        "                                                        ycenter_ths, height_ths,\n",
        "                                                        width_ths, add_margin,\n",
        "                                                        (optimal_num_chars is None))\n",
        "            if min_size:\n",
        "                horizontal_list = [i for i in horizontal_list if max(\n",
        "                    i[1] - i[0], i[3] - i[2]) > min_size]\n",
        "                free_list = [i for i in free_list if max(\n",
        "                    diff([c[0] for c in i]), diff([c[1] for c in i])) > min_size]\n",
        "            horizontal_list_agg.append(horizontal_list)\n",
        "            free_list_agg.append(free_list)\n",
        "\n",
        "        return horizontal_list_agg, free_list_agg\n",
        "\n",
        "    def recognize(self, img_cv_grey, horizontal_list=None, free_list=None,                  decoder = 'greedy', beamWidth= 5, batch_size = 1,                  workers = 0, allowlist = None, blocklist = None, detail = 1,                  rotation_info = None,paragraph = False,                  contrast_ths = 0.1,adjust_contrast = 0.5, filter_ths = 0.003,                  y_ths = 0.5, x_ths = 1.0, reformat=True, output_format='standard'):\n",
        "\n",
        "        if reformat:\n",
        "            img, img_cv_grey = reformat_input(img_cv_grey)\n",
        "\n",
        "        if allowlist:\n",
        "            ignore_char = ''.join(set(self.character)-set(allowlist))\n",
        "        elif blocklist:\n",
        "            ignore_char = ''.join(set(blocklist))\n",
        "        else:\n",
        "            ignore_char = ''.join(set(self.character)-set(self.lang_char))\n",
        "\n",
        "        if self.model_lang in ['chinese_tra','chinese_sim']: decoder = 'greedy'\n",
        "\n",
        "        if (horizontal_list==None) and (free_list==None):\n",
        "            y_max, x_max = img_cv_grey.shape\n",
        "            horizontal_list = [[0, x_max, 0, y_max]]\n",
        "            free_list = []\n",
        "\n",
        "        # without gpu/parallelization, it is faster to process image one by one\n",
        "        if ((batch_size == 1) or (self.device == 'cpu')) and not rotation_info:\n",
        "            result = []\n",
        "            for bbox in horizontal_list:\n",
        "                h_list = [bbox]\n",
        "                f_list = []\n",
        "                image_list, max_width = get_image_list(h_list, f_list, img_cv_grey, model_height = imgH)\n",
        "                result0 = get_text(self.character, imgH, int(max_width), self.recognizer, self.converter, image_list,                              ignore_char, decoder, beamWidth, batch_size, contrast_ths, adjust_contrast, filter_ths,                              workers, self.device)\n",
        "                result += result0\n",
        "            for bbox in free_list:\n",
        "                h_list = []\n",
        "                f_list = [bbox]\n",
        "                image_list, max_width = get_image_list(h_list, f_list, img_cv_grey, model_height = imgH)\n",
        "                result0 = get_text(self.character, imgH, int(max_width), self.recognizer, self.converter, image_list,                              ignore_char, decoder, beamWidth, batch_size, contrast_ths, adjust_contrast, filter_ths,                              workers, self.device)\n",
        "                result += result0\n",
        "        # default mode will try to process multiple boxes at the same time\n",
        "        else:\n",
        "            image_list, max_width = get_image_list(horizontal_list, free_list, img_cv_grey, model_height = imgH)\n",
        "            image_len = len(image_list)\n",
        "            if rotation_info and image_list:\n",
        "                image_list = make_rotated_img_list(rotation_info, image_list)\n",
        "                max_width = max(max_width, imgH)\n",
        "\n",
        "            result = get_text(self.character, imgH, int(max_width), self.recognizer, self.converter, image_list,                          ignore_char, decoder, beamWidth, batch_size, contrast_ths, adjust_contrast, filter_ths,                          workers, self.device)\n",
        "\n",
        "            if rotation_info and (horizontal_list+free_list):\n",
        "                # Reshape result to be a list of lists, each row being for \n",
        "                # one of the rotations (first row being no rotation)\n",
        "                result = set_result_with_confidence(\n",
        "                    [result[image_len*i:image_len*(i+1)] for i in range(len(rotation_info) + 1)])\n",
        "\n",
        "        if self.model_lang == 'arabic':\n",
        "            direction_mode = 'rtl'\n",
        "            result = [list(item) for item in result]\n",
        "            for item in result:\n",
        "                item[1] = get_display(item[1])\n",
        "        else:\n",
        "            direction_mode = 'ltr'\n",
        "\n",
        "        if paragraph:\n",
        "            result = get_paragraph(result, x_ths=x_ths, y_ths=y_ths, mode = direction_mode)\n",
        "\n",
        "        if detail == 0:\n",
        "            return [item[1] for item in result]\n",
        "        elif output_format == 'dict':\n",
        "            return [ {'boxes':item[0],'text':item[1],'confident':item[2]} for item in result]\n",
        "        else:\n",
        "            return result\n",
        "\n",
        "    def readtext(self, image, decoder = 'greedy', beamWidth= 5, batch_size = 1,                 workers = 0, allowlist = None, blocklist = None, detail = 1,                 rotation_info = None, paragraph = False, min_size = 20,                 contrast_ths = 0.1,adjust_contrast = 0.5, filter_ths = 0.003,                 text_threshold = 0.7, low_text = 0.4, link_threshold = 0.4,                 canvas_size = 2560, mag_ratio = 1.,                 slope_ths = 0.1, ycenter_ths = 0.5, height_ths = 0.5,                 width_ths = 0.5, y_ths = 0.5, x_ths = 1.0, add_margin = 0.1, output_format='standard'):\n",
        "        '''\n",
        "        Parameters:\n",
        "        image: file path or numpy-array or a byte stream object\n",
        "        '''\n",
        "        img, img_cv_grey = reformat_input(image)\n",
        "\n",
        "        horizontal_list, free_list = self.detect(img, min_size, text_threshold,                                                 low_text, link_threshold,                                                 canvas_size, mag_ratio,                                                 slope_ths, ycenter_ths,                                                 height_ths,width_ths,                                                 add_margin, False)\n",
        "        # get the 1st result from hor & free list as self.detect returns a list of depth 3\n",
        "        horizontal_list, free_list = horizontal_list[0], free_list[0]\n",
        "        result = self.recognize(img_cv_grey, horizontal_list, free_list,                                decoder, beamWidth, batch_size,                                workers, allowlist, blocklist, detail, rotation_info,                                paragraph, contrast_ths, adjust_contrast,                                filter_ths, y_ths, x_ths, False, output_format)\n",
        "\n",
        "        return result\n",
        "    \n",
        "    def readtextlang(self, image, decoder = 'greedy', beamWidth= 5, batch_size = 1,                 workers = 0, allowlist = None, blocklist = None, detail = 1,                 rotation_info = None, paragraph = False, min_size = 20,                 contrast_ths = 0.1,adjust_contrast = 0.5, filter_ths = 0.003,                 text_threshold = 0.7, low_text = 0.4, link_threshold = 0.4,                 canvas_size = 2560, mag_ratio = 1.,                 slope_ths = 0.1, ycenter_ths = 0.5, height_ths = 0.5,                 width_ths = 0.5, y_ths = 0.5, x_ths = 1.0, add_margin = 0.1, output_format='standard'):\n",
        "        '''\n",
        "        Parameters:\n",
        "        image: file path or numpy-array or a byte stream object\n",
        "        '''\n",
        "        img, img_cv_grey = reformat_input(image)\n",
        "\n",
        "        horizontal_list, free_list = self.detect(img, min_size, text_threshold,                                                 low_text, link_threshold,                                                 canvas_size, mag_ratio,                                                 slope_ths, ycenter_ths,                                                 height_ths,width_ths,                                                 add_margin, False)\n",
        "        # get the 1st result from hor & free list as self.detect returns a list of depth 3\n",
        "        horizontal_list, free_list = horizontal_list[0], free_list[0]\n",
        "        result = self.recognize(img_cv_grey, horizontal_list, free_list,                                decoder, beamWidth, batch_size,                                workers, allowlist, blocklist, detail, rotation_info,                                paragraph, contrast_ths, adjust_contrast,                                filter_ths, y_ths, x_ths, False, output_format)\n",
        "       \n",
        "        char = []\n",
        "        directory = 'characters/'\n",
        "        for i in range(len(result)):\n",
        "            char.append(result[i][1])\n",
        "        \n",
        "        def search(arr,x):\n",
        "            g = False\n",
        "            for i in range(len(arr)):\n",
        "                if arr[i]==x:\n",
        "                    g = True\n",
        "                    return 1\n",
        "            if g == False:\n",
        "                return -1\n",
        "        def tupleadd(i):\n",
        "            a = result[i]\n",
        "            b = a + (filename[0:2],)\n",
        "            return b\n",
        "        \n",
        "        for filename in os.listdir(directory):\n",
        "            if filename.endswith(\".txt\"):\n",
        "                with open ('characters/'+ filename,'rt',encoding=\"utf8\") as myfile:  \n",
        "                    chartrs = str(myfile.read().splitlines()).replace('\\n','') \n",
        "                    for i in range(len(char)):\n",
        "                        res = search(chartrs,char[i])\n",
        "                        if res != -1:\n",
        "                            if filename[0:2]==\"en\" or filename[0:2]==\"ch\":\n",
        "                                print(tupleadd(i))\n",
        "\n",
        "    def readtext_batched(self, image, n_width=None, n_height=None,                         decoder = 'greedy', beamWidth= 5, batch_size = 1,                         workers = 0, allowlist = None, blocklist = None, detail = 1,                         rotation_info = None, paragraph = False, min_size = 20,                         contrast_ths = 0.1,adjust_contrast = 0.5, filter_ths = 0.003,                         text_threshold = 0.7, low_text = 0.4, link_threshold = 0.4,                         canvas_size = 2560, mag_ratio = 1.,                         slope_ths = 0.1, ycenter_ths = 0.5, height_ths = 0.5,                         width_ths = 0.5, y_ths = 0.5, x_ths = 1.0, add_margin = 0.1, output_format='standard'):\n",
        "        '''\n",
        "        Parameters:\n",
        "        image: file path or numpy-array or a byte stream object\n",
        "        When sending a list of images, they all must of the same size,\n",
        "        the following parameters will automatically resize if they are not None\n",
        "        n_width: int, new width\n",
        "        n_height: int, new height\n",
        "        '''\n",
        "        img, img_cv_grey = reformat_input_batched(image, n_width, n_height)\n",
        "\n",
        "        horizontal_list_agg, free_list_agg = self.detect(img, min_size, text_threshold,                                                         low_text, link_threshold,                                                         canvas_size, mag_ratio,                                                         slope_ths, ycenter_ths,                                                         height_ths, width_ths,                                                         add_margin, False)\n",
        "        result_agg = []\n",
        "        # put img_cv_grey in a list if its a single img\n",
        "        img_cv_grey = [img_cv_grey] if len(img_cv_grey.shape) == 2 else img_cv_grey\n",
        "        for grey_img, horizontal_list, free_list in zip(img_cv_grey, horizontal_list_agg, free_list_agg):\n",
        "            result_agg.append(self.recognize(grey_img, horizontal_list, free_list,                                            decoder, beamWidth, batch_size,                                            workers, allowlist, blocklist, detail, rotation_info,                                            paragraph, contrast_ths, adjust_contrast,                                            filter_ths, y_ths, x_ths, False, output_format))\n",
        "\n",
        "        return result_agg\n",
        "\n",
        "\n",
        "# In[10]:\n",
        "\n",
        "\n",
        "import cv2\n",
        "\n",
        "reader = Reader(['en'], recog_network='english_g2',model_storage_directory=\"trained_model\",user_network_directory=\"network\")\n",
        "\n",
        "\n",
        "# In[13]:\n",
        "\n",
        "\n",
        "image = cv2.imread(\"WhatsApp Image 2021-10-06 at 7.04.36 PM.jpeg\")\n",
        "results = reader.readtext(image)\n",
        "\n",
        "def cleanup_text(text):\n",
        "    # strip out non-ASCII text so we can draw the text on the image\n",
        "    # using OpenCV\n",
        "    return \"\".join([c if ord(c) < 128 else \"\" for c in text]).strip()\n",
        "\n",
        "for (bbox, text, prob) in results:\n",
        "    # display the OCR'd text and associated probability\n",
        "    print(\"[INFO] {:.4f}: {}\".format(prob, text))\n",
        "    # unpack the bounding box\n",
        "    (tl, tr, br, bl) = bbox\n",
        "    tl = (int(tl[0]), int(tl[1]))\n",
        "    tr = (int(tr[0]), int(tr[1]))\n",
        "    br = (int(br[0]), int(br[1]))\n",
        "    bl = (int(bl[0]), int(bl[1]))\n",
        "    # cleanup the text and draw the box surrounding the text along\n",
        "    # with the OCR'd text itself\n",
        "    #text = cleanup_text(text)\n",
        "    cv2.rectangle(image, tl, br, (0, 255, 0), 2)\n",
        "    cv2.putText(image, text, (tl[0], tl[1] - 10),\n",
        "    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
        "\n",
        "# plt.imshow(image)\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# In[ ]:"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CUDA not available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
            "Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-2acf55456bed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2278\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2280\u001b[0;31m \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecog_network\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english_g2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_storage_directory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"trained_model\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muser_network_directory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"network\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-2acf55456bed>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang_list, gpu, model_storage_directory, user_network_directory, recog_network, download_enabled, detector, recognizer, verbose, quantize, cudnn_benchmark)\u001b[0m\n\u001b[1;32m   1930\u001b[0m                 LOGGER.warning('Downloading detection model, please wait. '\n\u001b[1;32m   1931\u001b[0m                                'This may take several minutes depending upon your network connection.')\n\u001b[0;32m-> 1932\u001b[0;31m                 \u001b[0mdownload_and_unzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetection_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdetector_model\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetection_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdetector_model\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filename'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_storage_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1933\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0mcalculate_md5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetector_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdetection_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdetector_model\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filesize'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrupt_msg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1934\u001b[0m                 \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Download complete'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'url'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUuhJ9XSZVRa"
      },
      "source": [
        "\n",
        "get_ipython().run_line_magic('pylab', 'inline')\n",
        "import matplotlib.pyplot as plt\n",
        "img = image\n",
        "imgplot = plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}